# Awesome-Survey-on-Foundation-Models-for-RL-Robotics

## Paper Taxonomy
| Index | Paper                                                        | Year | Conference/Journal | Application                                                  | Type of Generative/Foundational Model                        | The role that GM/FM plays in robotics/RL                     | Problem                                                      |
| ----- | ------------------------------------------------------------ | ---- | ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1     | RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback | 2024 | arxiv              | classic control, rigid and articulated object manipulation   | VLMs, leveraging vision language foundation models (VLMs) that are trained on diverse, general text and image corpora (e.g., GPT-4V (OpenAI, 2023), Gemini (Team et al., 2023)). | Query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. |                                                              |
| 2     | Foundation Reinforcement Learning: Towards Embodied Generalist Agents With Foundation Prior Assistance | 2023 | arxiv              | Foundation Actor-Critic on robotics manipulation tasks       | Policy Foundation Prior：Follow the work UniPi, which infers actions based on a language conditioned video prediction model and a universal inverse dynamics model. For the video prediction model, use VLM （Seer, Seer: Language instructed video prediction with latent diffusion models）predicts a video conditioned on one image and a language instruction with latent diffusion models, pre-trained on Something Something V2 (Goyal et al., 2017) and Bridge Data (Ebert et al., 2021) datasets. <br> Value Foundation Prior: utilize VIP (Vip: Towards universal visual reward and representation via value-implicit pre-training) , which trains a universal value function via pre-training on internet-scale robotics datasets. | Build prior knowledge for Policy and Value of RL.            |                                                              |
| 3     | Language Conditioned Imitation Learning over Unstructured Data | 2020 | arxiv              | Human language conditioned robotic manipulation, a single agent must execute a series of visual manipulation tasks in a row, each expressed in free-form natural language, e.g. “open the door all the way to the right...now grab the block...now push the red button...now open the drawer”. <br> Agents in this scenario are expected to be able to perform any combination of subtasks in any order. Incorporating free-form natural language conditioning into imitation learning, learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network. | Language conditioned visuomotor policy. Text conditioned policies with large pretrained neural language models. | The resulting language conditioned visuomotor policy can follow many free-form human text instructions over a long horizon in a simulated 3D tabletop setting, i.e. “open the door. . . now pick up the block. . . now press the red button” (see video). <br> Combine text conditioned policies with large pretrained neural language models to scale up the number of instructions an agent can follow. |                                                              |
| 4     | Policy adaptation from foundation model feedback             | 2023 | CVPR               | By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. The policy still fails in most cases given an unseen task or environment. Generalize the learned policy to unseen tasks and environments. | VLM                                                          | When deploying the trained policy to a new task or a new environment, let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong,  can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. |                                                              |
| 5     | CLIPORT : What and Where Pathways for Robotic Manipulation   | 2021 | CoRL               | End-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. This method is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. ( experiment in the Ravens). CLIPort grounds semantic concepts in precise spatial reasoning, but it is limited to 2D observation and action spaces. | A language-conditioned imitation learning agent that combines the broad semantic understanding (what) of CLIP with the spatial precision (where) of Transporter. | A two-stream architecture with semantic and spatial pathways for vision-based manipulation. Combines the best of both worlds: end-to-end learning for fine-grained manipulation with the multi-goal and multi-task generalization capabilities of vision-language grounding systems. Two-stream architecture for using internet pre-trained vision-language models for conditioning precise manipulation policies with language goals. |                                                              |
| 6     | Perceiver-actor: A multi-task transformer for robotic manipulation | 2022 | CoRL               | Exploit the 3D structure of voxel patches for efficient 6-DoF(degrees of freedom) behavior-cloning(BC) with Transformers (analogous to how vision transformers exploit the 2D structure of image patches). Conduct large-scale experiments in the RLBench environment. | A language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. | PERACT encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. PERACT is essentially a classifier trained with supervised learning to detect actions akin to prior work like CLIPort, except our observations and actions are represented with 3D voxels instead of 2D image pixels. Use a Perceiver Transformer to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. |                                                              |
| 7     | Language-driven representation learning for robotics         | 2023 | arxiv              | Visual representation learning for robotics.                 | Multi modal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others,  where Voltron differs, however, is in their novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics. | We then introduce Voltron, a framework for languagedriven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. |                                                              |
| 8     | CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning | 2022 | arxiv              | Novel framework designed to enhance scalability in robot learning using foundation models such as Stable Diffusion. Scaling robot learning, with specific focus on multi-task and multi-scene manipulation in kitchen environments, both in simulation and in the real world. | Visual generative models such as Stable Diffusion pretrained zero-shot visual representation models trained on out-of-domain data. | In the data collection stage, limited in-domain expert demonstration data is collected. In the data augmentation stage, CACTI employs visual generative models such as Stable Diffusion [117] to boost visual diversity by augmenting the data with scene and layout variations. In the visual representation learning stage, CACTI leverages pretrained zero-shot visual representation models trained on out-of-domain data to improve training efficiency. Finally, in the imitation policy training stage, a general multi-task policy is learned using imitation learning on the augmented dataset with compressed visual representations as input. |                                                              |
| 9     | MUTEX: Learning Unified Policies from Multimodal Task Specifications | 2023 | CoRL               | Policy learning from multimodal task specifications.         | Transformer-based model  can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them. | Trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure. |                                                              |
| 10    | Human-Timescale Adaptation in an Open-Ended Task Space       | 2023 | ICML               | Fast and flexible adaptation. Training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. This work considers navigation, coordination, and division of labor tasks. | Transformers as an architectural choice.                     | Use Transformers as an architectural choice to scale incontext fast adaptation via model-based RL. An agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. |                                                              |
| 11    | R3M: A Universal Visual Representation for Robot Manipulation | 2022 | CoRL               | Visual Representation Learning, Robotic Manipulation. Study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. | Visual representations.                                      | Visual representation for robot manipulation. Provides pretrained visual representation for robot manipulation using diverse human video datasets such as Ego4D and can be used as a frozen perception module for policy learning in robot manipulation tasks. |                                                              |
| 12    | Towards universal visual reward and representation via value-implicit pre-training | 2023 | ICLR               | A key unsolved problem to pre-training for robotic control is the challenge of reward specification. Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. | Visual representation.                                       | Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. Casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self supervised goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. |                                                              |
| 13    | LIV: Language-Image Representations and Rewards for Robotic Control | 2023 | ICML               | Vision-language representation and reward learning from action-free videos with text annotations for Robotic Control. Use LIV to pre-train the first control-centric vision-language representation from large human video datasets such as EpicKitchen. | Vision-language representation. Builds on our prior work Value Implicit Pre-Training (VIP),  a selfsupervised approach for learning visual goal-conditioned value functions and representations from videos, generalizing it to learning multi-modal, vision-language values and representations from language-aligned videos, as described above. | Given only a language or image goal, the pre-trained LIV model can assign dense rewards to each frame in videos of unseen robots or humans attempting that task in unseen environments. Further, when some target domain-specific data is available, the same objective can be used to fine-tune and improve LIV and even other pre-trained representations for robotic control and reward specification in that domain. |                                                              |
| 14    | Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation | 2021 | CoRL               | Natural Language, Offline RL, Visuomotor Manipulation. Learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. Grounding language(provides a convenient and flexible alternative for task specification) in the robot’s observation space. | Combine language-conditioned reward with visual model predictive control. Pretrained language models. | Learns a language-conditioned reward from offline data and uses it during model predictive control to complete language specified tasks. |                                                              |
| 15    | Do As I Can, Not As I Say: Grounding Language in Robotic Affordances | 2022 | CoRL               | Large language models can encode a wealth of semantic knowledge about the world, but a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. | LLMs                                                         | Provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. <be> They show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. <br> They use the language model to provide task-grounding (Say), enabling the determination of useful sub-goals based on high-level instructions, and a learned affordance function to achieve world-grounding (Can), enabling the identification of feasible actions to execute the plan. |                                                              |
| 16    | Inner Monologue: Embodied Reasoning through Planning with Language Models | 2022 | arxiv              | Studies the role of grounded environment feedback provided to the LLM, thus closing the loop with the environment. The feedback is used for robot planning with large language models by leveraging a collection of perception models (e.g., scene descriptors and success detectors) in tandem with pretrained language-conditioned robot skills. | LLMs applied to domains beyond natural language processing (such as planning and interaction for robots). | By leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. |                                                              |
| 17    | Text2Motion: From Natural Language Instructions to Feasible Plans | 2023 | Autonomous Robots  | Language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Long-horizon planning, Robot manipulation, Large language models. | LLM Language-based planning.                                 | A language-based planning framework that interfaces an LLM with a library of learned skills and a geometric feasibility planner [8] to solve complex sequential manipulation tasks. |                                                              |
| 18    | Voxposer: Composable 3d value maps for robotic manipulation with language models | 2023 | CoRL               | Synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. Manipulation, Large Language Models, Model-based Planning | VLMs                                                         | First observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. <br> More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. <br> Given the RGB-D observation of the environment and language instruction, VoxPoser utilizes large language models to generate code, which interacts with vision-language models to extract a sequence of 3D affordance maps and constraint maps. These maps are composed together to create 3D value maps. The value maps are then utilized as objective functions to guide motion planners to synthesize trajectories for everyday manipulation tasks without requiring any prior training or instruction. |                                                              |
| 19    | Zero-shot reward specification via grounded natural language | 2022 | ICML               | Learning text-conditioned policies. Robot manipulation tasks. | Large-scale visuolanguage models like CLIP.                  | Use developments in building large-scale visuo-lingual models like CLIP to devise a framework that generates the task reward signal just from goal text description and raw pixel observations which is then used to learn the task policy. |                                                              |
| 20    | Grounding Language with Visual Affordances over Unstructured Data | 2023 | ICRA               | Considers robot manipulation task.                           | Hierarchical language-conditioned agent that integrates the task-agnostic control of HULC (state-of-the-art language-conditioned imitation learning agent that learns 7DoF goal-reaching policies end-to-end) [10] with the object-centric semantic understanding of VAPO (extracts a self-supervised visual affordance model of unstructured data and not only accelerates learning, but was also shown to boost generalization of downstream control policies). | A self-supervised visuo-lingual affordance model is used to learn general-purposed language conditioned robot skills from unstructured offline data in the real world. <br> Given visual observations and language instructions as input, the affordance model outputs a pixel-wise heat map that represents affordance regions and the corresponding depth map. |                                                              |
| 21    | Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception | 2023 | arxiv              | The paper focuses on enhancing robot perception in human-robot interaction scenarios by using vision-language models (VLMs) to integrate inertial measurement unit (IMU) data with visual inputs. Robot perception requires accurate scene understanding, which is challenging when relying on a single modality like vision. The aim is to align the embedding spaces of different modalities (vision and IMU data) to improve scene understanding and reasoning without extensive retraining. | The approach involves extending frozen, pre-trained vision-language models to incorporate IMU data. This is achieved through a combination of supervised and contrastive training, aligning the embedding spaces of different modalities to the vision embedding space. This method leverages the abstract skills learned by large language models (LLMs) during pre-training and applies them to multiple modalities. | The paper addresses the challenge of robot perception by integrating inertial measurement unit (IMU) data with visual inputs using vision-language models (VLMs). By aligning the embedding spaces of different modalities through supervised and contrastive training, the VLM enhances scene understanding and reasoning without retraining. This approach improves robot perception, making VLMs more versatile and effective in various robotic and reinforcement learning tasks. |                                                              |
| 22    | Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation | 2022 | CVPR               | The research focuses on Vision-and-Language Navigation (VLN) within the context of both discrete and continuous environments. The primary application is to develop agents that can navigate real-world-like continuous spaces based on human instructions.  The main problem addressed is the domain gap between discrete and continuous VLN environments. Training agents in discrete spaces (which rely on pre-defined connectivity graphs) is significantly easier than in continuous spaces (which are more realistic but lack such graphs). This gap hinders the transferability and generalization of VLN agents trained in one environment to another. | The research utilizes a candidate waypoints predictor trained with refined connectivity graphs from Matterport3D to Habitat-Matterport3D environments. This predictor generates a set of accessible waypoints during navigation in continuous spaces. The models include: Cross-Modal Matching Agent (CMA) VLN\BERT. The training approach employs a simple imitation learning objective to enhance agent performance. | This research addresses the domain gap between discrete and continuous Vision-and-Language Navigation (VLN) environments by using a candidate waypoints predictor trained on refined connectivity graphs. The predictor generates navigable waypoints in continuous environments, enabling agents to perform high-level actions and navigate effectively based on visual observations. This approach enhances the transferability and generalization of VLN agents, allowing them to operate more robustly in realistic, continuous spaces, thereby improving their applicability in real-world robotics and reinforcement learning scenarios. |                                                              |
| 23    | NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation | 2023 | arxiv              | The application addressed in this paper is Vision-and-Language Navigation (VLN), which involves enabling AI agents to navigate unseen environments by following linguistic instructions. The primary aim is to improve generalization in navigation tasks, ensuring that agents can effectively operate in diverse and previously unencountered environments, ranging from out-of-distribution scenes to real-world applications without relying on maps, odometers, or depth inputs. | The foundation model used in this paper is a video-based large Vision-Language Model (VLM) called NaVid. This model leverages pre-trained vision encoders and large language models (LLMs) to interpret visual data from a monocular RGB camera and human-issued instructions. NaVid integrates these inputs to make real-time navigation decisions, bypassing the need for traditional navigation aids like maps and odometer data. | In this work, the VLM plays a critical role in enhancing the generalization and adaptability of robotic navigation systems. By encoding historical observations and spatio-temporal contexts from video data, NaVid provides a robust framework for action planning and instruction following. The VLM allows the robot to understand and execute navigation tasks based on visual observations and natural language instructions, addressing the challenges of odometer noise and domain discrepancies, thus facilitating smoother Sim2Real transitions and improving overall navigation performance in real-world settings. |                                                              |
| 24    | Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs | 2023 | CoRL               | The paper introduces the Open-Vocabulary 3D Scene Graph (OVSG), a framework aimed at enhancing robotic systems' ability to ground various entities, such as objects, agents, and regions, in real-world scenarios using free-form text queries. The key application is to improve context-aware localization, enabling robots to understand and execute complex commands like "pick up a cup on the kitchen table" or "navigate to a sofa where someone is sitting". This addresses the challenge of grounding semantic entities in real-world navigation and manipulation tasks, thereby enhancing the practical functionality of robots in everyday environments. | The foundation model used in this paper combines open-vocabulary detection with 3D scene graphs. By leveraging the strengths of 3D scene graphs, which encode spatial and relational context among objects, and incorporating an open-vocabulary approach, OVSG can handle a broader range of queries, including those involving unseen semantic categories and relationships. This hybrid approach enhances the system's ability to accurately ground entities in varied and dynamic real-world settings. | In this work, Vision-Language Models (VLMs) play a critical role in bridging the gap between natural language queries and robotic perception. By integrating VLMs with 3D scene graphs, the system can interpret complex, context-aware commands and translate them into actionable tasks for robots. This integration allows robots to utilize semantic and spatial information more effectively, enabling precise localization and interaction with objects based on detailed human instructions. Consequently, VLMs significantly enhance the robots' capability to understand and respond to open-vocabulary queries in reinforcement learning and real-world applications. |                                                              |
| 25    | Interactive Language: Talking to Robots in Real Time         | 2022 | arxiv              | The primary application of this paper is to develop a framework for real-time, natural language-instructable robots capable of executing a diverse array of tasks in the real world. These tasks range from basic commands to complex, long-horizon manipulation goals. The framework aims to produce robots that can proficiently understand and act upon natural language instructions, enabling them to perform tasks that require precise and dynamic control in real-time. | The foundation model utilized in this paper is based on behavioral cloning, trained on a vast dataset of language-annotated trajectories. This model leverages imitation learning techniques to interpret and execute natural language commands. The approach integrates components such as event-selectable hindsight relabeling and combines existing methods to create a scalable and efficient recipe for learning a wide range of language-conditionable skills. | In this work, the Vision-Language Model (VLM) plays a crucial role in bridging the gap between natural language instructions and robotic actions. VLMs facilitate the robot's understanding of complex, context-rich commands by mapping language inputs to corresponding visual and motor actions. This enables the robot to perform continuous-control visuo-motor manipulation in real-time, effectively responding to new instructions during ongoing task execution. By incorporating VLMs, the framework demonstrates improved interaction capabilities and task performance, allowing the robot to handle a broader spectrum of language-guided tasks in dynamic environments. |                                                              |
| 26    | ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts | 2022 | CVPR               | This paper addresses the challenge of Vision-Language Navigation (VLN), which involves guiding an embodied agent to navigate complex visual environments based on language instructions. The application problem tackled is enabling the agent to perform action-level modality alignment, ensuring that it can accurately interpret and execute instructions within the given visual context to achieve successful navigation. | The paper utilizes the Contrastive Language-Image Pretraining (CLIP) model as the foundation model. CLIP is known for its powerful cross-modality alignment capabilities, effectively bridging the gap between visual and textual information. It helps in collecting high-quality action prompts that are crucial for training the VLN agent to understand and act upon complex instructions. | In this work, foundation models like CLIP play a pivotal role in enhancing the VLN agent's ability to align actions with visual and language inputs. By providing modality-aligned action prompts, the model explicitly learns the alignment between different modalities, which improves the agent's decision-making during navigation. This approach leverages the strengths of CLIP in cross-modal alignment to address the inherent challenges in robotics and reinforcement learning, leading to more robust and interpretable navigation strategies. |                                                              |
| 27    | The Unsurprising Effectiveness of Pre-Trained Vision Models for Control | 2022 | ICML               | The application addressed in this paper is the use of pre-trained visual representations for control tasks in robotics and reinforcement learning (RL). The study focuses on whether these pre-trained visual models, which are typically used for computer vision tasks, can be effectively applied to diverse control environments, such as habitat navigation, dexterous manipulation, and robotic kitchen tasks, without being trained on data from these specific environments. | The paper utilizes pre-trained visual representation (PVR) models that have been trained on large-scale computer vision datasets like ImageNet. These models are used as frozen perception modules for various control tasks. The study investigates different PVR models, including those trained with supervised learning and self-supervised learning (SSL), and examines their effectiveness in control domains. | In this work, the pre-trained visual models (akin to visual language models, VLMs) serve as foundational perception components that can generalize across different control tasks. The paper demonstrates that these models, despite being trained on out-of-domain datasets, can outperform or match the performance of ground-truth state features in training control policies. This finding suggests that pre-trained visual models can significantly reduce the need for task-specific data collection and training in robotics and RL, thus enhancing the efficiency and scalability of deploying control policies in diverse environments. |                                                              |
| 28    | CoWs on PASTURE: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation | 2022 | arxiv              | The application addressed by this paper is language-driven zero-shot object navigation (L-ZSON). This involves enabling robots to find arbitrary objects based on natural language descriptions without needing extensive navigation training on domain-specific data. The research aims to enhance robots' practical utility by enabling them to locate objects described in human language, addressing a more challenging but highly applicable version of object navigation that includes finding uncommon, spatially described, and hidden objects. | The foundation model used in this paper is CLIP (Contrastive Language-Image Pretraining), adapted in a framework referred to as CLIP on Wheels (CoW). This model leverages the success of open-vocabulary models for image classification, object detection, and semantic segmentation to perform zero-shot object navigation without additional fine-tuning. The CoW framework enables robots to interpret natural language descriptions and localize objects accordingly. | In this work, the foundation models like CLIP play a crucial role in enabling robots to understand and act upon natural language instructions. CLIP's ability to match images with textual descriptions allows the CoW framework to perform object localization and navigation without requiring extensive pre-training on specific environments or objects. This approach demonstrates the potential of leveraging large language models and vision-language models to bridge the gap between human language and robotic perception, thereby improving the robots' efficiency in zero-shot tasks and expanding their applicability in real-world scenarios. |                                                              |
| 29    | A Recurrent Vision-and-Language BERT for Navigation          | 2021 | CVPR               | The application problem addressed in this paper is vision-and-language navigation (VLN), where an agent must navigate through an environment following natural language instructions. The main challenge in VLN lies in adapting the BERT architecture to the partially observable Markov decision process inherent in VLN tasks. This requires the model to handle history-dependent attention and decision-making to effectively interpret and act on the instructions. | The foundation model used in this paper is a recurrent vision-and-language BERT (V&L BERT) model, which is enhanced to be time-aware. The authors propose a recurrent function within the BERT architecture to maintain cross-modal state information, enabling the model to manage historical context and make informed navigation decisions. This approach leverages pre-trained V&L BERT models and fine-tunes them for the VLN task. | In this work, foundation models like V&L BERT play a critical role by providing a robust framework for integrating visual and language information, essential for navigating based on instructions. The recurrent V&L BERT model addresses the VLN challenges by utilizing self-attention mechanisms to process partial observations and maintain relevant sub-instruction localization. This allows the agent to perform navigation tasks more effectively, demonstrating the potential of foundation models to enhance multi-modal decision-making processes in robotics and reinforcement learning. |                                                              |
| 30    | ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes | 2023 | ICCV               | The paper addresses the challenge of understanding and manipulating continuous object states in robotics tasks, which is crucial for effective task learning and planning in the real world. Existing benchmarks often assume discrete object states, limiting the ability of robots to follow human instructions based on detailed and nuanced object states. This work introduces the ARNOLD benchmark, which focuses on language-grounded task learning with continuous object states in realistic 3D scenes, aiming to bridge the gap between simulated environments and real-world applications. | The foundation models used in this paper are state-of-the-art language-conditioned policy learning models. These models are designed to process and interpret human language instructions, enabling robots to understand and execute tasks in a continuous state space. The paper utilizes these models to assess task performance and generalization abilities across various scenes and object states. | In this work, foundation models like language-conditioned policy learning models play a crucial role in grounding language instructions to precise robot actions and object states. These models enable robots to interpret and act upon detailed language descriptions of tasks, promoting the learning of precise manipulations in continuous state environments. The models are evaluated on their ability to generalize learned skills to unseen scenarios, highlighting both the strengths and the areas needing improvement for effective language-grounded task learning in robotics and reinforcement learning (RL). |                                                              |
| 31    | BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs | 2024 | arxiv              | This paper addresses the challenge of generating effective behavior trees (BTs) for robots, particularly focusing on creating adaptable, flexible, and efficient task planning systems. The study aims to demonstrate that compact large language models (LLMs) with up to 7 billion parameters can be fine-tuned to produce satisfactory results in generating behavior trees, thus facilitating improved robot behaviors in dynamic environments. The practical applicability of this approach is validated through static syntactical analysis, a validation system, a simulated environment, and real robot deployments. | The research utilizes lightweight large language models (LLMs) with a maximum of 7 billion parameters. Specifically, it employs models such as GPT-3.5, llama2, llama-chat, and code-llama. These models are fine-tuned on a specially created dataset based on existing behavior trees to generate the desired robot behaviors. The study involves a comprehensive comparison of these LLMs across nine distinct tasks to evaluate their performance and effectiveness in generating behavior trees. | In this work, the foundation models, such as LLMs, play a crucial role in addressing the complexity of robot task planning. By leveraging the vast knowledge encoded within these models, the research demonstrates that LLMs can generate effective and efficient behavior trees, even with a limited number of parameters. The LLMs are fine-tuned to understand and generate complex task plans, allowing robots to perform high-level decision-making and adapt to dynamic environments. This approach reduces the resource intensity typically required for larger models while maintaining comparable effectiveness, thereby enhancing the practical deployment of such models in robotics and reinforcement learning applications. |                                                              |
| 32    | To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions | 2024 | arxiv              | The application of this paper is to enable robots to provide unobtrusive physical support within a group of humans. The concept, termed Attentive Support, focuses on ensuring that robots can assist in social and group interactions without disturbing or distracting the human participants. The goal is to facilitate seamless human-robot interactions in multiparty settings by interpreting human needs and constraints, offering help only when necessary or explicitly requested. | The paper employs a framework based on Large Language Models (LLMs) to implement the concept of Attentive Support. The adaptable LLM-based framework is used as a modular set of capabilities, which includes components like a robot-character to interpret and respond to human interactions effectively. This approach leverages the advanced natural language understanding and reasoning capabilities of LLMs to achieve its objectives. | In this work, the foundation models, such as LLMs, play a crucial role in interpreting natural language requests, understanding the current physical and social context, and making informed decisions about when and how to provide support. These models enable the robot to ground its actions in the physical and social states of the group members, ensuring that the robot’s behavior is neither obtrusive nor distracting. By leveraging the capabilities of LLMs, the robot can maintain awareness of human goals, constraints, and situational factors, thus enhancing its ability to support humans in various scenarios where physical assistance is required. |                                                              |
| 33    | Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues | 2024 | arxiv              | The primary application of this paper is in enhancing human-robot interaction (HRI) by improving the robot's ability to interpret verbal instructions, particularly in scenarios requiring social navigation. The paper addresses the shortcomings of Large Language Models (LLMs) in handling the nuances of verbal instructions, such as tone and affect, which are critical for effective human-robot communication. By integrating audio transcription with affective vocal features, the proposed approach, "Beyond Text," aims to make LLMs more reliable and effective in interpreting and responding to human instructions, thus facilitating more natural and trustworthy robot behavior in social contexts. | This paper utilizes advanced Large Language Models (LLMs) like GPT-4 and the Gemini series as the foundation models. The key innovation is the integration of audio transcription and analysis of affective vocal features—such as pitch, loudness, and duration—alongside textual input. This approach allows the LLMs to interpret the nuances of human speech more accurately, leveraging the combined strengths of text and audio inputs to enhance decision-making processes in robots. | In this work, foundation models like LLMs play a crucial role in advancing the capabilities of robots in understanding and responding to human verbal instructions. By extending the traditional text-based input to include audio cues, these models can capture and interpret the subtleties of human speech, such as emotional tone and emphasis. This multi-modal approach enables robots to better understand human intentions and uncertainties, leading to more accurate and contextually appropriate responses. As a result, the integration of LLMs with audio analysis significantly enhances the robots' performance in social navigation tasks, making them more robust against adversarial attacks and improving their overall reliability and trustworthiness in human-robot interactions. |                                                              |
| 34    | SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge | 2024 | AAAI               | The primary application addressed by this paper is the enhancement of planning capabilities using Large Language Models (LLMs) combined with heuristic planning methods. Traditional planning methods, which rely on domain-specific knowledge such as Planning Domain Definition Language (PDDL), face challenges in generating feasible and optimal plans in complex environments. This paper proposes a novel approach called SayCanPay that leverages the extensive world knowledge embedded in LLMs to generate candidate actions, assesses their feasibility, and evaluates their cost-effectiveness, thereby improving planning efficiency and effectiveness in various scenarios. | The foundation model utilized in this paper is a Large Language Model (LLM). The approach, SayCanPay, employs the LLM to generate potential candidate actions based on a given goal and initial observation. These actions are then scored for feasibility using a domain-specific affordance model, and for cost-effectiveness using a heuristic estimator. By integrating these evaluations, SayCanPay aims to identify the most feasible and cost-effective sequence of actions for planning tasks. | In this work, the foundation model, specifically the LLM, plays a crucial role in enhancing the planning process within the context of robotics and reinforcement learning (RL). The LLM is utilized to generate a broad set of potential actions based on its extensive training on world knowledge. This generation capability allows the model to propose actions that might not be explicitly defined in traditional planning frameworks. The generated actions are then evaluated for feasibility and payoff, integrating heuristic search techniques to identify optimal plans. This combined approach leverages the generative power and world knowledge of LLMs to address planning challenges in dynamic and complex environments, ultimately leading to more effective and adaptable robotic and RL systems. |                                                              |
| 35    | Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning | 2023 | arxiv              | The application of this paper is focused on improving robotic task planning by integrating physically-grounded perception with language understanding. It aims to enhance robots' capability to perform complex, long-horizon tasks in dynamic and varied environments by grounding their planning process in visual and spatial information. This integration allows robots to effectively interpret and execute high-level language instructions in real-world scenarios. | The paper utilizes Vision-Language Models (VLMs) as the foundation model. These models combine visual perception with language processing, allowing the system to generate a sequence of actionable steps based on visual observations and language instructions. The specific approach introduced is called Robotic Vision-Language Planning (ViLA), which directly incorporates perceptual data into the reasoning and planning process. | In this work, foundation models like LLMs and VLMs play a crucial role by providing the semantic and perceptual grounding necessary for effective task planning in robotics. By leveraging VLMs, the system can understand and integrate common-sense knowledge, spatial layouts, and object attributes, enabling robots to perform tasks more accurately and safely. This multimodal approach addresses the limitations of traditional LLM-based planners by ensuring that robots can reason about and adapt to their physical environment in real-time, leading to more robust and flexible robotic planning and execution. |                                                              |
| 36    | Corrective Planning of Robot Actions with Large Language Models | 2023 | ICRA               | The primary application addressed in this paper is the development of fully autonomous robotic systems capable of performing tasks traditionally handled by humans in open-world environments. The complexity of these environments necessitates advanced task and motion planning capabilities. This study specifically aims to enhance robot manipulation tasks by integrating reasoning, planning, and motion generation using a hierarchical architecture that includes feedback loops to handle errors in generated plans. | The foundation model employed in this study is Large Language Models (LLMs). These models are utilized to leverage their rich commonsense knowledge and implicit reasoning capabilities to generate potential action plans. The proposed architecture, CoPAL (closed-loop task planning mechanism with a multi-level feedback loop), relies on LLMs to create and refine task plans, incorporating feedback to improve plan execution and handling unexpected environmental changes. | In this work, Large Language Models (LLMs) play a crucial role in bridging the gap between high-level reasoning and low-level motion planning in robotics. LLMs are used to generate initial action plans and are subsequently involved in a feedback loop where their outputs are continuously checked and refined based on execution success or failure. This approach allows for dynamic adjustment of plans in real-time, enhancing the robot's ability to adapt to unforeseen changes and errors, thus improving the overall robustness and efficiency of task execution in complex, real-world scenarios. |                                                              |
| 37    | LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement | 2023 | arxiv              | The application of this paper focuses on the task of semantic object rearrangement in robotics, where robots are required to organize objects according to linguistic descriptions. This involves interpreting natural language instructions to generate actionable plans for rearranging objects in complex geometric patterns. The goal is to enhance the robots' capability to perform everyday tasks, such as setting up a kitchen, by accurately following verbal instructions. | This paper utilizes a framework called Language-Guided Monte Carlo Tree Search (LGMCTS), which integrates large language models (LLMs) with Monte Carlo Tree Search (MCTS) for task and motion planning (TAMP). The LLMs are used to generate intermediate representations from language inputs, which are then used by MCTS to formulate feasible action plans that achieve the specified geometric rearrangements. | In this work, LLMs play a crucial role in bridging the gap between natural language descriptions and executable robotic actions. The LLMs generate intermediate representations that capture the semantic and geometric constraints of the task, which are then used by MCTS to plan the sequence of actions required to achieve the desired object rearrangement. This approach leverages the strengths of LLMs in understanding and interpreting complex linguistic inputs and combines them with the robustness of traditional TAMP algorithms to create feasible and executable plans for robotic manipulation tasks. |                                                              |
| 38    | Prompt a Robot to Walk with Large Language Models            | 2023 | arxiv              | The application of this paper is focused on using large language models (LLMs) to control robot walking in dynamic environments. The authors aim to leverage the capabilities of LLMs to generate low-level control commands for robots, enabling them to perform dynamic tasks such as walking. This approach addresses the challenge of grounding LLMs in the physical world to create dynamic robot motions, without the need for task-specific fine-tuning of the models. | The foundation model used in this paper is GPT-4, a large language model pre-trained on vast internet-scale text data. The paper introduces a novel paradigm where few-shot prompts collected from the physical environment are used to interact with the LLM, enabling it to generate appropriate control commands for the robot. This method bypasses the need for extensive task-specific fine-tuning by utilizing the LLM's ability to generalize from the few-shot prompts provided. | In this work, the foundation model like GPT-4 plays a crucial role in generating control commands for robot motion. By using few-shot prompts derived from physical environment interactions, the LLM can produce dynamic and contextually appropriate actions for the robot. This framework positions the LLM as a feedback policy generator that outputs target joint positions for the robot, thereby enabling it to interact with and adapt to its physical surroundings effectively. The approach demonstrates the potential of LLMs to function as low-level feedback controllers for dynamic motion control in robotics, even in complex and high-dimensional environments. |                                                              |
| 39    | Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment | 2023 | arxiv              | The application of this paper focuses on improving the execution of tasks in robotic systems by addressing the misalignments between high-level plans generated by large language models (LLMs) and the actual low-level execution in the physical world. The proposed DoReMi framework aims to enable robots to detect and recover from these plan-execution misalignments in real-time, ensuring more reliable and efficient task completion. | This paper leverages large language models (LLMs) and vision language models (VLMs) as the foundation models. The LLMs are utilized for high-level planning and generating constraints for low-level execution, while VLMs are employed to continuously monitor and detect any constraint violations during the execution of low-level skills. | In this work, the foundation models like LLMs and VLMs play a crucial role in enhancing the reliability and efficiency of robotic task execution. LLMs aid in high-level planning and generating actionable constraints for low-level skills, facilitating rapid detection and recovery from misalignments. VLMs, on the other hand, act as constraint detectors, monitoring the execution process and providing precise feedback to ensure alignment between the plan and execution. This collaborative approach allows for immediate re-planning and correction, improving the overall success rate and reducing task completion time. |                                                              |
| 40    | Building Cooperative Embodied Agents Modularly with Large Language Models | 2024 | ICLR               | This paper addresses the challenge of multi-agent cooperation in environments requiring decentralized control, raw sensory observations, costly communication, and multi-objective tasks. The focus is on creating embodied agents that can plan, communicate, and collaborate to accomplish long-horizon tasks efficiently. The application is particularly relevant in scenarios like household tasks, where agents need to interact dynamically and make decisions based on partial observations and effective communication to achieve common goals. | The foundation model used in this paper is a Large Language Model (LLM), specifically leveraging the GPT-4 model. This model is integrated into a novel modular framework named Cooperative Embodied Language Agent (CoELA), which utilizes its rich world knowledge, reasoning abilities, and natural language understanding for effective multi-agent cooperation. Additionally, the paper explores fine-tuning techniques using LoRA (Low-Rank Adaptation) for optimizing the performance of the CoLLAMA model, derived from the LLAMA-2 foundation model, in collaborative tasks. | In this work, foundation models like LLMs play a crucial role in enhancing the cognitive capabilities of embodied agents. LLMs contribute by providing advanced language comprehension, reasoning, and dialogue generation, which are essential for planning and communication in multi-agent systems. The cognitive-inspired modular framework incorporates LLMs to perceive complex observations, maintain long-term memory, generate high-level plans, and execute actions efficiently. By harnessing these capabilities, the agents can perform better in cooperative tasks, achieve higher efficiency, and build trust in human-agent interactions, thus addressing key challenges in robotics and reinforcement learning. |                                                              |
| 41    | LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action | 2023 | CoRL               | The application focuses on robotic navigation using large pre-trained models of language, vision, and action. The system, LM-Nav, aims to enable mobile robots to follow high-level natural language instructions to navigate complex outdoor environments. The study demonstrates the ability of LM-Nav to execute long-horizon navigation tasks based on free-form textual instructions without requiring any fine-tuning or language-annotated navigational data. | GPT-3, a large language model (LLM) for parsing textual instructions into landmarks. CLIP, a vision-language model (VLM) for grounding textual landmarks in visual observations. ViNG, a visual navigation model (VNM) for constructing a topological map and executing navigation plans. | In this context, foundation models play a critical role in grounding textual instructions within the robot's visual observations. Specifically, CLIP is used to associate images with text, computing the likelihood that a given image corresponds to a textual landmark. This capability allows the robot to interpret and follow natural language instructions by matching described landmarks to its visual surroundings. The VLM facilitates the translation of language-based navigation commands into actionable visual targets, enabling the robot to navigate effectively in real-world environments based on high-level human instructions. The integration of VLMs with the LLM and VNM allows the LM-Nav system to create a comprehensive navigation plan that adheres to user instructions while optimizing the traversal path. |                                                              |
| 42    | Open-vocabulary Queryable Scene Representations for Real World Planning | 2023 | ICRA               | The application focuses on enabling robots to perform varied, real-world tasks by understanding and acting on diverse human commands within the context of their environment. The framework, NLMap, integrates natural language and visual-language models to create open-vocabulary, queryable scene representations that allow robots to understand and plan tasks based on the current state of their environment. This system aims to improve task planning and execution by enabling robots to query available objects and their locations in the scene before generating a context-conditioned plan. | Foundation models, specifically ViLD and CLIP, for generating and querying scene representations. Large Language Models (LLMs) for object proposal and planning. | In this context, Vision-Language Models (VLMs) play a crucial role in generating a queryable scene representation. VLMs like CLIP and ViLD are used to create embeddings for objects detected in the environment, allowing the scene representation to be queried with natural language. The VLMs provide open-vocabulary understanding and object detection capabilities, enabling the robot to identify and locate objects based on human commands. This information is then used by an LLM planner to generate and execute plans that are grounded in the actual context of the environment, significantly enhancing the robot's ability to perform long-horizon planning and real-world tasks. |                                                              |
| 43    | CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory | 2023 | ICRA               | The application focuses on building a queryable 3D scene representation for robotic tasks such as segmentation, instance identification, semantic search over space, and view localization. The proposed CLIP-Fields framework allows robots to create and use a spatial-semantic memory for various complex tasks in human environments. The system is designed to provide robots with a detailed understanding of their surroundings without relying on explicit human supervision. | CLIP (Contrastive Language-Image Pretraining) for capturing semantic information from images. Detic, an open-label object detection model for detecting objects and generating CLIP embeddings. Sentence-BERT for improving language query performance. | In this context, foundation models play a central role in creating a semantic understanding of the robot's environment. CLIP-Fields uses CLIP to generate embeddings that associate visual and language information, creating a semantic map that can be queried using natural language. The VLMs enable the robot to understand and navigate its environment based on semantic information derived from images, facilitating tasks such as semantic navigation, object search, and instance identification. The integration of VLMs allows the system to function with weak supervision from web-trained models, thus minimizing the need for extensive human annotations. |                                                              |
| 44    | Visual Language Maps for Robot Navigation                    | 2023 | ICRA               | The application focuses on improving robotic navigation by fusing pretrained visual-language model features with a 3D reconstruction of the physical world to create Visual Language Maps (VLMaps). These maps enable natural language indexing of the environment and enhance the robot's ability to follow complex language instructions for spatial navigation tasks. | Pretrained visual-language models such as LSeg, which is based on CLIP for generating semantic segmentation.Large Language Models (LLMs) for interpreting natural language commands and generating navigation goals. | In this context, foundation models are crucial for grounding language commands in visual observations and generating spatially precise navigation goals. The VLMs help create a semantic map that can be queried using natural language, allowing robots to navigate based on instructions like "move between the sofa and the TV" or "go to the right of the chair." These maps enable robots to understand and execute complex, zero-shot spatial navigation tasks without additional data collection or model fine-tuning. The integration of VLMs with LLMs further enhances the robot's ability to parse and act upon detailed language instructions, improving the efficiency and effectiveness of navigation in diverse environments. |                                                              |
| 45    | ConceptFusion: Open-set Multimodal 3D Mapping                | 2023 | ICRA               | The application focuses on creating open-set multimodal 3D maps from RGB images, depth estimates, and features from foundation models like CLIP, DINO, and AudioCLIP. These maps enable robots to query their environment using diverse modalities such as text, images, audio samples, and clicks on the 3D map. The aim is to enhance robotic navigation, planning, and interaction with objects in a scene by integrating semantic concepts with 3D spatial data in a zero-shot manner. | CLIP for aligning images and text. DINO for visual features. AudioCLIP for audio features. Additional segmentation models like SAM and Mask2Former for instance segmentation. | In this context, foundation models play a crucial role in providing semantic understanding and enabling multimodal queries within the 3D maps. The VLMs help map visual and linguistic information onto 3D spaces, allowing robots to understand and interact with their environment through natural language, images, and sounds. This facilitates complex spatial reasoning tasks and enables robots to perform tasks based on high-level semantic queries, enhancing their ability to navigate and manipulate objects in real-world environments without the need for additional training or fine-tuning. |                                                              |
| 46    | Fast Traversability Estimation for Wild Visual Navigation    | 2023 | ICRA               | The application focuses on real-time traversability estimation for robotic navigation in challenging natural environments such as forests and grasslands. The proposed system, Wild Visual Navigation (WVN), aims to enable robots to learn and adapt to traversable terrains quickly through online self-supervised learning based on visual inputs, allowing autonomous navigation in complex outdoor terrains. | Self-supervised visual transformer models, specifically DINO-ViT (Vision Transformer trained using the DINO method), for extracting high-dimensional visual features. | In this context, foundation models like self-supervised visual transformer models are utilized to extract meaningful semantic and instance features from visual inputs without requiring manual labels. These features are processed to estimate the traversability of terrains based on proprioception and control performance. The system leverages high-dimensional features to generate supervision signals in real-time, enabling the robot to distinguish between traversable and untraversable areas after a short human demonstration. This allows the robot to adapt quickly to new environments and perform tasks such as negotiating obstacles in high grass and following footpaths autonomously. |                                                              |
| 47    | HomeRobot: Open-Vocabulary Mobile Manipulation               | 2023 | CoRL               | The application focuses on enabling robots to perform open-vocabulary mobile manipulation (OVMM) tasks in household environments. The framework, HomeRobot, aims to benchmark and improve the performance of mobile manipulators in real-world settings, where the robot needs to navigate, perceive, and manipulate a wide variety of objects based on natural language instructions. | DETIC (DEtection Transformed by Image-level Supervision) for open-vocabulary object detection and segmentation. | In this context, foundation models like DETIC are crucial for enabling the robot to understand and interact with its environment based on natural language instructions. DETIC provides the robot with the ability to detect and segment a wide variety of objects, including those not seen during training, by leveraging large-scale image-level supervision. This capability allows the robot to follow instructions such as "move the toy animal from the chair to the table" by identifying and locating the specified objects and receptacles. The integration of VLMs enhances the robot's perception, enabling it to perform complex manipulation tasks in dynamic and unstructured environments. The HomeRobot framework demonstrates the effectiveness of combining VLMs with traditional robotic control and navigation systems to achieve robust and scalable mobile manipulation capabilities. |                                                              |
| 48    | Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation | 2023 | CoRL               | The application focuses on enhancing robotic manipulation tasks by leveraging 3D perceptual representations. The Act3D framework aims to improve the spatial precision of end-effector pose prediction in multi-task robotic manipulation tasks. The system is designed to perform complex manipulation tasks by representing the robot’s workspace using a 3D feature field with adaptive resolutions. | CLIP for pre-training 2D image features, which are then lifted into 3D using sensed depth information. | In this context, foundation models like CLIP are used to extract and featurize 2D image views, which are then integrated into a 3D feature field using sensed depth data. This integration allows the robot to construct a detailed 3D representation of its workspace, enabling high spatial precision in predicting 3D end-effector poses. The model leverages relative-position attention mechanisms to process these 3D features efficiently, facilitating fine-grained manipulation tasks and generalization across different environments and camera viewpoints. The use of VLMs like CLIP enhances the robot's ability to interpret visual data and perform complex tasks with high accuracy, significantly improving performance on benchmarks like RLBench. |                                                              |
| 49    | Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation | 2023 | CoRL               | The application focuses on enhancing robotic manipulation tasks using a few-shot learning approach. The goal is to enable robots to pick up novel objects based on a few demonstrations or natural language descriptions, leveraging distilled feature fields that integrate 3D geometry with rich semantic information from 2D vision-language models. | CLIP for extracting semantic features from images. DINO Vision Transformer (ViT) for providing dense visual descriptors. | In this context, foundation models like CLIP play a crucial role in providing the semantic understanding necessary for language-guided manipulation. The system uses CLIP features to interpret natural language commands and associate them with visual features in the robot's environment. By distilling these 2D features into a 3D feature field, the robot can leverage both the spatial and semantic information to perform manipulation tasks such as grasping and placing objects. This approach enables the robot to generalize to new objects and tasks that were not explicitly seen during training, demonstrating robust open-ended generalization capabilities. |                                                              |
| 50    | GNFactor Multi-Task Real Robot Learning with Generalizable Neural Feature Fields | 2023 | CoRL               | The application focuses on enabling robots to execute diverse manipulation tasks from visual observations in unstructured real-world environments. The system, GNFactor, aims to improve the generalization of robotic manipulation by using Generalizable Neural Feature Fields (GNF) and behavior cloning. GNFactor is evaluated on multiple real robot tasks and RLBench tasks to demonstrate its effectiveness and generalization capabilities. | Vision-language foundation models, specifically Stable Diffusion, for distilling rich semantic information into 3D voxel representations. | In this context, foundation models are used to provide semantic understanding and enhance the 3D volumetric representations. The VLMs, such as Stable Diffusion, are utilized to extract 2D features that are then distilled into a 3D voxel space, creating a rich and informative representation of the scene. This enables the robot to understand and manipulate objects based on both their 3D structure and semantic information. The GNFactor framework leverages these enhanced representations for policy learning, improving the robot's ability to perform multi-task manipulation in diverse and complex environments. The integration of VLMs allows the robot to interpret language instructions and execute tasks with greater accuracy and flexibility. |                                                              |
| 51    | SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities | 2024 | CVPR               | The application focuses on enhancing the spatial reasoning capabilities of Vision-Language Models (VLMs) to improve their performance in tasks requiring understanding of 3D spatial relationships. The proposed system, SpatialVLM, aims to generate and train on a large-scale spatial Visual Question Answering (VQA) dataset to improve the VLM's ability to perform both qualitative and quantitative spatial reasoning. This enhanced capability is particularly useful for applications in robotics and other fields requiring precise spatial understanding. | Vision-Language Models (VLMs) such as CLIP for filtering and generating semantic annotations from 2D images. Pre-trained expert models for object detection, depth estimation, semantic segmentation, and object-centric captioning. | In this context, foundation models are enhanced to perform direct spatial reasoning tasks by integrating 3D spatial information into their training. The VLMs are trained on a synthetic dataset generated using internet-scale real-world images, which includes spatial reasoning questions and answers. This allows the VLMs to answer complex spatial questions, make quantitative estimations, and perform spatial reasoning tasks that are critical for robotics applications. The enhanced VLMs can be used to generate rewards, perform success detection, and plan tasks in robotics by providing precise spatial information and reasoning capabilities. The integration of these spatially-aware VLMs with large language models (LLMs) further allows for complex chain-of-thought reasoning, enabling robots to understand and execute tasks with high spatial accuracy. |                                                              |
| 52    | OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics | 2024 | ICRA               | The application focuses on integrating various open-knowledge models for robotics to perform pick-and-drop tasks in real-world home environments. The OK-Robot framework combines vision-language models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, aiming to achieve high success rates in open-vocabulary mobile manipulation (OVMM). | CLIP for vision-language representation and object detection. OWL-ViT for open-vocabulary object detection. Segment Anything Model (SAM) for segmentation. AnyGrasp for generating grasp poses. | In this context, foundation models play a crucial role in providing semantic understanding and enabling the robot to detect and locate objects based on natural language queries. CLIP and OWL-ViT are used to identify objects in the environment and generate semantic embeddings stored in a VoxelMap, which serves as the robot's memory module. The VLMs enable zero-shot navigation to objects and support the robot in performing manipulation tasks by identifying and segmenting objects accurately. The integration of VLMs with navigation and grasping primitives allows OK-Robot to achieve a 58.5% success rate in cluttered environments and 82.4% in cleaner settings, demonstrating the effectiveness of combining open-knowledge models for complex robotic tasks. |                                                              |
| 53    | MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting | 2024 | ICRA               | The application focuses on enabling robots to perform open-vocabulary manipulation tasks specified by free-form language descriptions. The approach, Marking Open-vocabulary Keypoint Affordances (MOKA), employs vision-language models (VLMs) to solve manipulation tasks involving complex and diverse environments and task goals. The tasks include tool use, deformable body manipulation, and object rearrangement in real-world settings. | Vision-Language Models (VLMs) such as GPT-4V for affordance prediction and motion generation based on RGB images. GroundingDINO and SAM (Segment Anything Model) for object segmentation. | In this context, foundation models play a central role in bridging visual observations with robot motions. MOKA leverages VLMs to predict affordances and generate corresponding motions through a compact point-based representation. The VLMs are prompted to solve visual question-answering (VQA) problems, effectively converting affordance reasoning into a series of visual prompts. This enables the VLMs to generate motions by reasoning about keypoints and waypoints on 2D images, which are then translated into executable trajectories for the robot. The use of VLMs allows MOKA to perform diverse and complex manipulation tasks specified by natural language instructions, demonstrating robust performance in zero-shot and in-context learning scenarios. |                                                              |
| 54    | DNAct: Diffusion Guided Multi-Task 3D Policy Learning        | 2024 | arxiv              | The application focuses on multi-task object manipulation in complex environments. The DNAct framework integrates neural rendering pre-training and diffusion training to create a unified, semantic-aware, and multi-modal 3D representation. This representation enables robots to perform various manipulation tasks with enhanced generalization and robustness. | Stable Diffusion for extracting 2D semantic features. Neural Radiance Fields (NeRF) for constructing a 3D scene representation. | In this context, foundation models like Stable Diffusion are used to extract rich semantic features from 2D images, which are then distilled into a 3D space using NeRF. This process creates a comprehensive 3D semantic representation that is used to guide the robot's manipulation tasks. The VLMs provide semantic understanding and context, enabling the robot to interpret complex scenes and perform multi-task manipulation with improved accuracy and efficiency. The diffusion training further refines these representations by learning multi-modal features from the action sequences, enhancing the robustness and generalizability of the learned policies. |                                                              |
| 55    | Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation? | 2020 | L4DC               | The application focuses on the task specification for robot manipulation. Specifically, it aims to provide a low-effort and intuitive modality for task specification to engage non-expert end-users, enabling personalized robot agents in home environments. The study explores using various forms of goal specification, such as images from the internet, hand sketches, or simple language descriptions, to facilitate the task specification process. | ImageNet-supervised ResNet50 ImageNet-trained MoCo CLIP (a multi-modal embedding network that handles both visual and language goals) | The role of foundation models in this context is to enable zero-shot task specification and goal-conditioning in robot manipulation tasks. The proposed Zero Shot Task-specification (ZeST) framework utilizes foundation models to embed both the observation and goal specifications, compute features of these embeddings, and measure the similarity between them. High similarity indicates that the goal specification has been achieved. This approach aims to overcome the limitations of traditional goal-conditioned methods, which require compact state space goals or goal images from the same scene. By leveraging off-domain images and language instructions, ZeST aims to reduce the task specification burden and facilitate multi-task learning and policy execution in reinforcement learning settings. |                                                              |
| 56    | AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS | 2024 | arxiv              | The application focuses on orchestrating robotic agents for large-scale, autonomous data collection in diverse and unstructured real-world environments. The system, named AutoRT, aims to deploy operational robots in new scenarios with minimal human supervision, enabling them to perform various tasks based on high-level objectives and environmental affordances. The ultimate goal is to gather diverse robotic experience to improve robot learning and autonomy. | Vision-Language Models (VLMs) for scene understanding and grounding. Large Language Models (LLMs) for generating instructions and guiding data collection. | In the context of AutoRT, foundation models play a crucial role in scene understanding and grounding. They describe the environment by identifying objects and their locations, which allows the system to generate appropriate manipulation tasks. The VLMs enable the robots to comprehend their surroundings and provide the necessary context for the LLMs to propose tasks. The LLMs, guided by the VLMs' outputs, generate diverse and novel instructions for the robots to execute. This collaborative use of VLMs and LLMs allows AutoRT to perform tasks autonomously or under minimal supervision, ensuring the scalability and diversity of the collected data while adhering to safety and operational constraints. |                                                              |
| 57    | ViNT: A Foundation Model for Visual Navigation               | 2023 | CoRL               | The application focuses on visual navigation for mobile robotics. The goal is to develop a general-purpose pre-trained model, called Visual Navigation Transformer (ViNT), that can be adapted to various navigation tasks in different environments using visual inputs. ViNT aims to enable efficient navigation by leveraging a Transformer-based architecture to learn navigational affordances from diverse datasets. | Visual Navigation Transformer (ViNT), which is based on a Transformer architecture. ViNT is trained on a large and diverse set of robotic navigation datasets, enabling it to learn general navigational capabilities and adapt to new tasks and environments. | In this context, foundation models are utilized to enhance the navigation capabilities of mobile robots by providing a flexible and generalizable model that can handle various goal specifications. ViNT uses VLMs to interpret visual inputs and generate navigation plans based on those inputs. The model can adapt to different task specifications by fine-tuning or prompt-tuning, allowing it to be used in a wide range of navigation applications, from indoor mapping to kilometer-scale outdoor navigation. The VLM's role is to generalize across different environments and robot embodiments, enabling zero-shot deployment and efficient adaptation to new tasks with minimal additional data. |                                                              |
| 58    | Composing Pre-Trained Object-Centric Representations for Robotics From "What" and "Where" Foundation Models | 2024 | ICRA               | The application focuses on robotic manipulation in multi-object scenes. The paper introduces POCR (Pre-trained Object-Centric Representations) to enhance robot learning by leveraging pre-trained visual representations and object segmentation models. The goal is to improve the robot's ability to understand and manipulate objects in diverse environments by combining "what" (identity and properties of objects) and "where" (location and spatial relationships) information. | Segment Anything Model (SAM) for object segmentation ("where" information)   Pre-trained visual representations such as LIV (Language-Image Representations), R3M, and ImageNet models for encoding object properties ("what" information) | In this context, foundation models enhance robotic manipulation by providing a comprehensive understanding of objects and their spatial relationships within a scene. SAM generates segmentation masks to identify and locate objects, while pre-trained visual encoders describe object properties. This combination allows the POCR framework to create detailed and accurate representations of the environment, which are used for policy learning in robotic manipulation tasks. The VLMs enable robots to generalize across different tasks and environments, improving their ability to learn and perform complex manipulation tasks with minimal task-specific training data. |                                                              |
| 59    | Gesture-Informed Robot Assistance via Foundation Models      | 2023 | CoRL               | The application focuses on human-robot interaction, specifically using gestures to inform robot assistance in various tasks. The proposed system, GIRAF, aims to improve the interpretation of deictic gestures (e.g., pointing) in table-top manipulation tasks, enhancing the robot's ability to understand and respond to human instructions effectively. The system is evaluated on its ability to reason about diverse gestures and perform tasks based on human-robot collaboration. | Large Language Models (LLMs), such as GPT-3.5, for reasoning and task planning. Vision-Language Models (VLMs) for scene description and grounding. Expert models like MediaPipe for gesture detection and classification. | In this context, foundation models are used to describe the scene and detect objects, providing essential information for grounding gestures. The VLMs identify and label objects in the environment, while the LLMs reason about the gestures and generate robot policies based on multimodal instructions (language and gestures). VLMs help in grounding the context of the task, enabling the robot to interpret the meaning of gestures and perform the corresponding actions accurately. The GIRAF framework integrates these models to enable more natural and efficient human-robot interaction by leveraging gesture and language inputs. |                                                              |
| 60    | Towards A Unified Agent with Foundation Models               | 2023 | ICLR               | The application focuses on enhancing reinforcement learning (RL) agents using foundation models to tackle tasks in sparse-reward environments. The goal is to improve exploration efficiency, skill reuse, task scheduling, and learning from observations in robotic manipulation tasks. The framework is tested in a simulated robotic manipulation environment where the robot is tasked with stacking objects. | Large Language Models (LLMs), specifically FLAN-T5, for reasoning and generating sub-goals. Vision-Language Models (VLMs), specifically CLIP, for scene understanding and providing rewards based on visual observations. | In this context, foundation models play a pivotal role in describing the environment and providing intrinsic rewards for sub-goal completion. The VLMs map visual inputs to text descriptions, enabling the RL agent to understand and interact with its environment through language. This allows the agent to efficiently explore sparse-reward environments by breaking down tasks into sub-goals and using the VLM to verify if sub-goals are achieved, thus providing additional rewards. The VLMs also enable the agent to reuse data from past experiences and transfer learned skills to new tasks by identifying and relabeling successful trajectories from an offline buffer, significantly improving learning efficiency and scalability. |                                                              |
| 61    | History Compression via Language Models in Reinforcement Learning | 2022 | ICML               | The application focuses on reinforcement learning (RL) in partially observable environments. The paper introduces HELM (History comprEssion via Language Models), which leverages pre-trained language models for history compression to improve sample efficiency in RL. HELM is evaluated in environments like Minigrid and Procgen, where maintaining a memory of past observations is crucial for optimal decision-making. | Pretrained Language Transformer (PLT) specifically TransformerXL for history representation and compression. | In this context, foundation models are not directly used; instead, the focus is on using pre-trained language transformers (like TransformerXL) to compress history in RL tasks. The pre-trained language model captures and compresses past observations, creating a compact representation that improves the RL agent's ability to make decisions based on past experiences. The proposed HELM framework integrates this compressed history into actor-critic network architectures, enhancing sample efficiency and performance in partially observable environments. The TransformerXL helps transform partially observable Markov decision processes (POMDPs) into Markov decision processes (MDPs) by providing a comprehensive history representation, facilitating better learning and decision-making for RL agents. |                                                              |
| 62    | Semantic HELM: A Human-Readable Memory for Reinforcement Learning | 2023 | NeurIPS            | The application focuses on reinforcement learning (RL) in partially observable environments. The paper introduces Semantic HELM (SHELM), a novel memory mechanism that represents past events in human-readable language to enhance interpretability and performance. SHELM is evaluated in various environments, including MiniGrid, MiniWorld, Avalon, and Psychlab, where maintaining a memory of past observations is crucial for optimal decision-making. | CLIP for associating visual inputs with language tokens. Pre-trained language models, such as TransformerXL, for memory representation and compression. | In this context, foundation models play a crucial role in mapping visual observations to language tokens. CLIP is used to embed visual inputs and retrieve semantically related language tokens, which are then passed to a pre-trained language model serving as the memory module. This process allows the RL agent to maintain a coherent and human-readable representation of past events, enhancing the agent's ability to perform tasks that require memory. The VLMs enable the agent to understand and interpret visual inputs in a way that is comprehensible to humans, facilitating better troubleshooting and refinement of the memory mechanism. The human-readable memory component significantly improves the interpretability and performance of RL agents in partially observable environments. |                                                              |
| 63    | Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning | 2024 | ICLR               | The application focuses on using pre-trained Vision-Language Models (VLMs) as zero-shot reward models for reinforcement learning (RL) tasks. The study explores how VLMs can specify tasks via natural language descriptions and provide reward signals without the need for manually engineered reward functions. This approach is validated in environments like MuJoCo, where a humanoid robot is trained to perform complex tasks using natural language prompts. | CLIP (Contrastive Language-Image Pretraining), a vision-language model used to compute rewards based on the cosine similarity between the current environment state and a natural language task description. | In this context, foundation models are used as reward models to guide the RL agent's behavior. The VLMs, specifically CLIP, provide reward signals based on the similarity between a task description and the current state of the environment. This method allows the RL agent to learn complex tasks without manually specified reward functions. The VLMs are crucial in translating natural language prompts into meaningful rewards, facilitating zero-shot task specification and enabling the agent to learn tasks such as kneeling, sitting in a lotus position, doing the splits, and more. The study demonstrates the effectiveness of VLM-RMs in various RL benchmarks and highlights the scalability and robustness of VLMs in providing reward signals for RL agents. |                                                              |
| 64    | VISION-LANGUAGE FOUNDATION MODELS AS EFFECTIVE ROBOT IMITATORS | 2024 | ICLR               | The application focuses on utilizing vision-language foundation models (VLMs) for robotic manipulation tasks. The proposed framework, RoboFlamingo, aims to leverage existing VLMs, specifically OpenFlamingo, to perform language-conditioned manipulation tasks through imitation learning. The study evaluates RoboFlamingo's performance on tasks that require understanding and executing sequential instructions for manipulating objects. | OpenFlamingo, a vision-language model that integrates vision and language understanding. The framework is built on pre-trained VLMs, utilizing models like MPT (MosaicML Pretrained Transformers) and LLaMA (Large Language Model) for different configurations. | In this context, foundation models play a critical role in understanding and executing language-conditioned manipulation tasks. RoboFlamingo uses pre-trained VLMs to comprehend visual observations and language instructions at each decision step. The model decouples vision-language understanding from decision-making by adding a policy head that processes the fused representations from VLMs to predict robot actions. This approach allows the model to handle open-loop control and deploy on low-performance platforms, making it a cost-effective and flexible solution for robotic manipulation. The VLMs provide the capability to generalize across various tasks and environments, achieving state-of-the-art performance in benchmarks like CALVIN for long-horizon language-conditioned tasks. |                                                              |
| 65    | Learning Generalizable Feature Fields for Mobile Manipulation | 2024 | arxiv              | The paper addresses the challenge of open-vocabulary navigation and mobile manipulation tasks for robots. Specifically, it focuses on enabling robots to navigate and manipulate objects in diverse and dynamic real-world environments using language instructions. | Generalizable Neural Radiance Fields (Gen-NeRF) combined with CLIP feature distillation. The model, named GeFF (Generalizable Feature Fields), is designed to provide a unified scene representation for both navigation and manipulation by encoding geometric and semantic information from an RGB-D stream. | The foundation models in this work is used to align the rich scene priors obtained from the neural feature fields with natural language. This is achieved through feature distillation from the pre-trained CLIP model, allowing the robot to understand and act upon language instructions. The VLM enables the robot to perform real-time, language-conditioned navigation and manipulation tasks by decoding the scene representation into actionable information for the robot. |                                                              |
| 66    | Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers | 2022 | IROS               | The paper addresses the challenge of reshaping existing robot trajectories based on natural language commands. The focus is on improving human-robot collaboration by providing an intuitive interface that allows users to modify robot trajectories according to specific semantic or safety constraints. | The foundation models used in this paper are BERT and CLIP. These pre-trained language models are employed to encode user commands and object semantics, which are then combined with trajectory information using a multi-modal attention transformer. | The primary application discussed in this paper is reshaping robot trajectories using natural language commands. This involves creating a flexible language-based interface for human-robot collaboration that allows users to modify existing robot trajectories according to specific operational constraints and user commands. | The foundation models used in this work are BERT and CLIP. These models are leveraged to encode user commands and align them with trajectory information through a multi-modal attention mechanism. |
| 67    | Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language | 2023 | ICLR               | The primary application discussed in this paper is the creation of Socratic Models (SMs) for multimodal reasoning and dialogue. These models enable tasks such as zero-shot image captioning, video-to-text retrieval, answering free-form questions about egocentric video, multimodal assistive dialogue, and robot perception and planning. | The foundation models used in this work include large pretrained models such as BERT, GPT-3, and CLIP. These models are utilized to handle various data modalities including text, images, and audio, and are combined through a modular framework to perform complex multimodal tasks. | In the context of robotics, foundation models play a key role in perception-driven planning and multimodal reasoning. For example, visual-language models (VLMs) can be used for object detection and scene understanding, while large language models (LLMs) can generate natural language instructions or code for robot actions. This allows robots to perform language-conditioned tasks and interact with users in a more intuitive and flexible manner. The SMs framework leverages the strengths of these pretrained models to enable robots to parse and generate plans from free-form human instructions, enhancing their ability to perform complex tasks in diverse environments. |                                                              |
| 68    | Correcting Robot Plans with Natural Language Feedback        | 2022 | arxiv              | The primary application discussed in this paper is correcting robot plans with natural language feedback. This involves enabling users to provide corrections in the form of natural language to update the robot's cost function during task execution, allowing the robot to adapt its behavior based on user instructions and preferences. | The foundation models used in this work include CLIP for encoding visual and language inputs. The paper leverages a pre-trained CLIP model to process natural language corrections and visual observations, which are then integrated with a U-Net architecture to generate cost maps. | The foundation models play a crucial role in interpreting natural language feedback and integrating it with visual observations to update the robot's cost function. By using CLIP and U-Net, the system can generate cost maps that reflect the corrections specified by the user, allowing the robot to modify its trajectory and behavior in real-time. This approach leverages the generalization capabilities of pre-trained models to handle diverse and novel instructions, enhancing the robot's ability to adapt to new environments and tasks based on user feedback. |                                                              |
| 69    | Housekeep: Tidying Virtual Households using Commonsense Reasoning | 2022 | ECCV               | The primary application discussed in this paper is tidying virtual households using commonsense reasoning. The goal is for an embodied agent to rearrange misplaced objects in a household environment without explicit instructions, based on human preferences for object placements. | The foundation model used in this work includes a large language model (LLM) like RoBERTa, which is fine-tuned on a dataset of human preferences for object placements. The model is used to capture the commonsense reasoning necessary for determining the correct locations of objects in a household. | The foundation model plays a critical role in enabling the agent to understand and apply commonsense reasoning for household tidying tasks. The LLM is used to predict the most likely locations for objects based on human preferences, thus guiding the agent's rearrangement actions. The model helps the agent generalize to unseen objects and environments, allowing it to perform tidying tasks effectively without explicit instructions. The modular approach integrates planning, exploration, and navigation, leveraging the LLM to ensure that the agent can adapt to various household scenarios and object placements. |                                                              |
| 70    | Code as Policies: Language Model Programs for Embodied Control | 2023 | ICRA               | The primary application discussed in this paper is the use of large language models (LLMs) for embodied control in robotics. Specifically, the paper explores how LLMs can be used to write robot policy code from natural language commands, enabling robots to perform various tasks such as object manipulation and navigation based on these commands. | The foundation model used in this work includes large language models (LLMs) trained on code completion, such as OpenAI Codex. These models are capable of generating Python code that can control robot behavior by processing perception outputs and parameterizing control primitives. | In this work, the foundation model plays a central role by translating natural language commands into executable robot policy code. The LLMs generate code that can express functions or feedback loops, process perception outputs, and parameterize control primitive APIs. This approach allows the robots to perform spatial-geometric reasoning, generalize to new instructions, and execute precise actions based on ambiguous descriptions, effectively bridging the gap between high-level natural language instructions and low-level robot control. |                                                              |
| 71    | VIMA: General Robot Manipulation with Multimodal Prompts     | 2023 | ICRA               | The primary application discussed in this paper is generating situated robot task plans using large language models (LLMs). The goal is to create plans that can be directly executed by robots in various environments, taking into account the available actions and objects in the environment. | The foundation model used in this work is a large language model (LLM) such as GPT-3. The LLM is prompted with program-like specifications of the available actions and objects in the environment and example programs to generate action sequences. | The foundation model plays a crucial role in scoring potential next actions during task planning and generating action sequences directly from natural language instructions. By leveraging programmatic prompts, the LLM can generate executable plans that include conditional reasoning and error-correction based on state feedback from the environment. This approach ensures that the generated plans are feasible and aligned with the robot's capabilities and the specific environmental context. |                                                              |
| 72    | VIMA: General Robot Manipulation with Multimodal Prompts     | 2022 | arxiv              | The primary application discussed in this paper is general robot manipulation using multimodal prompts. The goal is to create a generalist robot agent capable of performing a wide range of manipulation tasks by processing multimodal prompts that include interleaved textual and visual tokens. | The foundation model used in this work is a transformer-based model called VisuoMotor Attention model (VIMA). VIMA leverages a pre-trained language model (T5) for encoding multimodal prompts and an object-centric approach using Mask R-CNN and Vision Transformer (ViT) for processing visual information. | The foundation model plays a crucial role in interpreting and integrating multimodal prompts to generate robot actions. VIMA processes textual and visual inputs, encodes them using a transformer-based architecture, and decodes motor actions autoregressively. This allows the robot to understand complex task specifications, adapt to new tasks, and perform manipulation tasks with high generalization capabilities. The model's ability to scale with both data and model size enables it to outperform state-of-the-art methods in zero-shot generalization settings. |                                                              |
| 73    | "No, to the Right" – Online Language Corrections for Robotic Manipulation via Shared Autonomy | 2023 | HRI                | The primary application discussed in this paper is online language corrections for robotic manipulation via shared autonomy. The framework, named LILAC (Language-Informed Latent Actions with Corrections), enables users to provide real-time language corrections during robot task execution, facilitating precise and adaptive control over the robot. | The foundation model used in this work includes Distil-RoBERTa for encoding language instructions and corrections. Additionally, GPT-3 is employed to determine the context-dependence of language utterances, which aids in refining the control space for the robot based on the given instructions. | The foundation models play a critical role in understanding and integrating natural language corrections into the robot's control system. Distil-RoBERTa processes the language inputs, while GPT-3 provides context-dependence gating, ensuring that corrections are interpreted correctly and applied in real-time. This allows the robot to adapt its actions based on user feedback, enabling more precise and flexible manipulation tasks. The shared autonomy paradigm splits control between the human and the robot, with the language model facilitating effective communication and control refinement. |                                                              |
| 74    | Diffusion-based Generation, Optimization, and Planning in 3D Scenes | 2023 | CVPR               | The primary application discussed in this paper is generation, optimization, and planning in 3D scenes using a diffusion-based approach. The paper introduces the SceneDiffuser, a conditional generative model designed to handle various tasks such as human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. | The foundation model used in this work is a diffusion-based generative model. SceneDiffuser leverages the diffusion process for scene-conditioned generation, optimization, and planning, which helps in traversing sufficient scene-conditioned distribution modes and integrates physics-based objectives to ensure physically plausible and goal-oriented trajectories. | The foundation model plays a crucial role in unifying the generation, optimization, and planning processes in 3D scenes. By employing a diffusion-based approach, SceneDiffuser mitigates the discrepancies among different modules and addresses the posterior collapse problem seen in previous scene-conditioned generative models. The model's iterative sampling strategy allows for joint formulation of scene-aware generation, physics-based optimization, and goal-oriented planning, enabling more consistent and plausible results in complex 3D tasks. |                                                              |
| 75    | ChatGPT for Robotics: Design Principles and Model Abilities  | 2024 | IEEE Access        | The primary application discussed in this paper is using ChatGPT for various robotics tasks by leveraging its natural language processing capabilities. The paper explores how ChatGPT can be integrated into robotics workflows to handle tasks such as code generation, task planning, and interactive dialogues to control robotic systems. | The foundation model used in this work is OpenAI's ChatGPT, which is a large language model trained to generate human-like text based on the input it receives. ChatGPT's ability to understand and generate text allows it to be adapted for different robotics tasks, providing a versatile tool for interaction and control. | ChatGPT plays a crucial role in interpreting user instructions, generating code, and facilitating interactive dialogues to perform and refine robotics tasks. By leveraging its language understanding capabilities, ChatGPT can convert high-level natural language commands into executable code, assist in task planning, and adapt to new instructions through iterative feedback. This enables a more intuitive and flexible approach to programming and controlling robotic systems, making it accessible even to non-technical users. |                                                              |
| 76    | Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control | 2023 | arxiv              | The primary application discussed in this paper is guiding text generation with grounded models for embodied agents. The goal is to create a system that can leverage the semantic knowledge of large language models (LLMs) while ensuring the generated action sequences are feasible and grounded in the physical capabilities and constraints of the robots. | The foundation model used in this work is a large language model (LLM) trained to predict text sequences, complemented by grounded models that provide state-dependent probabilities for specific tokens. The LLM used for this purpose includes models like GPT-3, which are capable of generating coherent and contextually relevant text based on given prompts. | The foundation model plays a central role in generating high-probability action sequences based on semantic knowledge, while the grounded models ensure these sequences are feasible and safe for the robot to execute. The paper proposes a decoding strategy that integrates the token probabilities from the LLM with the grounded model objectives, allowing the system to generate text that is both semantically meaningful and physically realizable. This approach enables robots to perform complex, long-horizon tasks by leveraging the strengths of both LLMs and grounded models. |                                                              |
| 77    | TidyBot: Personalized Robot Assistance with Large Language Models | 2023 | Autonomous Robots  | The primary application discussed in this paper is personalized robot assistance for household cleanup tasks using large language models (LLMs). The system, named TidyBot, is designed to tidy up rooms by picking up objects and putting them away according to user-specific preferences. | The foundation models used in this work include large language models (LLMs) such as GPT-3. These models are employed to generalize user preferences from a few examples and generate personalized rules for object placement. | The foundation model plays a crucial role in summarizing user preferences and generating generalized rules for object placement. By leveraging the summarization capabilities of LLMs, the system can infer personalized rules from a small number of examples, allowing the robot to understand and follow user-specific preferences. The LLMs enable the robot to interpret and apply these generalized rules to new objects and scenarios, enhancing the robot's ability to perform personalized household cleanup tasks effectively. |                                                              |
| 78    | Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model | 2023 | arxiv              | The primary application discussed in this paper is mapping multi-modal instructions to robotic actions using a large language model (LLM). The system, named Instruct2Act, is designed to translate complex high-level instructions into precise policy codes that control robotic manipulation tasks. This includes tasks such as object manipulation, visual goal-reaching, and visual reasoning in tabletop manipulation domains. | The foundation models used in this work include large language models (LLMs) like GPT-3, along with visual foundation models such as the Segment Anything Model (SAM) and CLIP. These models are employed to encode both the visual and textual inputs, enabling the robot to understand and execute multi-modal instructions effectively. | The foundation models play a central role in the Instruct2Act framework by generating executable Python programs from multi-modal instructions. The LLM interprets the instructions and generates code that invokes various perception models (SAM for segmentation and CLIP for object classification) to understand the environment. This information is then used to generate precise action sequences that the robot can execute. The integration of LLMs with visual models allows the system to generalize across different tasks and environments with strong zero-shot performance. |                                                              |
| 79    | Robots That Ask For Help:Uncertainty Alignment for Large Language Model Planners | 2023 | arxiv              | The primary application discussed in this paper is uncertainty alignment for large language model (LLM) planners in robotics. The framework, named KNOWNO, aims to enable robots to know when they don’t know and seek human help when necessary. This approach is designed to improve the efficiency and autonomy of robots in complex multi-step planning tasks while providing statistical guarantees on task completion. | The foundation models used in this work are large language models (LLMs) like PaLM-2L and GPT-3. These models are leveraged to generate possible next steps for the robot based on natural language instructions and observations from the environment. Conformal prediction is then used to align the uncertainty of these LLMs. | In this framework, the foundation models play a central role in generating and predicting the next steps in a robot's plan based on natural language instructions. The LLMs provide the initial set of possible actions and their confidence scores. Conformal prediction is then used to calibrate these confidence scores and generate a prediction set. If the set contains more than one high-confidence action, the robot asks for human help to resolve the ambiguity. This combination of LLMs and conformal prediction helps ensure that the robot operates with calibrated confidence, reducing the likelihood of making incorrect decisions autonomously and minimizing the need for human intervention. |                                                              |
| 80    | RoCo: Dialectic Multi-Robot Collaboration with Large Language Models | 2023 | arxiv              | The primary application discussed in this paper is dialectic multi-robot collaboration using large language models (LLMs). The proposed approach, RoCo, leverages LLMs for both high-level communication among robots and low-level path planning. The system enables robots to discuss and reason task strategies collectively, generate sub-task plans, and plan trajectories for multi-robot manipulation tasks. | The foundation models used in this work include large language models (LLMs) such as GPT-3 or GPT-4. These models are employed to facilitate dialogue-based task coordination and generate sub-task plans and waypoints for the robots. | The foundation models play a crucial role in enabling high-level task coordination and low-level motion planning through dialogue-based interactions. The LLMs are used to generate multi-agent dialogues that help robots discuss task strategies, propose sub-task plans, and improve plans based on environmental feedback. The LLMs also assist in planning 3D spatial paths for the robots, reducing the complexity of the motion planning process. This approach enhances the interpretability, flexibility, and adaptability of multi-robot systems in various task scenarios. |                                                              |
| 81    | SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning | 2023 | CoRL               | The primary application discussed in this paper is grounding large language models (LLMs) using 3D scene graphs for scalable robot task planning. The system, named SayPlan, is designed to facilitate large-scale task planning for robots in complex environments, including multi-floor and multi-room settings. | The foundation model used in this work is a large language model (LLM) such as GPT-4. This model is employed to interpret and generate task plans based on natural language instructions. The LLM works in conjunction with a 3D scene graph representation of the environment to understand spatial and semantic relationships. | The foundation model plays a crucial role in generating and refining task plans grounded in the physical environment. SayPlan utilizes the LLM to perform semantic searches within a collapsed 3D scene graph to identify relevant subgraphs necessary for task completion. The LLM generates high-level plans that are further refined through an iterative replanning process, incorporating feedback from a scene graph simulator to ensure feasibility and correctness. This approach allows the system to handle complex, long-horizon tasks by leveraging the LLM's reasoning capabilities and the detailed spatial information provided by the 3D scene graphs. |                                                              |
| 82    | Video Language Planning                                      | 2024 | ICLR               | The primary application discussed in this paper is enabling visual planning for complex long-horizon tasks using video and language models. The algorithm, named Video Language Planning (VLP), aims to generate detailed multimodal specifications (video and language) to complete tasks such as multi-object rearrangement and bi-arm dexterous manipulation. | The foundation models used in this work include vision-language models (VLMs) for policies and value functions, and text-to-video models for dynamics modeling. These models are trained on large-scale internet data to understand and generate both visual and textual descriptions. | The foundation models play crucial roles in VLP by generating high-level text actions and predicting low-level outcomes through video simulations. VLMs are used as policies to infer possible next-step actions and as heuristic functions to evaluate the favorability of action sequences. The text-to-video models simulate the resulting state of these actions, allowing for the generation of long-horizon video plans that guide the robot in executing complex tasks. This approach integrates the abstract planning capabilities of language models with the detailed dynamics understanding of video models, enabling robots to perform long-horizon tasks more effectively. |                                                              |
| 83    | Zero-shot robotic manipulation with pretrained image-editing diffusion models | 2024 | ICLR               | The primary application discussed in this paper is zero-shot robotic manipulation using pretrained image-editing diffusion models. The system, named SuSIE (SUbgoal Synthesis via Image Editing), leverages a high-level planner to propose intermediate subgoals for a low-level controller to achieve, enabling the robot to perform complex manipulation tasks in novel environments. | The foundation model used in this work is InstructPix2Pix, a pretrained image-editing diffusion model. This model is fine-tuned on video data, including human videos and robot rollouts, to generate hypothetical future subgoal observations based on the robot's current observation and a language command. | The foundation model plays a crucial role in generating intermediate subgoals that guide the robot's low-level goal-conditioned policy. By leveraging the semantic understanding from Internet-scale pretraining and visual understanding capabilities, the diffusion model generates subgoals that enable the robot to achieve better generalization and precision in manipulation tasks. This decoupling of high-level planning and low-level control allows the robot to operate effectively in novel scenarios and handle tasks involving new objects and environments. |                                                              |
| 84    | Creative Robot Tool Use with Large Language Models           | 2023 | CoRL               | The primary application discussed in this paper is creative robot tool use using large language models (LLMs). The system, named RoboTool, aims to imbue robots with the ability to creatively use tools to solve tasks that involve implicit physical constraints and long-term planning. This includes tasks that are difficult to achieve without innovative and flexible tool use, such as grasping objects out of reach or traversing obstacles. | The foundation model used in this work includes GPT-4, a large language model. RoboTool leverages GPT-4 to interpret natural language instructions, discern key task-related concepts, and generate executable code for controlling robots in both simulated and real-world environments. | The foundation model plays a crucial role in RoboTool by handling the natural language processing and reasoning required for creative tool use. GPT-4 is used across four main components: Analyzer: Interprets natural language to identify key concepts and task-related constraints. Planner: Generates comprehensive strategies based on the identified key concepts and language input. Calculator: Computes parameters for each skill, such as target positions. Coder: Translates plans into executable Python code. These components enable RoboTool to comprehend explicit or implicit physical constraints and environmental factors, demonstrating creative and efficient tool use in various tasks. By leveraging the knowledge encoded in LLMs, RoboTool offers a flexible and user-friendly solution for complex robotics tasks that traditionally rely on explicit optimization-based methods. |                                                              |
| 85    | Generative Expressive Robot Behaviors using Large Language Models | 2024 | HRI                | The primary application discussed in this paper is generating expressive robot behaviors using large language models (LLMs). The goal is to enable robots to demonstrate expressive behaviors that are adaptable to different social contexts and user preferences, enhancing human-robot interaction. | The foundation models used in this work include large language models (LLMs) like GPT-4. The system, named Generative Expressive Motion (GenEM), leverages LLMs for generating expressive behaviors based on language instructions and iterative human feedback. | The foundation model plays a central role in interpreting language instructions, performing social reasoning, and generating control code for the robot. The LLMs enable the robot to understand social norms and generate behaviors that are adaptable and composable. By using few-shot chain-of-thought prompting, the LLM translates human language instructions into parametrized control code, allowing the robot to perform expressive behaviors that can be refined through user feedback. This modular approach ensures that the generated behaviors are socially appropriate and can be adjusted based on real-time interactions. |                                                              |
| 86    | PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs | 2024 | ICML               | The primary application discussed in this paper is enabling vision-language models (VLMs) to perform zero-shot robotic control and spatial inference tasks. The system, named PIVOT (Prompting with Iterative Visual Optimization), aims to solve spatial reasoning tasks, such as robotic navigation and manipulation, by iteratively refining visual prompts to elicit actionable knowledge from VLMs. | The foundation models used in this work are vision-language models (VLMs) such as GPT-4V and Gemini. These models are utilized to understand and process visual annotations and textual prompts, enabling them to handle spatial reasoning and robotic control tasks without task-specific fine-tuning. | The foundation models play a crucial role in interpreting visual annotations and selecting optimal actions iteratively. PIVOT leverages VLMs to generate candidate robot actions, which are visualized as annotations on an image. The VLM then evaluates these actions, and the process iteratively refines the action set until an optimal solution is found. This approach allows the system to perform zero-shot control of robotic systems, navigation, manipulation, and other spatial inference tasks, demonstrating the potential and limitations of using VLMs for such applications without in-domain training data. |                                                              |
| 87    | SayTap: Language to Quadrupedal Locomotion                   | 2023 | CoRL               | The primary application discussed in this paper is mapping natural language commands to quadrupedal locomotion using large language models (LLMs). The system, named SayTap, is designed to enable quadrupedal robots to follow diverse locomotion instructions provided by users through natural language, allowing for flexible and interactive control of quadrupedal robot behaviors. | The foundation model used in this work is a large language model (LLM). The model is prompted with human commands and translates them into desired foot contact patterns, which are then used by a Deep Reinforcement Learning (DRL) based controller to generate low-level motor commands for the robot. | The foundation model plays a crucial role in translating natural language instructions into structured foot contact patterns that can be understood by the locomotion controller. This involves generating contact patterns from user instructions, which are then used by the DRL-based controller to achieve the specified locomotion behaviors. The LLM helps bridge the gap between high-level human commands and low-level robotic actions, enabling the robot to perform complex and diverse locomotion tasks based on natural language input. |                                                              |
| 88    | Language to Rewards for Robotic Skill Synthesis              | 2023 | CoRL               | The primary application discussed in this paper is synthesizing robotic skills from natural language instructions using large language models (LLMs). The approach aims to convert high-level language descriptions into reward functions that can be optimized to generate low-level control policies, enabling robots to perform complex tasks such as locomotion and dexterous manipulation. | The foundation model used in this work is a large language model (LLM) trained to understand and generate text, specifically GPT-4. This model is utilized to translate natural language instructions into parameterized reward functions, which are then optimized by a model predictive control (MPC) framework to generate appropriate robotic actions. | The foundation model plays a crucial role in interpreting natural language commands and generating reward functions that represent the desired robotic behavior. By leveraging the LLM's capabilities, the system can produce reward functions that are then optimized in real-time using MuJoCo MPC, enabling the robot to perform a wide range of tasks. This method bridges the gap between high-level language instructions and low-level robotic actions, providing a flexible and data-efficient approach to skill synthesis in robotics. |                                                              |
| 89    | Reasoning about the Unseen for Efficient Outdoor Object Navigation | 2023 | arxiv              | The primary application discussed in this paper is efficient outdoor object navigation for robotic agents using large language models (LLMs). The proposed system, Reasoned Explorer, aims to improve outdoor navigation by enabling robots to handle the complexities of real-world outdoor environments, such as parks or search and rescue scenarios, where spatial delineations are ambiguous and semantic ambiguities exist. | The foundation models used in this work are large language models (LLMs) such as GPT-4. The system leverages LLMs to reason about possible future scenarios and generate feasible navigation plans in outdoor environments. Additionally, vision-language models (VLMs) are used for processing visual information and generating textual descriptions of the environment. | The foundation models play a crucial role in enabling the robot to reason about the unseen and plan efficient navigation paths in outdoor settings. LLMs are used in two capacities: Visionary LLM: Projects future agent states and potential scenarios to anticipate navigation paths. Evaluator LLM: Assesses the feasibility of achieving the navigation goals within those projected scenarios. This dual LLM approach allows the system to iteratively refine navigation plans by expanding imaginary nodes in space using a Rapidly-exploring Random Tree (RRT) method. This process enhances the agent's decision-making capabilities, ensuring efficient and successful navigation in complex and dynamic outdoor environments. The method also introduces a new metric, the Computationally Adjusted Success Rate (CASR), to balance planning costs and computational efficiency. |                                                              |
| 90    | Eureka: Human-Level Reward Design via Coding Large Language Models | 2024 | ICLR               | The primary application discussed in this paper is human-level reward design for reinforcement learning (RL) tasks using large language models (LLMs). The system, named EUREKA, is designed to generate and optimize reward functions for a variety of RL tasks, including complex dexterous manipulation tasks such as pen spinning with a simulated Shadow Hand. | The foundation models used in this work include state-of-the-art coding LLMs such as GPT-4. These models are leveraged for their zero-shot generation, code-writing, and in-context improvement capabilities to perform evolutionary optimization over reward code. | The foundation model plays a crucial role in generating and refining reward functions that are used to train RL policies. EUREKA utilizes the LLMs to zero-shot generate executable reward functions based on the environment's source code and task description. The LLM then iteratively improves the reward functions through an evolutionary search process that involves reward sampling, evaluation, and reflection. This process allows EUREKA to generate reward functions that outperform human-engineered rewards in a diverse set of RL environments, thereby facilitating the acquisition of complex skills via reinforcement learning. |                                                              |
| 91    | LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model | 2023 | arxiv              | The application of autonomous robot control. Specifically, it focuses on generating complex robot behavior based on an operator’s commands. This approach can be applied to various fields such as mobile robotics, drone operation, robot manipulator systems. | Transformer-based Large Language Model (LLM) specifically fine-tuned from the Stanford Alpaca 7B model. The model is trained on 8.5k instruction-following demonstrations to generate robot behavior trees (BTs) from textual descriptions provided by human operators. | The LLM plays a critical role in interpreting human operator descriptions and generating corresponding robot behavior trees (BTs). The fine-tuned LLM can accurately create structured and logical BTs that are executable by robots. This enables robots to manage tasks and instructions that were not part of their initial training set, demonstrating the flexibility and capability of LLMs in enhancing autonomous robot control and human-robot interaction. | Solves the problem of creating complex and logical robot behavior trees (BTs) from human operator descriptions. Traditionally, designing BTs requires technical expertise and is time-consuming. The proposed method automates this process, making it accessible for non-technical specialists and efficient for implementation in robots with limited onboard computational resources. |
| 92    | Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning | 2023 | ICML               | The application of leveraging LLMs to improve the efficiency and generalization capabilities of agents in reinforcement learning (RL) tasks. Specifically, it investigates how LLMs can be functionally grounded to act as policies in interactive environments, thereby enabling agents to perform complex decision-making tasks more effectively. | The FLAN-T5 variant, a specific architecture of LLMs fine-tuned for various tasks. The study explores different sizes and configurations of FLAN-T5 to assess their effectiveness in RL environments and their ability to generalize knowledge across different tasks. | LLMs play the role of adaptive policies that guide agents through interactive textual environments. By leveraging online reinforcement learning, the LLMs are progressively updated as the agent interacts with the environment. This functional grounding approach allows the LLMs to improve their decision-making capabilities, enhance sample efficiency, and generalize knowledge to new tasks and objects, thereby addressing key challenges in robotics and RL. | Solves the critical problem of the misalignment between the abstract knowledge embedded in LLMs and the concrete requirements of interactive environments. Traditional LLMs struggle with grounding their knowledge to perform practical tasks in an RL setting, where understanding and interacting with the environment in real-time is crucial. The study aims to enhance the functional competence of LLMs in these scenarios. |
| 93    | Large Language Models as Commonsense Knowledge for Large-Scale Task Planning | 2023 | arxiv              | LLMs in robotic task planning, particularly in the context of daily household tasks. It aims to enhance the ability of robots to understand and execute complex, multi-step instructions provided in natural language by leveraging the commonsense knowledge embedded in LLMs. | The method employed in this paper integrates LLMs with Monte Carlo Tree Search (MCTS). The LLMs provide commonsense knowledge and generate possible world states, which MCTS uses to guide the search process, reducing complexity and improving decision-making efficiency. | LLMs serve a dual role: they act as a commonsense model of the world and as a heuristic policy for guiding search algorithms like MCTS. By using LLMs to sample likely states and provide heuristic guidance, the method significantly reduces the search space and enhances the robot's ability to reason about and execute complex tasks based on natural language instructions. | The specific problem addressed by this paper is the challenge of planning complex tasks in robotics, where natural language instructions often contain ambiguities and require the robot to infer intermediate steps to achieve the final goal. Traditional approaches struggle with the vast search space and the need for extensive training data to cover all possible scenarios. |
| 94    | AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation | 2023 | arxiv              | Robot manipulation tasks, such as constructing a smiley face using building blocks. These tasks involve high-level cognitive capabilities requiring complex multi-step reasoning and perception. The framework developed in this paper aims to enhance robots' ability to understand and execute detailed language instructions for such tasks, ultimately improving their performance in various real-world applications like household chores, manufacturing, and healthcare. | Employs Large Language Models (LLMs), particularly GPT-4, as the foundation model. These models are utilized to generate detailed multi-round prompt designs and sub-task plans, which are crucial for guiding the robot through the complex manipulation tasks. The framework named CogLoop incorporates these models to enhance the robot's reasoning and execution capabilities. | The foundation model (GPT-4) plays a critical role in generating high-quality task plans and sub-task sequences that are necessary for robot manipulation tasks. By leveraging the LLMs, the framework can effectively reduce the need for extensive human intervention and enable more accurate and efficient execution of complex tasks. The multi-modal approach combining visual and language inputs facilitates a holistic reinforcement learning process, bridging the gap between task planning and robotic control, and significantly improving the robot's cognitive capabilities and performance. | The difficulty in bridging the cognitive gap between high-level task planning and low-level control execution in robot manipulation tasks. Traditional methods struggle with sub-optimal results due to their reliance on immediate observations and lack of comprehensive planning, which limits the robots' ability to perform complex multi-step tasks efficiently. |
| 95    | LLM+P: Empowering Large Language Models with Optimal Planning Proficiency | 2023 | arxiv              | Focuses on improving robot planning capabilities, specifically in scenarios requiring long-horizon planning. The goal is to enable robots to solve complex manipulation tasks by understanding natural language instructions and generating optimal or feasible plans to achieve the specified goals. | Introduces a hybrid framework called LLM+P, which integrates Large Language Models (LLMs) with classical planning algorithms. The LLM used is unspecified but is designed to handle natural language input, while classical planners utilize the Planning Domain Definition Language (PDDL) to generate solutions. | The LLM component is responsible for converting natural language descriptions of planning problems into formal representations that classical planners can understand. The classical planner then generates a plan, which is translated back into natural language or directly executed by the robot. This combination allows for leveraging the natural language understanding capabilities of LLMs and the efficient problem-solving abilities of classical planners, enabling robots to handle complex planning tasks more effectively. The approach ensures correct and optimal solutions for a wide range of planning problems, demonstrating significant improvements over using LLMs alone. | The limitation of Large Language Models (LLMs) in reliably solving long-horizon robot planning problems. While LLMs demonstrate impressive zero-shot generalization abilities, they struggle with planning tasks that require deep reasoning and optimal solution generation. Classical planners, although efficient, are not designed to interpret natural language instructions. The paper aims to combine the strengths of both LLMs and classical planners to overcome these limitations. |
| 96    | ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application | 2023 | arxiv              | Translating natural language instructions into executable robot actions using OpenAI's ChatGPT. The aim is to leverage ChatGPT’s conversational capabilities and few-shot learning to facilitate task planning in robotics, allowing for the execution of complex, multi-step tasks in various domestic environments. | OpenAI's ChatGPT, a large language model (LLM) fine-tuned to understand and generate human-like text. ChatGPT is employed to process natural language instructions and output task plans in a structured format suitable for robotic execution. | ChatGPT plays a crucial role in interpreting natural language instructions and generating corresponding robot actions and task plans. It processes the input instructions, adapts to the environmental context, and outputs a sequence of actions with explanations. By leveraging few-shot learning, ChatGPT can effectively integrate user feedback, update environmental data, and plan multi-step tasks, thereby enhancing the robot's ability to perform complex manipulations without extensive prior training or data collection. | The challenge of creating robust and adaptable task plans for robots based on natural language instructions. Specifically, it tackles the issues of integrating natural language understanding with robot execution systems, reducing the extensive record-keeping for task planning, and adapting to various environments without extensive data collection or retraining. |
| 97    | ReAct: Synergizing Reasoning and Acting in Language Models   | 2023 | ICLR               | Improving language models' capabilities in language understanding and interactive decision-making by combining reasoning traces and task-specific actions. The proposed approach, ReAct, enables large language models (LLMs) to interleave reasoning and acting, thereby enhancing their performance in diverse language and decision-making tasks such as question answering, fact verification, and interactive decision-making benchmarks. | The approach involves prompting LLMs to generate both verbal reasoning traces and actions in an interleaved manner. This method leverages the strengths of pre-trained language models in understanding and generating language while adding the ability to interact with external sources and environments. | Combining reasoning and acting for enhanced performance in task-solving scenarios. By integrating reasoning traces with task-specific actions, the LLMs can dynamically update their plans and handle exceptions more effectively. This approach improves model interpretability, trustworthiness, and adaptability, showcasing significant advantages over traditional reinforcement learning methods and imitation learning in interactive decision-making tasks. | Improving the synergy between reasoning and acting in LLMs. Traditionally, these capabilities have been studied separately, leading to limitations such as hallucination, error propagation, and inadequate handling of dynamic environments. The specific problem tackled is how to effectively combine reasoning (for better understanding and tracking) with acting (for interaction and execution) to enhance the overall task-solving abilities of LLMs. |
| 98    | LLM as A Robotic Brain: Unifying Egocentric Memory and Control | 2023 | arxiv              | proposes the application of a unified framework called LLM-Brain to enhance the adaptability and functionality of multimodal embodied AI systems. The goal is to enable these systems, which include robots or other intelligent agents, to dynamically interact with their environment using a large-scale language model (LLM) as the core component. The framework aims to address various downstream embodied AI tasks, such as active exploration, embodied question answering, and vision-language navigation, by leveraging the capabilities of LLMs. | The LLM is employed as the central component of the LLM-Brain framework, acting as a robotic brain that unifies egocentric memory and control. The LLM is integrated with multiple multimodal language models to perform various tasks in a zero-shot learning manner, enabling the system to generalize across different scenarios and tasks without requiring extensive additional training. | The LLM plays a crucial role in facilitating communication and coordination among different components of the robotic system. It serves as the central brain that processes and integrates sensory inputs, plans actions, and controls the robot's behavior. By using natural language as the medium for communication, the LLM enables closed-loop interactions between perception, planning, control, and memory. This approach allows the robot to efficiently perform tasks such as exploration and question answering based on its accumulated knowledge and observations, thus addressing the limitations of traditional methods that rely on extensive pre-training and fine-tuning. | Integrating memory and control in embodied AI systems. Traditional methods often require separate frameworks for these aspects, leading to inefficiencies and limited generalizability. Specifically, the paper seeks to solve the problem of enabling embodied AI systems to perform complex tasks without extensive pre-training or task-specific fine-tuning. The proposed solution focuses on achieving seamless communication and cooperation between different modalities (e.g., vision, language) within a single unified framework. |
| 99    | LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models | 2023 | arxiv              | Using large language models (LLMs) as planners for embodied agents to follow natural language instructions and complete complex tasks in visually perceived environments. The application lies in creating versatile agents capable of quickly learning and performing a wide range of tasks through few-shot learning and physical grounding. | Utilizes large language models (LLMs), such as GPT-3, to create the LLM-Planner. This approach leverages the generative capabilities of LLMs to generate high-level plans and dynamically adjust these plans based on environmental feedback. | LLMs are used to directly generate plans for embodied agents, allowing them to handle a wide range of tasks by generating sequences of subgoals. The LLMs provide a hierarchical planning structure where high-level plans are generated and then executed through a low-level planner. This reduces the number of calls to LLMs and integrates physical grounding by re-prompting the LLMs with observed objects to dynamically adjust plans based on the environment. This approach significantly improves sample efficiency and adaptability of the agents. | Addresses the high data cost and poor sample efficiency of existing methods for training versatile agents. Specifically, it tackles the challenge of enabling agents to plan and update their actions in complex and partially-observable environments without needing extensive labeled data for training. |
| 100   | Reward Design with Language Models                           | 2023 | ICLR               | The application of this paper lies in the domain of reinforcement learning (RL), specifically in the design of reward functions. The paper addresses the challenge of specifying reward functions that align with human users' objectives for training RL agents. It explores using large language models (LLMs) as a proxy to simplify the reward design process, enabling RL agents to be trained more effectively and efficiently in tasks like the Ultimatum Game, matrix games, and the DEALORNO DEAL negotiation task. | Leverages large language models (LLMs) such as GPT-3 as the foundation model for their approach. These models are trained on vast amounts of internet-scale text data and have shown the ability to learn from few-shot or zero-shot examples, making them suitable for generating reward signals based on minimal user input. | The LLM serves as a proxy reward function by interpreting user-provided prompts that describe the desired behavior. The LLM evaluates the RL agent's behavior against the desired behavior, providing a reward signal that guides the agent's learning process. This approach leverages the LLM's in-context learning abilities and its prior knowledge of human behavior, allowing users to specify their preferences intuitively with minimal examples. The LLM thus enables more accurate and user-aligned RL training without the need for complex reward engineering or extensive labeled data. | Tackles the problem of the "reward design problem" in RL, where specifying human notions of desired behavior through reward functions is difficult and often impractical. Traditional methods either require detailed crafting of reward functions or large amounts of labeled data, both of which are costly and complex. The paper proposes an innovative solution using LLMs to infer user preferences and generate corresponding reward signals, thus simplifying the reward design process. |
| 101   | Collaborating with language models for embodied reasoning    | 2022 | NeurIPS            | Enhance the reasoning capabilities of reinforcement learning (RL) agents in complex and ambiguous environments. This is achieved by combining the strengths of Large Scale Language Models (LSLMs) with traditional RL methods to create a system capable of handling new tasks with minimal training data. The system aims to demonstrate how combining planning, acting, and reporting modules can improve the agent's performance in environments requiring logical reasoning and task generalization. | A combination of a pre-trained Large Scale Language Model (LSLM) and a traditional RL agent. The system is divided into three parts: a Planner, an Actor, and a Reporter. The Planner is an LSLM responsible for task description and logical reasoning, which breaks down complex tasks into manageable instructions. The Actor is an RL agent that executes these instructions in the environment. The Reporter observes the environment, provides feedback, and communicates visual properties back to the Planner. | Enhancing the reasoning capabilities of the RL agent. The Planner (LSLM) interprets tasks and generates a sequence of instructions for the Actor. The Reporter module, which can be trained using reinforcement learning, closes the feedback loop by observing the environment and providing necessary feedback to the Planner. This integration allows the RL agent to effectively reason through tasks, adapt to new environments, and improve performance without extensive hand-specified feedback, thus addressing the limitations of traditional RL agents in complex environments. | The difficulty RL agents face in reasoning through complex tasks in embodied environments. Traditional RL agents require extensive training data and struggle to generalize to new tasks. Moreover, LSLMs, despite their strong reasoning abilities, are not inherently capable of direct interaction with the environment. The paper tackles the challenge of integrating these complementary abilities to enable effective reasoning and action in RL agents. |
| 102   | Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents | 2022 | ICML               | The utilization of world knowledge learned by large language models (LLMs) to act in interactive, embodied environments. Specifically, the paper investigates the potential of grounding high-level tasks expressed in natural language (e.g., "make breakfast") into actionable steps (e.g., "open fridge") that can be executed in environments like VirtualHome, which simulates household activities. | Utilizes large pre-trained language models, specifically mentioning GPT-3 and Codex. These models are employed to generate plausible action sequences from high-level task descriptions based on their extensive training on large corpora of human-produced language. | Providing the semantic knowledge required to generate plausible action plans for given tasks. The models are used to create initial action plans, which are then refined using several proposed tools to improve executability. These tools include mapping generated outputs to admissible actions, autoregressively generating actions based on past admissible actions, and weak supervision through prompt examples. This approach enhances the ability of LLMs to generate executable and semantically correct action plans for robotic applications and reinforcement learning scenarios in interactive environments. | Addresses the challenge of converting high-level, natural language task descriptions into mid-level action plans that are executable in interactive environments without requiring additional training. The core problem tackled is the frequent non-executability of plans generated naively by LLMs due to their inability to map precisely to admissible actions within the environment. |
| 103   | Language, Camera, Autonomy! Prompt-engineered Robot Control for Rapidly Evolving Deployment | 2024 | HRI                | Enhancing the usability of robots for human-robot interaction (HRI) by utilizing large language models (LLMs). The Context-observant LLM-Enabled Autonomous Robots (CLEAR) platform enables robots to perceive and interact with their environment using natural language, facilitating more intuitive and efficient robot control across various tasks and scenarios without the need for code manipulation. | Large language models, specifically GPT-3.5, GPT-4, and LLaMA2, integrated with the YOLOv8 vision model. These models are used without any additional fine-tuning or pre-training specific to the robotic tasks, demonstrating the platform's agnostic approach to the underlying LLMs. | Enabling autonomous robot behavior through natural language processing and context understanding. The CLEAR platform employs these LLMs to interpret human commands and environmental cues, generating appropriate robotic actions. This approach not only simplifies the interaction process but also enhances the robot's ability to adapt to different tasks and environments, significantly improving the overall efficiency and effectiveness of human-robot interactions. | Addresses the challenge of robot autonomy and usability in HRI by eliminating the need for model fine-tuning and complex programming. By leveraging off-the-shelf pre-trained LLMs and vision models, the CLEAR platform provides a flexible and robust solution for controlling different types of robots, thereby simplifying the process for untrained users to interact with and command robots dynamically. |
| 104   | Planning with diffusion for flexible behavior synthesis      | 2022 | ICML               | Generating states and actions as trajectories, do not learn a environment model. | diffusion model, especially class-based diffusion model, utilize skills, rewards, cost guide diffusion model sample trajectories. | diffusion model plays a dynamic model and planning stage. Traditional methods perform poorly on problems such as long horizon planning and sparse rewards. In long horizon planning, generating a entire trajectory can reduce Cumulative error. | For complex environments, there are large errors in the generation of actions and diffusion has the disadvantage of high computational complexity during both training and inference. |
| 105   | Is Conditional Generative Modeling all you need for Decision-Making? | 2023 | ICLR               | predicting states via a distribution of states, generating a single action via inverse dynamics | class-free diffusion model, Use labels as guidance, use class-free diffusion model to guide sampling, and basically use reward and cost as guidance in experiments. | Simplify the modeling process by diffusing the state sequence instead of the state and action at the same time, Use classifier-free guidance and low-temperature sampling to extract the best behavior in the dataset. Infer the action from the generated state sequence through an inverse dynamics model. Avoids the instability caused by dynamic programming and value function estimation in traditional RL. | In reinforcement learning, this work is not applicable to reward-free and sparse reward environments because it requires labels to guide the diffusion model to generate states. |
| 106   | Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning | 2023 | ICLR               | reducing errors when evaluating, address limitation of expressiveness in policy | a diffusion model represent the policy. The limitation of expressiveness in policy classes used by existing offline RL methods, which leads to highly suboptimal solutions. | The diffusion model is used to better capture the multi-modal, skewed, and dependent nature of the action distributions in the offline dataset. By using a diffusion model, the policy can be more accurately represented, which helps in improving policy learning and performance. | Conduct more extensive comparisons with a wider range of state-of-the-art offline RL methods to further validate the effectiveness of Diffusion-QL. |
| 107   | Provably Efficient Reinforcement Learning for Online Adaptive Influence Maximization | 2023 | ICLR               | online adaptive influence maximization in social networks, involves selecting a set of initial users to maximize the spread of information or influence across the network. | diffusion model,  how to effectively maximize the spread of influence in a social network where the network model is unknown and must be learned over time. The challenge is to select seed nodes adaptively based on the current state of the network and the feedback received, rather than selecting all nodes at once before the diffusion process begins. | model maintains an estimate of the network and selects seed nodes based on real-time feedback. The RL algorithm optimistically explores the network while also aiming to improve the policy for selecting seed nodes. The model learns from node-level feedback, which is less granular than edge-level feedback but more practical in large networks. | In large networks, the state and its value can grow unboundedly, which complicates the analysis and control of the diffusion process. |
| 108   | Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling | 2023 | ICLR               | Autonomous Driving: Learning driving policies from pre-collected driving data rather than putting vehicles in potentially dangerous situations. Robotics: Training robots using data from simulations or past operations to perform tasks without new physical interactions, which can be expensive and time-consuming. | diffusion model, capture the diversity in behavior data, thus avoiding the limitations of unimodal policy models. | improve the policy learning process in Offline RL. The key idea is to decouple the policy into two components: Generative Behavior Model: This model captures the distribution of behaviors in the dataset. The authors suggest using highly expressive generative models like diffusion-based methods to model diverse and complex behaviors. Action Evaluation Model: This model evaluates the actions generated by the behavior model to ensure they are effective. | Efficient Sampling and Planning: The proposed method combines generative behavior modeling with in-sample planning to enhance computational efficiency and reduce extrapolation errors. Out-of-Sample Actions: Ensuring that learned policies do not select actions not present in the training data is critical to avoid unreliable performance. |
| 109   | Reasoning with Latent Diffusion in Offline Reinforcement Learning | 2024 | ICLR               | How to stitch fragments of suboptimal trajectories from a dataset while avoiding extrapolation errors. | The diffusion model generates multiple latent features through the state, and selects the best latent through Q learning. | A latent diffusion model is used to model the compressed latent feature of trajectory sequences. Decoupling the diffusion model from the low-level policy allows the use of an autoregressive decoder to model the low-level policy. | The method is somewhat cumbersome, and many components need to be trained, including policy, VAE, and diffusion model. At the same time, multiple latant features need to be inferred, which makes training more troublesome. |
| 110   | Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations | 2024 | ICLR               | Lack of robustness in the face of state observation perturbations, which may come from sensor errors or adversarial attacks, especially in offline setting. | diffusion model, Given the estimated state trajectory, the action generated by the last step agent, and the current noise state from the environment, a conditional diffusion model is used to estimate the current state by denoising. | Use conditional diffusion model as a denoising tool instead of generative model. A non-Markov training objective is proposed to minimize the total entropy of denoised states in RL trajectories, alleviating the error accumulation problem in the model basis estimation. | observation perturbations can be potentially addressed by traditional methods, especially when we assume observation perturbations does not affect reward and transition functions. |
| 111   | Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation | 2024 | ICLR               | generating high preference trajectory from a low-preference trajectry, especially environment require well-designed reward functions, which can be challenging to create or inaccurate rewards. | diffusion model. It avoids the difficulties of learning accurate transition-wise rewards from trajectory-level preferences. It leverages the power of generative models (diffusion models) to improve entire trajectories. It can work with limited preference data by using Preference Augmentation. | Uses a diffusion model to generate preferred trajectories directly, avoiding the need for transition-wise reward learning. Introduces "Preference Augmentation" to generate more preference pairs for training. Extracts a policy through imitation learning on the generated high-preference trajectories. | The method may face challenges with very high-dimensional state-action spaces or extremely long trajectories. |
| 112   | Score Regularized Policy Optimization through Diffusion Behavior | 2024 | ICLR               | Slow sampling speed of diffusion models in offline RL: Diffusion policies typically require 5-100 iterative inference steps to generate an action sample, which is computationally expensive. | It leverages the generative capabilities of diffusion models without directly sampling actions from them, addressing the slow sampling speed issue. The reverse-KL objective encourages mode-seeking behavior, helping to avoid the "mode covering" issue. It maintains the expressiveness of diffusion models while significantly reducing computational costs. | Extracts a simple deterministic inference policy from critic and diffusion behavior models, avoiding the iterative diffusion sampling process during evaluation. Uses the score function of the behavior distribution to directly regularize the policy gradient instead of the policy loss. Combines new framework with implicit Q-learning and continuous-time diffusion behavior modeling. | Since the key idea is to use existing pre-trained diffusion models, there is not other than the algorithmic contribution, |
| 113   | Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning | 2024 | NIPS               | Large, discrete, multidimensional, and unordered action spaces in resource allocation problems (e.g., placing multiple security resources). Complex constraints on valid actions that are hard to express mathematically but can be checked by an oracle. | Normalizing flow, Normalizing flows allow for efficient sampling and density estimation, which is crucial for representing complex policies in large action spaces. The flow-based approach provides a compact representation by only generating sampled actions and their log probabilities, rather than explicitly representing the entire action distribution. | Use of a conditional normalizing flow (specifically Argmax Flow) to represent the stochastic policy compactly. Integration of the flow-based policy into an actor-critic framework (A2C). | The approach involves multiple components (flow networks, ELBO optimization, sandwich estimator) which might increase computational complexity and training time. |
| 114   | Simple Hierarchical Planning with Diffusion                  | 2024 | ICLR               | Long horizon planning tasks of RL. The computational efficiency and generalization ability of the diffusion-based  method. | diffusion model, the framework with two diffusion model, the hierarchical structure can handle long horizon planning problems more effectively and reduce computational complexity. | High-level planner: used to generate sparse sub-goal sequences. Low-level planner: used to generate dense trajectories between adjacent sub-goals. | Using fixed interval states as sub-goals is a simplifying assumption and may not be optimal. More intelligent sub-goal selection mechanisms can be explored in the future. |
| 115   | Hierarchical diffusion for offline decision making           | 2023 | ICML               | Long-term decision-making problem in offline reinforcement learning | diffusion model, Generative learning can avoid some problems faced by temporal difference learning. Diffusion models have advantages in long horizon modeling, such as low dependence on horizon. | Use a cascade framework: reward-conditioned target diffuser for sub-goal discovery, target-conditioned trajectory diffuser for generating corresponding action sequences. Use planning-based sub-goal extraction and Transformer-based diffusion to deal with data contamination and long-range sub-goal dependency problems | Rely on high-quality sub-goal extraction, which may be challenging in some complex environments. |
| 116   | Safe offline reinforcement learning with feasibility-guided diffusion model | 2024 | ICLR               | Most existing methods only enforce soft constraints, i.e., keep the expectation of constraint violations below a predetermined threshold, which may lead to potentially unsafe outcomes. Implementing hard constraints in offline settings is challenging because it requires a balance between three highly complex and related aspects: safety constraint satisfaction, reward maximization, and behavioral regularization of offline datasets. | diffusion model, It can strictly guarantee security while optimizing high returns. The originally complex triple problem is transformed into a feasibility-dependent goal, simplifying the problem. The decoupled learning process provides stronger security performance and learning stability. The diffusion model has strong expressive power and is suitable for extracting complex strategies. | Using reachability analysis in safety control theory, the hard safety constraints are equivalently transformed into identifying the maximum feasible region for a given offline dataset. A feasibility-dependent objective function is proposed to maximize the reward value in the feasible region and minimize the safety risk in the infeasible region. Safety constraint compliance, reward maximization, and offline policy learning are achieved through three decoupled processes. A guided diffusion model is used to effectively extract the optimal policy. | There seems to be an inconsistency between the goal and the actual approach. The paper aims for 100% guarantee in safety. However, the approach still need to estimate the feasible areas and then to yield the value function threshold. There could be estimation error and this estimation requires very thorough dataset provided, which may not be practical in reality. |
| 117   | Safe Offline Reinforcement Learning with Real-Time Budget Constraints | 2023 | ICML               | real time budgets Constraints                                | diffusion model, It provides flexibility for dynamically switching constrained optimization objectives, which is suitable for solving real-time budget constraint problems. | The diffusion model-based Diffuser is used as the backbone network of TREBI to model the optimal trajectory distribution. In the learning phase, offline datasets are used to approximate the trajectory distribution of the behavior policy. In the inference phase, budget-dependent trajectory planning is used to achieve adaptive response to the real-time budget. | Using a reward with a penalty for exceeding a cost threshold as a guide is too simplistic and lacks theoretical validity. |
| 118   | Compositional Diffusion-Based Continuous Constraint Solvers  | 2023 | CoRL               | Continuous Constraint Satisfaction Problems (CCSPs) in the context of robotic reasoning and planning. These problems involve finding continuous values (such as object placements and robot trajectories) that satisfy a set of geometric, physical, and qualitative constraints. | diffusion model, In robotic manipulation planning, it is crucial to select continuous values that meet complex constraints such as stability, lack of collision, and user requirements. traditional methods is inefficient and often fails to find globally satisfying solutions, especially when dealing with multiple constraints simultaneously. | Trains diffusion models to generate distributions of feasible solutions for individual constraint types. Combines these diffusion models to address novel problem instances.  Casts the global constraint satisfaction task as an energy minimization problem, solved using the annealed unadjusted Langevin algorithm (ULA). Allows for independent or simultaneous training of component diffusion models based on paired compositional problems and solutions. | The approach might face scalability issues as the complexity of the constraint graph increases. |
| 119   | DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability | 2023 | CoRL workshop      | Task and Motion Planning (TAMP) for autonomous robot manipulation. This involves planning sequences of actions that a robot must perform to achieve a goal, taking into account both high-level tasks and low-level motions. | diffusion models to represent distributions over continuous state and action parameters as generative samplers. | Using diffusion models to learn generative samplers that represent constraints in TAMP problems. Training these models on datasets of parameters that satisfy the constraints. Integrating these learned samplers into a TAMP solver to find action parameter values that satisfy constraints along a plan. Applying the learned samplers to both known and unknown objects by defining constraints in a latent space. Utilizing classifier-based and classifier-free guidance for conditional sampling to meet specific constraints. | Ensuring that the approach scales efficiently to more complex environments and larger action spaces. |
| 120   | StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects | 2023 | RSS                | robotic manipulation, specifically for robots operating in human environments. Applications include setting tables, assembling furniture, and organizing objects in a household or industrial setting. | diffusion model, aims to enable robots to understand and execute complex rearrangement tasks based on abstract language instructions. | The generative models, particularly the diffusion model used in StructDiffusion, play a crucial role in this framework. They help in predicting possible goal poses for objects by iteratively refining initial guesses and ensuring that these poses are physically valid and semantically appropriate. The diffusion model is combined with a transformer architecture that processes both language tokens and object encodings, allowing for the generation of diverse and feasible object configurations. | High-level language instructions can be ambiguous or vague, making it challenging for the model to infer precise object arrangements. |
| 121   | Diffusion Policy: Visuomotor Policy Learning via Action Diffusion | 2023 | RSS                | robot manipulation tasks. These tasks vary in complexity and include actions in both simulated and real-world environments. | learning visuomotor policies for robot manipulation tasks. Visuomotor policies enable robots to perform tasks by interpreting visual input and generating corresponding motor actions. | These models are used to represent the robot's visuomotor policy. The key advantages of using generative models in this context include: Expressing Multimodal Distributions. (the Diffusion Policy can express complex multimodal action distributions.) | The diffusion process involves multiple iterations of denoising, which can be computationally expensive. The paper could benefit from a more detailed analysis of the computational overhead and potential optimizations. |
| 122   | Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning | 2023 | ICML               | guided sampling for diffusion models in offline reinforcement learning. Guided sampling is crucial for embedding human-defined guidance during the sampling procedure, allowing the diffusion models to generate samples that align with specified preferences or constraints. | enhance the capability of diffusion models to incorporate human-defined guidance in tasks. | Diffusion models are powerful generative models that gradually add noise to data and then learn to denoise it, effectively generating new samples from the learned distribution. In this context, the role of the generative model is to produce samples that adhere to the guidance defined by an energy function, ensuring that the generated samples meet specific criteria or constraints. | Unknown Intermediate Guidance. The intermediate guidance, which should ideally combine the sampling distribution and the energy function, is unknown and difficult to estimate accurately. |
| 123   | MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL | 2023 | ICML               | Meta RL.  challenge of generalizing across tasks with varying rewards or dynamics in offline Reinforcement Learning (RL). | Dependence on Context Encoder Accuracy: The performance of MetaDiffuser heavily relies on the accuracy of the pre-trained context encoder. Improving the robustness and efficiency of this component could further enhance the overall framework. | During meta-training, an accurate context encoder is pre-trained to capture task-relevant information from mixed offline trajectories. The compact task representation is used as a contextual label for the diffusion model, which generates task-oriented trajectories. During meta-testing, the conditional diffusion model denoises the predicted context from warm-start data to generate desired trajectories. | The performance of MetaDiffuser heavily relies on the accuracy of the pre-trained context encoder. Improving the robustness and efficiency of this component could further enhance the overall framework. |
| 124 | Embodied Foundation Models for Large Scale Orchestration of Robotic Agents | 2024 | arxiv     | Leveraging LLMs to enhance the capabilities of mobile manipulation robots in executing long-horizon tasks within large, unexplored environments. The aim is to improve the autonomous reasoning and action capabilities of robots, enabling them to perform complex tasks such as navigation and object manipulation in dynamic and previously unseen settings. |  LLMs integrated with dynamically built scene graphs. These scene graphs are constructed from open-vocabulary object detections and are continuously updated as the environment changes. The approach is zero-shot and open-vocabulary, allowing it to handle a wide range of objects and scenarios without needing extensive retraining. | LLMs are used to generate high-level plans and instructions based on the structured scene representations provided by the dynamic scene graphs. The integration of LLMs with object-centric action spaces allows for the generation of coherent, context-aware plans that facilitate efficient task execution in robotics. By leveraging the LLMs' accumulated knowledge, the approach improves the robot's ability to perform complex tasks in diverse and unexplored environments, demonstrating substantial improvements over traditional methods in terms of search efficiency and task performance. | The challenge of grounding language models in dynamically evolving, unstructured environments. Traditional approaches often fail to handle large, unexplored spaces with numerous objects and long time horizons. This work specifically tackles the problem of creating structured representations that allow LLMs to generate feasible and efficient plans for real-world robotic execution, reducing the risk of impractical sequences and hallucinations. |
| 125 | An Embodied Generalist Agent in 3D World | 2023 | arxiv       | LEO, an embodied multimodal generalist agent designed to address real-world tasks in a 3D environment. LEO's primary application is to enhance the general intelligence of AI systems, enabling them to perceive, ground, reason, plan, and act effectively within 3D spaces, which is crucial for advancing capabilities in fields such as robotics and virtual reality. | LEO incorporates an LLM-assisted pipeline for generating high-quality 3D vision-language data and employs scalable Transformer architectures for its multimodal task sequences. | LLM and generative model are pivotal in bridging the gap between vision, language, and embodied action. | Challenge of enabling current models, which primarily rely on 2D data, to handle tasks in the 3D environment. |
| 126 | RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation | 2024 | ICML |             | presents a generative robotic agent, RoboGen, aimed at automating the learning of diverse robotic skills using generative simulation. The application lies in scaling up robotic skill learning with minimal human supervision by leveraging advancements in foundation and generative models. | Foundation models and generative models for simulating diverse tasks and environments. These models are utilized to autonomously generate high-level task proposals, scene descriptions, asset selections, policy learning choices, and training supervisions. | Facilitate the automation of generating diverse tasks and scenes for robotic training, effectively scaling up the learning process. These models enable the robotic agent to propose tasks, generate simulation environments, and learn policies autonomously, minimizing the need for extensive human intervention. | The challenge of constructing simulation environments and generating varied training supervisions, which traditionally require significant human effort and tedious processes. |
| 127 | Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning | 2023 | CoRL         | The development of a scalable approach for LLM-based large-scale task planning for robots using 3D scene graphs (3DSGs) in expansive, multi-floor, and multi-room environments.| The use of a pre-trained LLM in conjunction with a 3D scene graph representation. | LLM is employed to conduct a semantic search within a hierarchical 3DSG for task-relevant subgraphs, to reduce planning horizon through classical path planning integration, and to refine plans iteratively using feedback from a scene graph simulator, ensuring feasible and executable plans in large-scale environments. | Grounding LLMs in large-scale environments to enable effective generalist planning for diverse tasks, dealing with the complexity and high-dimensionality of these environments.
| 128 | Large Language Models as Generalizable Policies for Embodied Tasks | 2024 | ICLR         | Applies large language models (LLMs) to policy learning for embodied visual tasks, aiming to develop generalizable policies. | Uses a pre-trained and frozen LLM, adapting it through reinforcement learning (RL) by adding learned input and output adapter layers. | LLM is used to generate actions in embodied environments based on task instructions and visual observations, demonstrating strong generalization capabilities and efficiency through reinforcement learning. | The challenge of enabling LLMs to generalize to new tasks with zero or few-shot training and handle complex paraphrasing of task instructions. | 
| 129 | Open X-Embodiment: Robotic Learning Datasets and RT-X Models | 2023 | arxiv      | Develop a generalist X-robot policy that can be efficiently adapted to new robots, tasks, and environments in the field of robotic manipulation. | Large-scale pre-trained models, specifically mentioning models like RT-1 and RT-2, trained on data from multiple robotic manipulators. | These models enable positive transfer and improve the capabilities of multiple robots by leveraging experience from diverse datasets, leading to better generalization and new capabilities in robotic manipulation. | The challenge of training separate models for each robotic application, robot, and environment. It seeks to find a more efficient method to train models that can generalize across different robotic platforms and tasks. |
| 130 | RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | 2023 | arxiv       | Applies vision-language models trained on Internet-scale data to end-to-end robotic control to enhance generalization and enable emergent semantic reasoning in robotic tasks. | Utilizes vision-language-action (VLA) models, specifically a model called RT-2 (Robotics Transformer 2), which builds on large pre-trained vision-language models. | The foundation model is fine-tuned to output low-level robotic actions from visual and language inputs, thereby enabling the robot to interpret and act on complex commands and exhibit emergent capabilities such as novel object handling and semantic reasoning. | Incorporating the rich semantic knowledge from large-scale vision-language models into low-level robotic control to boost generalization and perform multi-stage semantic reasoning. |
| 131 | Generative Agents: Interactive Simulacra of Human Behavior | 2023 | arxiv        | Create generative agents that can simulate believable human behavior in interactive environments, such as immersive environments, rehearsal spaces for interpersonal communication, and prototyping tools. | A large language model architecture that combines memory, reflection, and planning components to simulate human-like agents. | The foundation model supports the agents in storing experiences, synthesizing memories, and planning actions dynamically, which enables the agents to behave coherently and adaptively in various situations, similar to human decision-making and planning in robotics and reinforcement learning applications. | The challenge of creating computational agents that exhibit long-term coherent behavior by remembering, reflecting, and planning based on their experiences and interactions. |
| 132 | Chat with the Environment: Interactive Multimodal Perception using Large Language Models | 2023 | IROS         | Programming robot behavior to perform high-level planning and reasoning in complex, multimodal environments. | LLM-based framework named Matcha (Multimodal environment chatting) agent. | The LLM serves as the backbone to instruct epistemic actions, reason over multimodal sensations (vision, sound, haptics, proprioception), and plan task execution based on interactively acquired information, thereby enhancing the robot's high-level planning and reasoning abilities. | The challenge of grounding LLMs in multimodal sensory input and continuous action output, enabling robots to interact with their environment and acquire novel information. |
| 133 | PaLM-E: An Embodied Multimodal Language Model | 2023 | arxiv        | Enabling large language models (LLMs) to perform complex embodied reasoning tasks, particularly in the context of robotics, visual question answering, and captioning. | Utilizes a pre-trained large language model, specifically PaLM-E (with 562 billion parameters), integrating it with vision-language models and embodied multimodal inputs. | To process and integrate continuous visual and state estimation inputs with textual inputs, enabling it to perform sequential decision-making and reasoning in embodied tasks. | The challenge of grounding LLMs in real-world contexts by incorporating continuous sensor modalities, which traditional text-based LLMs lack. This is essential for tasks requiring real-world inference and decision-making. |
| 134 | Large Language Models as Zero-Shot Human Models for Human-Robot Interaction | 2023 | arxiv      | Explores the use of LLMs as zero-shot human models for human-robot interaction (HRI) to improve social robot planning processes. | Utilizes state-of-the-art LLMs, specifically FLAN-T5 and a variant of GPT-3.5. | LLMs are used to generate human-like behavior patterns and reasoning in HRI scenarios, demonstrating their potential to act as effective human models without additional training data. | The challenge of creating accurate human behavior models for HRI without the need for extensive interaction data, which is typically difficult and costly to obtain. |
| 135 | Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence | 2023 | arxiv      | Improve embodied AI tasks such as locomotion, navigation, dexterous manipulation, and mobile manipulation through the use of pre-trained visual representations (PVRs). | Vision transformers (ViT-B and ViT-L) trained with Masked Auto-Encoding (MAE) on a large dataset combining egocentric videos and ImageNet. | The foundation models (PVRs) are used to enhance the performance of embodied AI tasks by leveraging large-scale pre-training. Task- or domain-specific adaptations of the models further improve performance. | The challenge of finding a universally dominant PVR that can improve performance across a diverse range of embodied AI tasks, demonstrating that scaling dataset size and diversity does not universally improve performance. |
| 136 | Translating Natural Language to Planning Goals with Large-Language Models | 2023 | arxiv      | LLMs in translating natural language goals into structured planning languages for use in robotics and automated planning systems. | The study utilizes GPT-3.5, a large language model, to translate natural language instructions into PDDL goals. | The LLM acts as a translator that leverages commonsense knowledge and reasoning to fill in missing details from underspecified goals, making them suitable for automated planning systems. | The challenge of whether LLMs can effectively translate ambiguous or underspecified natural language goals into precise planning goals that can be understood and executed by AI planners, specifically in the context of PDDL (Planning Domain Definition Language). | 
| 137 | RT-1: Robotics Transformer for Real-World Control at Scale | 2022 | arxiv      | The paper is applied in the field of robotics, aiming to develop models capable of learning from multi-task data and performing various robotic tasks in the real world. | Uses a Transformer-based model, named RT-1 (Robotics Transformer 1). | RT-1 encodes high-dimensional inputs, such as camera images, instructions, and motor commands, into compact token representations. | The challenge of robots being unable to generalize effectively in zero-shot or small sample scenarios, particularly focusing on the difficulty of collecting and processing large-scale, real-world robotic data. |
| 138 | PDDL Planning with Pretrained Large Language Models | 2022 | NeurIPS | The application of few-shot prompting of pretrained large language models (LLMs) towards solving PDDL (Planning Domain Definition Language) planning problems. | Utilizes OpenAI's Codex LLM, a variant of the GPT-3 model, for its experiments on PDDL domains. | LLM serves to generate potential solutions to PDDL planning problems, which are then used to initialize the queue of a heuristic-search planner. | Two specific challenges: (1) determining to what extent LLMs can independently solve PDDL planning problems, and (2) exploring how LLMs can be used to guide AI planners in solving these problems. |
| 139 | LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent | 2023 | arxiv        | 3D visual grounding in household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. | LLM as the foundation model, specifically integrating it with visual grounding tools like OpenScene or LERF. | The LLM decomposes complex language queries into semantic constituents and evaluates spatial and commonsense relations to ground objects in a 3D scene, improving the zero-shot grounding capability and handling complex language tasks. | The issue of handling complex language queries for visual grounding without relying on extensive labeled data, making it capable of zero-shot, open-vocabulary grounding. | 
| 140 | Semantic Mechanical Search with Large Vision and Language Models | 2023 | CoRL       | Mechanical search, where robots need to manipulate objects to find a fully occluded target object in environments like shelves. | Uses Vision and Language Models (VLMs) and Large Language Models (LLMs) to generate a semantic occupancy distribution, aiding in the mechanical search process. | VLMs and LLMs are used to understand scene semantics and generate explicit intermediate representations that help in predicting target object positions, thus improving the robot's mechanical search capabilities. | The difficulty of mechanical search in real-world settings by proposing a novel framework called Semantic Mechanical Search (SMS), which leverages semantic relationships between objects to improve search efficiency. |
| 141 | Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation | 2024 | arxiv        | Vision-and-Language Navigation (VLN), where an agent navigates through a 3D environment using natural language instructions. | Pre-trained hierarchical neural radiance representation model (HNR) for generating multi-level semantic representations of future environments. | The HNR model helps the agent in accurately predicting the semantic features of future candidate locations, allowing for more efficient and robust navigation decisions without the need for pixel-wise RGB image reconstruction. | The issue of predicting future environments for better navigation planning, overcoming the limitations of existing methods that suffer from image distortion and high computational costs. |
| 142 | Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation | 2024 | arxiv         | Vision-and-Language Navigation in Continuous Environments (VLN-CE), focusing on enabling embodied AI agents to navigate towards a target goal by following natural language instructions. | Utilizes a cross-modal transformer architecture, which fuses language features from the instructions with visual observations of the agent. | The cross-modal transformer helps in detecting and localizing instruction errors by effectively connecting the semantic meaning of the language instructions with the visual observations. | The specific problem of instruction errors in VLN-CE, which occur due to human mistakes, inaccuracies, or ambiguities in describing spatial environments. |
| 143 | Learning to Follow and Generate Instructions for Language-Capable Navigation | 2024 | IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE         | Visual-language navigation (VLN), where the goal is to develop embodied agents capable of following natural language instructions to navigate in previously unseen environments. | Utilizes a Transformer-based, multi-task learning framework as its foundational model, incorporating two unimodal encoders for language and route encoding and two multimodal decoders for route-to-instruction and instruction-to-route translation. | Simultaneously learn instruction grounding and generation, effectively allowing it to perform both tasks with a single model instance. | Can both follow navigation instructions and generate route descriptions, addressing the limitation of existing VLN agents that are either only good at navigation or only at language generation. |
| 144 | Real-time Vision-based Navigation for a Robot in an Indoor Environment | 2023 | arxiv         | The paper focuses on developing an obstacle-avoidance navigation system for autonomous navigation in home environments. | The system utilizes an RGBD camera for perception and an Nvidia Jetson Xavier NX for onboard computation, relying solely on visual information by using RGB images. | The vision-based techniques are employed to navigate the environment, enabling the robot to devise an optimal navigation path from the bottom center to the top center of the environment, thereby demonstrating the potential of these techniques for real-time, autonomous navigation. | the challenge of enabling a low-cost, 3D-printed, four-legged walking robot to autonomously navigate toward a specified destination while avoiding obstacles using vision-based techniques and advanced path-planning algorithms. |
| 145 | Touchdown: Natural language navigation and spatial reasoning in visual street environments | 2019 | CVPR         |  the problem of jointly reasoning about language and vision in a navigation and spatial reasoning task within a busy urban environment. | employs a language-conditioned variant of the UNET architecture for the spatial description resolution (SDR) task. | The UNET model is used to solve the problem of identifying the location of the hidden object (Touchdown) by reconstructing image features based on the given language description. | the challenge of following navigation instructions in natural language to reach a specific location and then identifying a hidden object based on a spatial description. |
| 146 | Navigating to objects in the real world | 2023 | Science Robotics         | deploy mobile robots for semantic navigation in uncontrolled environments such as homes, schools, and hospitals. | utilizes modular learning approaches and compares them with classical and end-to-end learning methods for semantic navigation tasks. | Modular learning is shown to effectively transfer from simulation to real-world settings, achieving a high success rate by abstracting input sensor data and improving sample efficiency, thus bridging the sim-to-real gap in semantic navigation tasks. | The paper addresses the challenge of transferring learned visual navigation policies from simulation to real-world robots, specifically focusing on the performance gap between sim and real environments. |
| 147 | Bevbert: Multimodal map pre-training for language-guided navigation | 2023 | ICCV          | The application focus of this paper is on vision-and-language navigation (VLN), aiming to improve an agent's ability to navigate using natural language instructions. | proposes BEVBert, a novel map-based pre-training paradigm that involves a cross-modal transformer to learn better visual-textual associations from bird's-eye views. | BEVBert constructs offline hybrid maps and uses a cross-modal transformer for map-instruction interaction, enhancing spatial-aware cross-modal reasoning. This approach reduces uncertainty in decision-making, aiding the agent in achieving robust navigation policies and state-of-the-art performance on VLN benchmarks. | the problem of incomplete and duplicate observations in panoramic views that hinder cross-modal spatial reasoning. It proposes a new map-based pre-training paradigm that is spatial-aware, improving the agent's spatial understanding in VLN tasks. |
| 148 | Learning to Act with Affordance-Aware Multimodal Neural SLAM | 2022 | IROS     | improving planning and navigation in embodied artificial intelligence (AI) systems to enable them to effectively interact with their environment and perform complex tasks. | employs an Affordance-aware Multimodal Neural SLAM (AMSLAM) approach, integrating neural SLAM with affordance-aware semantic representations and task-driven multimodal exploration. | The AMSLAM approach improves exploration efficiency and long-horizon planning by using affordance-aware semantic maps to guide navigation and multimodal exploration, enhancing the robot's ability to generalize to unseen environments and perform complex tasks efficiently. | tackles the bottleneck of exploration efficiency and long-horizon planning in embodied multimodal tasks by introducing a neural SLAM approach that utilizes several modalities and predicts affordance-aware semantic maps. | 
|149|Learning to Learn Faster from Human Feedback with Language Model Predictive Control|2023|ICRA|The primary application discussed in this paper is learning to learn faster from human feedback in robotics through Language Model Predictive Control (LMPC). The goal is to enhance the teachability of large language models (LLMs) by improving their ability to adapt to human instructions and feedback, thereby enabling non-experts to teach robots new tasks more efficiently.|The foundation model used in this work is a large language model (LLM) like PaLM 2. The LLM is fine-tuned to handle language-based human-robot interactions and generate robot code from human feedback, effectively bridging the gap between high-level human instructions and low-level robot actions.|The foundation model plays a crucial role in interpreting and generating robot code based on natural language instructions. The LLM is fine-tuned using the LMPC framework, which treats human-robot interactions as a partially observable Markov decision process (POMDP). This approach allows the LLM to predict future interactions and optimize the robot's behavior to minimize the number of corrections needed from the user. LMPC fine-tuning enables the model to improve teachability by reducing the average number of human corrections and increasing task success rates across various robot embodiments and tasks.|
|150|CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning|2022|CoRL|The primary application discussed in this paper is scalable multi-task and multi-scene visual imitation learning for robotic manipulation. The CACTI framework is designed to train robots to perform a wide range of tasks in diverse environments, specifically focusing on manipulation tasks in kitchen settings both in simulation and the real world.|The foundation model used in this work includes state-of-the-art generative models for data augmentation, as well as pre-trained visual representation models. These models are leveraged to enhance the diversity and quality of training data and to compress visual observations into low-dimensional latent spaces for efficient policy learning.|The foundation models play several crucial roles in the CACTI framework: Data Augmentation: Generative models are used to augment the collected data with visual diversity, such as different object textures and layouts, using techniques like in-painting with Stable Diffusion. Visual Representation Learning: Pre-trained models are utilized to compress visual observations into latent embeddings, which are then used for training the imitation policies. This helps in handling diverse and complex visual scenes efficiently. Policy Training: The augmented and compressed data enable the training of a general multi-task policy that can perform multiple manipulation tasks across varying scenes and object layouts. This approach helps in scaling up robot learning while maintaining high data quality and training efficiency.|
|151|Scaling Robot Learning with Semantically Imagined Experience|2023|arxiv|The primary application discussed in this paper is scaling robot learning using a novel data augmentation technique called Semantically Imagined Experience (ROSIE). The ROSIE framework aims to significantly increase the diversity and amount of data available for training robots by leveraging text-to-image diffusion models. This approach is designed to enable robots to perform a variety of manipulation tasks and generalize to novel scenarios without the need for additional real-world robot data collection.|The foundation model used in this work includes state-of-the-art text-to-image diffusion models, such as DALL-E 2, Imagen, or Stable Diffusion. These models are used to perform aggressive data augmentation by generating photorealistic images of unseen objects, backgrounds, and distractors, guided by natural language text.|The foundation model plays a crucial role in generating diverse and semantically meaningful data for robot learning. ROSIE uses text-guided diffusion models to inpaint or augment scenes in existing robotic datasets, allowing robots to encounter new objects and environments during training. This approach enhances the robustness and generalization of manipulation policies by providing rich, diverse, and realistic training data. The generated data helps robots learn to solve new tasks with new objects and improves their ability to handle novel distractors, ultimately expanding their capabilities in real-world scenarios.|
|152|GenSim: Generating Robotic Simulation Tasks via Large Language Models|2024|ICLR|The primary application discussed in this paper is the generation of diverse robotic simulation tasks using large language models (LLMs). The system, named GenSim, aims to automatically create rich simulation environments and expert demonstrations to train general robotic policies. This approach addresses the challenges of task-level diversity and generalization in simulation data, which are crucial for effective training of multitask robotic policies.|The foundation model used in this work includes large language models (LLMs) such as GPT-4. These models are employed to propose new simulation tasks, generate corresponding implementations in code, and validate the achievability and diversity of the tasks.|The foundation model plays a crucial role in both the generation and validation of simulation tasks. GenSim uses LLMs to generate tasks in two modes: Goal-Directed Generation: The LLM proposes a task curriculum to achieve a specific target task. Exploratory Generation: The LLM iteratively proposes novel tasks, building upon previous tasks to solve more complex challenges. The LLMs are also used to generate expert demonstration data for each task, facilitating the training of multitask policies. This automated generation and validation process significantly enhances the task-level generalization of the trained policies, improving their ability to perform well in both simulation and real-world environments.
|153|URDFormer: Constructing Interactive Realistic Scenes from Real Images via Simulation and Generative Modeling|2023|CoRL|The primary application discussed in this paper is the construction of interactive realistic scenes from real images for use in simulation and robotics. The system, named URDFormer, is designed to generate simulation scenes with realistic dynamic and kinematic properties from real-world images, enabling scalable scene generation for machine learning applications in robotics and computer vision.|The foundation models used in this work include generative models, specifically text-to-image diffusion models, to generate realistic images and simulation content. These models are used to create a paired dataset of realistic images and corresponding simulation scenes, which can then be used to train an inverse model to map from real images to simulation content.|The foundation model plays a crucial role in both the forward and inverse processes. In the forward process, generative models are used to create realistic images from procedurally generated simulation scenes, ensuring visual realism and structural consistency. In the inverse process, the URDFormer model, trained on the paired dataset, predicts the underlying structure and simulation parameters from real-world images. This enables the generation of interactive simulation environments that accurately reflect the kinematic and semantic properties of real-world scenes, facilitating applications in robotics, such as task planning, manipulation, and navigation.|
|154|RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation|2024|ICML|The primary application discussed in this paper is automated robot learning using generative simulation. The system, named RoboGen, aims to automatically generate diverse tasks, scenes, and training supervisions to facilitate scalable robotic skill learning. This approach allows the robot to acquire a wide range of skills with minimal human supervision by leveraging generative models to create simulation environments and training data.|The foundation models used in this work include state-of-the-art generative models and large language models (LLMs) such as GPT-4. These models are utilized to generate task proposals, relevant assets, and training supervisions, including reward functions, scene configurations, and skill learning algorithms.|The foundation models play a crucial role in enabling the autonomous generation of diverse and meaningful tasks and training environments. RoboGen employs LLMs to propose tasks, describe them in natural language, and suggest suitable learning algorithms (e.g., reinforcement learning, motion planning, trajectory optimization). Generative models are used to create realistic scenes and object configurations for the proposed tasks. The system's capability to continuously generate new tasks and environments helps in the training of generalizable robotic policies, thereby scaling up robot learning in simulation with minimal human involvement.|
|155|RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches|2024|ICLR|The primary application discussed in this paper is robotic task generalization using a novel policy conditioning method called RT-Trajectory. The goal is to enable robots to generalize to new tasks beyond those contained in their training datasets by using coarse trajectory sketches. This method is particularly focused on improving generalization to new motions and task configurations that the robot has not seen before.|The foundation models used in this work include transformer-based architectures for policy learning. The RT-Trajectory framework uses these models to learn from trajectory sketches, which serve as a new form of task specification. The model learns to map these sketches to control actions, enabling it to generalize across various manipulation tasks.|The foundation model plays a critical role in interpreting and executing coarse trajectory sketches. RT-Trajectory leverages hindsight trajectory sketches, which are extracted from demonstration data, to condition the policy. These sketches provide a mid-level representation of tasks that balance expressiveness and practicality. The transformer-based model learns to interpret these sketches and generate corresponding control actions, allowing the robot to perform a wide range of tasks, including those involving unseen objects and novel motions. This approach enhances the robot's ability to generalize to new scenarios, offering a significant improvement over traditional language-conditioned and goal-conditioned policies.|
|156|Behavior Transformers: Cloning k modes with one stone|2022|NeurIPS|The primary application discussed in this paper is learning behaviors from distributionally multi-modal data using transformers. The method, named Behavior Transformers (BeT), is designed to handle and model multi-modal continuous action distributions from pre-collected, unlabeled demonstration data. This approach is applicable to various domains, including robotic manipulation and self-driving, where the demonstration data may exhibit multiple modes of behavior.|The foundation model used in this work is a transformer-based sequence model, specifically designed to handle the multi-token prediction of multi-modal actions. BeT uses transformers retrofitted with action discretization via k-means clustering and a multi-task action correction inspired by offset prediction in object detection.|The foundation model plays a crucial role in predicting multi-modal continuous actions from demonstration data. BeT leverages the transformer architecture's ability to model complex sequences and context-based multi-token predictions, enabling it to capture the diverse modes present in human demonstrations. The model discretizes continuous actions into categorical bins and learns to predict both the action bin and a residual offset, which together reconstruct the original continuous action. This allows BeT to model high-dimensional, multi-modal action distributions without relying on complex generative models, improving the handling of diverse behaviors in robotic tasks.|
|157|Leveraging Language for Accelerated Learning of Tool Manipulation|2023|CoRL|The primary application discussed in this paper is accelerating the learning of tool manipulation skills in robots by leveraging linguistic information about the tools. The system, named ATLA (Accelerated Learning of Tool Manipulation with Language), aims to enable robots to adapt quickly to new tools by using natural language descriptions of the tools' properties and common uses, thereby improving the efficiency and generalization of robotic manipulation policies.|The foundation models used in this work include large language models (LLMs) such as GPT-3 for generating detailed and scientific descriptions of tools. Additionally, a pre-trained BERT model is used to convert these descriptions into feature representations that inform the robot's policy learning.|The foundation models play a critical role in providing rich, semantic knowledge about tools, which helps in the faster adaptation of manipulation policies. LLMs are utilized to generate language descriptions that provide information about tool geometry, shape, and common uses. These descriptions are then processed by a BERT model to obtain feature representations that condition the robot's policy learning. This integration of language information allows the robot to generalize its manipulation skills to new, unseen tools, leveraging the shared structure and affordances of the tools described in natural language.|
|158|LATTE: LAnguage Trajectory TransformEr|The primary application discussed in this paper is reshaping robotic trajectories using natural language commands. The system, named LATTE, aims to provide a flexible interface for human-robot interaction, allowing users to modify robotic trajectories in real-time by using free-form text inputs. This method addresses the challenge of combining high-level human semantic instructions with low-level geometric and kinodynamic constraints of robots.|The foundation models used in this work include pre-trained large language models (LLMs) such as BERT and CLIP. These models are employed to encode user intents and target objects directly from natural language inputs and scene images. The system integrates these inputs with geometrical data using a transformer encoder-decoder architecture.|The foundation models play a crucial role in interpreting and aligning semantic information with geometric trajectory data. LATTE leverages BERT and CLIP to process natural language commands and contextual scene images, generating embedded representations that inform the reshaping of robot trajectories. The system's transformer-based architecture then decodes this combined semantic and geometric information to produce modified trajectories that meet user specifications while adhering to safety and dynamic constraints. This approach allows for more intuitive and generalizable control of robotic systems across various platforms, including manipulators, aerial vehicles, and legged robots.|
|159|Real-World Robot Learning with Masked Visual Pre-training|2023|CoRL|The primary application discussed in this paper is real-world robot learning through self-supervised visual pre-training. The approach focuses on using a diverse dataset of real-world images and videos to train robots for various manipulation tasks, including motor control tasks like reaching, pushing, and picking objects in realistic settings.|The foundation model used in this work includes masked autoencoders (MAEs) and vision transformers (ViTs). The MAE framework is employed for self-supervised learning, where the model learns visual representations by reconstructing masked portions of input images. The vision transformers are used as the primary architecture for encoding these visual representations.|The foundation models play a critical role in providing robust visual representations that are used for learning control policies in robotic tasks. The MAE-trained vision transformers serve as the backbone for encoding visual input, which is then used to train control policies through behavior cloning. The pre-trained visual encoders are frozen during policy training, allowing for efficient and effective learning across a range of robotic tasks without overfitting. This setup leverages large-scale, diverse real-world data to improve the sample efficiency and generalization of robotic policies, enabling better performance compared to other approaches like CLIP or supervised ImageNet pre-training.|
|160|GNM: A General Navigation Model to Drive Any Robot|2023|ICRA|The primary application discussed in this paper is general-purpose navigation for a variety of robotic platforms using a single, unified policy. The system, named GNM (General Navigation Model), is designed to enable diverse robots, including those with different sensors and dynamics, to perform vision-based navigation tasks such as exploration, path planning, and goal-reaching in various environments, both indoor and outdoor.|The foundation model used in this work is a goal-conditioned model trained on a large, heterogeneous dataset of navigation trajectories. The GNM employs a shared abstraction across different robots, including a normalized action space that accommodates variations in robot dynamics and capabilities. The architecture integrates temporal context and visual inputs to generalize navigation behaviors across multiple robot embodiments.|The foundation model plays a crucial role in enabling broad generalization across different robotic platforms and environments. The GNM is trained on a diverse set of navigation datasets collected from multiple types of robots, allowing it to learn generalizable navigation policies that can be deployed on new robots without requiring robot-specific data collection. The model uses a combination of topological graphs for high-level planning and image-goal policies for low-level control, ensuring robustness against sensor and actuation variations. This generalization capability is demonstrated through the deployment of the trained GNM on new robots, including a quadrotor, and its performance in challenging scenarios like degraded sensing and actuation.|
|161|From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data|2023|ICLR|The primary application discussed in this paper is learning task-centric behaviors from uncurated robot data, commonly referred to as "play data." The approach aims to extract useful behaviors for robotic tasks from noisy and diverse datasets collected by non-expert human demonstrators, without requiring explicit task labels or reward information.|The foundation model used in this work is a transformer-based model named Conditional Behavior Transformers (C-BeT). C-BeT builds on the Behavior Transformers (BeT) framework, enhancing it with a goal-conditioned policy learning component. The model is designed to handle multi-modal action distributions and can generate a wide range of behaviors from a single dataset by conditioning on future desired states.|The foundation model plays a crucial role in generating and conditioning on desired future states to produce task-specific behaviors. C-BeT leverages the multi-modal generation capabilities of transformers to model the probability distribution of actions given a sequence of states and a desired future observation. This approach allows the model to learn from diverse, uncurated play data and produce targeted behaviors without the need for explicit task labels or rewards, making it particularly useful for real-world applications where such data may not be readily available.|
|162|STAP: Sequencing Task-Agnostic Policies|2023|ICRA|The application of this paper is in robotic skill acquisition and manipulation. The paper focuses on the execution of long-horizon manipulation tasks, which require robots to sequence and coordinate various learned skills effectively. The primary goal is to solve complex tasks involving dependencies between actions that were not seen during training.|The foundation model used in this work is based on Reinforcement Learning (RL), particularly leveraging off-the-shelf RL algorithms to acquire policies and Q-functions for each skill. The skills are represented as task-agnostic policies that can be sequenced to solve novel tasks. The Q-functions are used to estimate the feasibility of the skill sequences.|The foundation model plays a crucial role in planning and executing long-horizon tasks. It does so by: Skill Acquisition: Learning individual skills that are task-agnostic, meaning they are not tailored to specific tasks but can be composed to solve a wide range of tasks. Planning: Utilizing learned Q-functions to optimize sequences of skills for specific tasks, ensuring the geometric feasibility of the entire sequence. Uncertainty Quantification: Integrating techniques to avoid out-of-distribution states, thereby improving the robustness and reliability of the planning process.|
| 163 | Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation | 2023 | CVPR			|  the need for robots to navigate and find arbitrary objects described by people using natural language, without requiring extensive domain-specific navigation training. | utilizes CLIP (Contrastive Language-Image Pre-training) as the foundational model, adapted into the framework called CLIP on Wheels (CoW). | CLIP is used for open-vocabulary object localization, enabling robots to perform zero-shot object navigation by interpreting and acting on natural language descriptions without additional navigation-specific training. |  language-driven zero-shot object navigation (L-ZSON), where robots must locate objects based on natural language descriptions without prior training on those objects or environments. | 
| 164 | Anyloc: Towards universal visual place recognition | 2023 | 		IEEE ROBOTICS AND AUTOMATION LETTERS |  Visual Place Recognition (VPR) which is crucial for robot localization in various environments such as urban, aerial, underwater, and subterranean. | utilizes general-purpose feature representations derived from large-scale pretrained self-supervised models (referred to as foundation models). | serve as a robust substrate for building a universal VPR solution. By using unsupervised feature aggregation methods, the approach AnyLoc achieves significantly higher performance across a wide range of challenging environments and conditions. | the challenge of creating a universal VPR solution that can operate robustly across diverse environments without the need for retraining or fine-tuning, overcoming the limitations of existing environment- and task-specific VPR approaches. |
| 165 | Vln bert: A recurrent vision-and-language bert for navigation | 2021 | 			CVPR | vision-and-language navigation (VLN), specifically aiming to improve a robot's ability to navigate complex environments following human instructions. | a recurrent vision-and-language BERT model, or VLN⟲BERT, which incorporates a recurrent function within the original BERT architecture to manage history-dependent state representations. | reduces memory consumption by controlling self-attention mechanisms, enabling it to be trained on a single GPU. This design allows the model to address both navigation and other vision-and-language tasks simultaneously |  the difficulty of adapting the BERT architecture to the partially observable Markov decision process (POMDP) in VLN, which requires handling history-dependent state representations and decision making. | 
|166  | Language-Conditioned Path Planning | 2023 | CoRL 			|  the problem of path planning in robotic manipulation, particularly in scenarios that require contact with objects in the environment. | introduces Language-Conditioned Collision Functions (LACO), which uses a single-view image, language prompt, and robot configuration to predict collisions. | LACO leverages language prompts to modulate collision predictions, enabling flexible and context-aware path planning. This allows the robot to plan paths that can handle both desired and controlled collisions, enhancing its ability to perform manipulation tasks effectively. | Traditional path planning approaches avoid any collisions, limiting their effectiveness in tasks requiring contact. This paper proposes a method to incorporate contact-awareness into path planning, allowing robots to interact with objects that are safe to collide with while avoiding harmful collisions. |
| 167 | Navgpt: Explicit reasoning in vision-and-language navigation with large language models | 2024 | AAAI 			|  The paper introduces NavGPT, a purely LLM-based instruction-following navigation agent designed to demonstrate the reasoning capabilities of GPT models in complex visual and language navigation tasks. The application focuses on enhancing navigation in embodied robotic agents by leveraging LLMs for high-level planning and decision-making. |  employs GPT models, specifically utilizing the capabilities of large language models (LLMs) such as ChatGPT and GPT-4, as the foundation models for NavGPT. | the LLMs play a crucial role in interpreting visual observations, understanding navigation history, and making real-time decisions for navigation. NavGPT uses the reasoning capabilities of LLMs to deconstruct instructions into sub-goals, integrate commonsense knowledge, identify landmarks, monitor progress, and adapt to anomalies, thus enabling high-level planning and decision-making in navigation tasks. | the ability of large language models (LLMs) to perform high-quality navigational planning and execution based on visual observations, navigation history, and future explorable directions. The paper aims to fill the gap in understanding how LLMs can be applied to vision-and-language navigation (VLN) tasks. |
| 168 | March in chat: Interactive prompting for remote embodied referring expression | 2023 | 			ICCV |   Vision-and-Language Navigation (VLN) to enhance the practical use of robotic agents in real-world environments by making them understand high-level human instructions. | The paper utilizes Large Language Models (LLMs) in conjunction with a novel model called March-in-Chat (MiC). | The LLMs in the MiC model assist in dynamic planning and generating step-by-step navigation instructions based on high-level commands, making the agent's actions adaptable and environment-aware. | the challenge of enabling VLN agents to infer a navigation plan based on short, high-level instructions, which are closer to human commands in practice, particularly in complex and dynamic environments. | 
| 169 | Embodied task planning with large language models | 2023 | 			arxiv | enable embodied agents to execute complex human instructions in realistic environments by integrating commonsense knowledge and visual perception. | using large language models (LLMs) like GPT-3.5 and multimodal models such as LLaVA. |  LLMs are employed to generate action plans from natural language instructions, while the model is fine-tuned with visual context to ensure the plans are executable within the physical constraints of the environment. | the challenge of generating feasible action plans for embodied agents that consider the physical scene constraints, avoiding inapplicable or infeasible actions due to the lack of environmental context. |
| 170 | Text2motion: From natural language instructions to feasible plans | 2023 | 			Autonomous Robots | a framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning using natural language instructions. | involves a hybrid LLM planner that combines a library of learned skills with a geometric feasibility planner to construct plans. | LLM is used to interpret natural language instructions and generate symbolic plans, while the geometric feasibility planner verifies and ensures the plans are executable by resolving geometric dependencies and assessing partial affordance perception. This synergy enhances the overall success rate of complex manipulation tasks. | the challenge of verifying the correctness and feasibility of plans generated by Large Language Models (LLMs) for complex sequential tasks, ensuring these plans can be executed with long-horizon feasibility. |
| 171 | Embodiedgpt: Vision-language pre-training via embodied chain of thought | 2023 | 			NeurIPS | enabling robots to perform long-horizon tasks in physical environments through multi-modal understanding and execution capabilities. | EmbodiedGPT, a multi-modal foundation model that combines visual and language understanding through a large-scale dataset and a 7B parameter large language model (LLM). This model uses techniques like the "Chain of Thoughts" mode and prefix tuning for efficient training and planning. | embodiedGPT improves task execution by generating high-quality plans and extracting effective features from the environment. It forms a closed-loop system integrating high-level planning with low-level control, thereby significantly enhancing the success rate and adaptability of robots in performing embodied tasks. | the challenge of creating a robust, end-to-end multi-modal foundation model (EmbodiedGPT) that enhances the planning and control capabilities of embodied agents, allowing them to execute complex tasks by interpreting high-level language instructions. | 
| 172 | Tptu: Task planning and tool usage of large language model-based ai agents | 2023 | 			NeurIPS-2023 Workshop | the application of Large Language Models (LLMs) as AI agents capable of handling complex tasks by leveraging task planning and tool usage. | utilizes various LLMs, such as GPT-4, as the foundational models for creating AI agents. | LLMs serve as the core controllers in AI agents, enabling them to manipulate tools, interact with users, and execute sub-tasks in a structured framework.  | the insufficiency of LLMs in handling complex tasks that require task planning and tool usage, as well as their integration into real-world applications. | 
| 173 | Robot task planning based on large language model representing knowledge with directed graph structures | 2023 | 			arxiv |  the challenge of task planning for robots in highly unstructured environments, where traditional methods struggle. | uses a Large Language Model (LLM) prompt template, named Think_Net_Prompt, which leverages models like GPT-3 and GPT-4. | The LLM is used to enhance the expressivity of structured professional knowledge and to generate detailed task descriptions and plans. It aids in recursively decomposing tasks and extracting parameters from text descriptions, thereby improving the precision and flexibility of robot task planning in complex environments. | specifically solves the difficulty of planning tasks that involve complex, unstructured environments by proposing a method that decomposes tasks and decouples robot task planning from the binding of executable tasks. | 
| 174 | Multi-skill mobile manipulation for object rearrangement | 2022 | 			arxiv |  long-horizon mobile manipulation tasks for object rearrangement, which involves decomposing a full task into a sequence of subtasks for robotic systems. |  The paper does not explicitly mention the use of a foundation or generative model. Instead, it proposes a modular approach with mobile manipulation skills and a region-goal navigation skill. | The proposed method replaces stationary manipulation skills with mobile ones and trains a navigation skill with region goals. This approach mitigates the compounding errors, provides greater flexibility in interacting with objects from multiple locations, and shows superior performance in long-horizon tasks compared to traditional methods. |  The main problem addressed is the compounding errors in skill chaining when using stationary manipulation skills and navigation skills learned individually on subtasks. This often leads to failure in reaching or manipulating the target objects efficiently. | 
| 175 | Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose Service Robots | 2023 | arxiv 			| develop a robotic system capable of performing General Purpose Service Robot (GPSR) tasks in a daily life environment, specifically within the RoboCup@home competition setting. | utilizes Large Language Models (LLMs) and Vision Language Models (VLMs) as the foundational models. | The foundation models are used for high-level task planning, object detection with open vocabulary, and managing robot actions through a state machine task executable, enabling the robot to perform tasks stably and decisively based on success or failure feedback in real-time environments. | addresses the challenge of enabling robots to understand spoken language commands and generate appropriate life support actions in a home environment. | 
|176 | Robocook: Long-horizon elasto-plastic object manipulation with diverse tools | 2023 | 			arxiv | focuses on developing an intelligent robotic system, RoboCook, which can perceive, model, and manipulate elasto-plastic objects using various tools for tasks such as making dumplings and alphabet letter cookies. | The methods used include Graph Neural Networks (GNNs) for learning complex interactions between soft objects and tools, and a PointNet-based tool classification model. Additionally, a self-supervised policy learning approach is employed for improving task performance. | GNNs are utilized to understand and predict interactions between tools and deformable objects from visual data. The PointNet-based model helps in classifying tools to determine the most appropriate one for a given task. The self-supervised policy learning method enhances the robot's ability to grip, roll, and press, thereby improving its manipulation capabilities and robustness against disturbances. | addresses three main challenges in the application scenario: deformable object manipulation, long-horizon planning, and tool usage. It aims to enable a robot to perform complex soft object manipulation tasks autonomously and efficiently. |
| 177 | All in one: Multi-task prompting for graph neural networks | 2023 | KDD		 |  the application of graph neural networks (GNNs) across various tasks such as social computing, anomaly detection, and network analysis by introducing a multi-task prompting method inspired by natural language processing (NLP). | The approach is based on the concept of prompt learning from NLP, applied to graph tasks. It involves using a pre-trained GNN model and leveraging prompts to adapt it to various downstream tasks efficiently. | the prompt learning method enables the pre-trained GNN model to be adapted quickly to new graph tasks without extensive fine-tuning. This improves the efficiency and reliability of the model in handling multiple tasks, potentially benefiting applications in robotics and reinforcement learning where diverse and adaptive task handling is crucial. | the challenge of negative transfer and inefficiency in pre-training and fine-tuning GNNs for diversified tasks at node, edge, and graph levels. It aims to unify the format of prompts and improve model generalization across different graph tasks. |
| 178 | Exploring large language models for communication games: An empirical study on werewolf | 2023 | arxiv			| engaging large language models (LLMs) in communication games like "Werewolf," which serve as a proxy for natural language communication challenges in fields such as economics, social science, and artificial intelligence. | utilizes large language models (LLMs) like ChatGPT, which remain frozen. The method relies on the retrieval and reflection of past communications and experiences to improve performance in communication games. | The frozen LLMs are used to retrieve and reflect on historical information, enhancing their reasoning abilities and enabling them to make better decisions in communication games. This method allows LLMs to learn from experiences without tuning their parameters, thus addressing the challenge of limited context length and the need for historical context in decision-making. | how to enable LLMs to effectively participate in communication games without fine-tuning their parameters and without relying on human-annotated data. |
|179|LILA: Language-Informed Latent Actions|2022|CoRL|The primary application discussed in this paper is enhancing human-robot collaboration in assistive settings through the use of natural language. The framework, named LILA (Language-Informed Latent Actions), is designed to enable users to control robots using both discrete language inputs and a low-dimensional physical controller. This approach is particularly aimed at assisting individuals with physical disabilities in performing everyday tasks.|The foundation models used in this work include language models such as Distil-RoBERTa for encoding natural language instructions. The system integrates these language models with state-conditioned auto-encoders to create a low-dimensional control space informed by user-provided language instructions.|The foundation model plays a crucial role in interpreting natural language commands and modulating the robot's control space based on these commands. LILA uses language to condition the control space, allowing the robot to disambiguate between different tasks that may have overlapping states but require different actions based on the user's intent. For example, a command like "grab the cereal bowl" results in a control space where one dimension controls the distance to the bowl, while another controls the pose of the robot's gripper. This integration of language and state data helps create more intuitive and efficient control mechanisms for high-dimensional robotic systems, particularly in assistive teleoperation settings.|
|180|Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models|2022|CoRL|The primary application discussed in this paper is enabling robots to acquire new manipulation skills by augmenting language instructions in offline datasets. The approach, named DIAL (Data-driven Instruction Augmentation for Language-conditioned control), leverages vision-language models to propagate knowledge onto large datasets of unlabelled demonstration data, enhancing the ability of imitation learning policies to generalize to new tasks.|The foundation model used in this work includes large-scale pretrained vision-language models (VLMs) such as CLIP. These models are employed to weakly relabel existing offline datasets with additional language instructions, thus providing richer semantic annotations for the data.|The foundation models play a crucial role in generating and augmenting language instructions for robot demonstrations. By fine-tuning CLIP on a small, curated dataset of robot manipulation trajectories with crowd-sourced natural language descriptions, DIAL uses these refined models to label a much larger set of unstructured trajectory data. This relabeling process allows the training of language-conditioned policies that can understand and respond to a broader range of natural language instructions. This method enables robots to learn new skills that were not explicitly annotated in the original dataset, enhancing the generalization and versatility of robotic systems in real-world applications.|
|181|Open-World Object Manipulation using Pre-Trained Vision-Language Models|2023|CoRL|The paper focuses on enabling robots to perform object manipulation tasks in open-world settings, where the robot must handle objects it has never encountered before. The specific challenge addressed is teaching robots to generalize from known objects to new, unseen objects using pre-trained vision-language models (VLMs). The developed approach, named Manipulation of Open-World Objects (MOO), allows robots to interpret and execute instructions involving novel objects, such as identifying and manipulating a "pink stuffed whale" without prior experience with stuffed animals.|The foundation model utilized in this work is a pre-trained vision-language model (VLM), specifically the CLIP model, which is used to extract object-identifying information from images and language commands. The VLM helps the robot understand the semantics of various objects based on their descriptions in human language, leveraging extensive static data available on the internet.|In this context, the VLM serves as a crucial component for grounding language in visual observations. It allows the robot to identify and localize objects within images based on natural language instructions. The VLM's output is used to condition the robot's policy, guiding it to recognize and interact with objects specified in the instructions, even if those objects were not present in the robot's training data. The role of the VLM is to bridge the gap between linguistic descriptions and physical object recognition, enabling robots to handle a diverse range of tasks in real-world environments by understanding the semantic content of instructions and applying this understanding to manipulate unseen objects.|
|182|Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?|2023|NeurIPS|The primary application discussed in this paper is the evaluation and development of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI (EAI). The work aims to determine how effectively these models can support a diverse range of sensorimotor skills, environments, and robot embodiments, focusing on tasks like locomotion, navigation, and manipulation.|The foundation models evaluated in this study include various pre-trained visual representation models like vision transformers (ViTs) using Masked Auto-Encoding (MAE), CLIP, R3M, and others. These models are trained on large-scale datasets including ImageNet, egocentric video datasets, and other sources, designed to generalize across multiple EAI tasks.|The foundation models play a critical role in providing robust and generalized visual representations for a wide range of embodied AI tasks. The study finds that while these models can significantly improve performance compared to learning from scratch, no single model is universally dominant across all tasks. The research highlights the importance of adapting these models to specific domains or tasks, as adaptation can lead to substantial performance improvements. The models are evaluated on a curated benchmark, CORTEXBENCH, which includes 17 tasks across different environments and robot types, demonstrating the strengths and limitations of each model.|
|183|Chain-of-Thought Predictive Control|2024|ICML|The primary application discussed in this paper is generalizable policy learning from demonstrations for complex low-level control tasks, such as contact-rich object manipulations. The system, named Chain-of-Thought Predictive Control (CoTPC), aims to improve the learning and generalization capabilities of imitation learning policies by leveraging sub-optimal demonstrations.|The foundation model used in this work is a transformer-based architecture. Specifically, CoTPC employs a hierarchical imitation learning approach that incorporates subskill discovery and prediction. The model uses transformers to handle the multi-step subskill decomposition of tasks, learning to predict these subskills as a sequence of planning steps, referred to as the chain-of-thought (CoT).|The foundation model plays a critical role in discovering and utilizing subskill-level guidance from demonstrations. CoTPC uses an unsupervised approach to discover subskills from sub-optimal demonstrations by grouping temporally close and functionally similar actions into subskill segments. The model then employs transformers to predict these subskills and use them as guidance for generating low-level actions. This method helps improve the generalization of policies learned from demonstrations by dynamically updating the subskill guidance during task execution. The CoT framework allows the model to handle noisy and discontinuous demonstrations more effectively, improving performance on various challenging low-level control tasks.|
|184|ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes|2023|ICCV|The primary application discussed in this paper is language-grounded task learning with continuous states in realistic 3D scenes. The ARNOLD benchmark aims to evaluate and improve the ability of AI agents to perform manipulation tasks based on natural language instructions. The tasks involve precise object manipulations in varied and realistic 3D environments, making it a comprehensive benchmark for testing language-conditioned policy learning models.|The foundation models used in this work include state-of-the-art language-conditioned policy learning models. These models are evaluated for their ability to ground language in continuous object states and perform manipulations based on those instructions. The benchmark does not specify particular models but rather serves as a testing ground for various models capable of handling the challenges presented by continuous states and complex task descriptions.|The foundation models play a critical role in grounding language instructions into specific, continuous object states and corresponding robot actions. The ARNOLD benchmark assesses models on their ability to generalize learned skills to novel goal states, objects, and scenes, providing a rigorous test of their understanding and execution capabilities. The models must interpret detailed task descriptions, which specify goals in continuous ranges (e.g., opening a drawer to a certain percentage), and execute the corresponding actions in a realistic 3D environment. The performance on ARNOLD highlights the strengths and limitations of current language-conditioned manipulation models in handling real-world complexities and achieving fine-grained control.|
| 185 | Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models | 2023 | CoRL | Learn skill-centric diffusion models and composes their learned distributions to generate long-horizon plans for unseen task skeletons during inference.| Train unconditional diffusion models for each skill to capture the joint distribution of the skill's preconditions, control parameters, and effects. At inference time, linearly link these skill models to form a long-term planning skeleton. | Propose a probabilistic framework that uses a skill-dependent diffusion model to generate long-time horizon plans. We sample from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We use a skill-level generative model to capture the joint distribution of preconditions, skill parameters, and effects for each skill. We use a classifier-guided approach to satisfy specific constraints. | Only validated on a fully observable environment setup with no degree of partial observability and operates on lowdimensional state space of the system i.e. 6-DoF poses of the objects. use a fixed set of primitive skills, and thus, the framework requires either expert data to train models or the pre-trained models to perform compositional planning.                   
| 186 | Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning | 2023 | nips | multi-task reinforcement learning (RL). Specifically, it addresses the challenge of modeling and planning in large-scale multi-task offline RL settings. Traditional RL approaches often focus on single-task scenarios, but this paper aims to develop a method that can handle diverse and multimodal data distributions across multiple tasks.  | leverage the generative capabilities of diffusion models, which have shown success in vision and natural language processing (NLP), and apply them to multi-task RL. Aim to use these models to improve both planning and data synthesis in environments with multiple tasks. | Generative Planning: MTDIFF-P, a variant of the model, outperforms state-of-the-art algorithms in planning across multiple tasks. Data Synthesis: MTDIFF-S, another variant, generates high-quality data for testing tasks given a single demonstration as a prompt, enhancing low-quality datasets and improving performance even on unseen tasks. | Although the model handles diverse datasets, its robustness to highly noisy or extremely low-quality data could be further examined and enhanced.