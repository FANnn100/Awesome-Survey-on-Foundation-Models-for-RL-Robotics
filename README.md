# Awesome-Survey-on-Foundation-Models-for-RL-Robotics

## Paper Taxonomy
| Paper | Year | Conference/Journal | Application | Type of foundation model | The role that VLM plays in robotics/RL |
| --- | --- | --- | --- | --- | --- |
| RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback | 2024 | arxiv | classic control, rigid and articulated object manipulation | VLMs, leveraging vision language foundation models (VLMs) that are trained on diverse, general text and image corpora (e.g., GPT-4V (OpenAI, 2023), Gemini (Team et al., 2023)). | Query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. |
| Foundation Reinforcement Learning: Towards Embodied Generalist Agents With Foundation Prior Assistance | 2023 | arxiv | Foundation Actor-Critic on robotics manipulation tasks | Policy Foundation Prior：Follow the work UniPi, which infers actions based on a language conditioned video prediction model and a universal inverse dynamics model. For the video prediction model, use VLM （Seer, Seer: Language instructed video prediction with latent diffusion models）predicts a video conditioned on one image and a language instruction with latent diffusion models, pre-trained on Something Something V2 (Goyal et al., 2017) and Bridge Data (Ebert et al., 2021) datasets. <br> Value Foundation Prior: utilize VIP (Vip: Towards universal visual reward and representation via value-implicit pre-training) , which trains a universal value function via pre-training on internet-scale robotics datasets. | Build prior knowledge for Policy and Value of RL. |
| Language Conditioned Imitation Learning over Unstructured Data | 2020 | arxiv |Human language conditioned robotic manipulation, a single agent must execute a series of visual manipulation tasks in a row, each expressed in free-form natural language, e.g. “open the door all the way to the right...now grab the block...now push the red button...now open the drawer”. <br> Agents in this scenario are expected to be able to perform any combination of subtasks in any order. Incorporating free-form natural language conditioning into imitation learning, learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network.|Language conditioned visuomotor policy. Text conditioned policies with large pretrained neural language models. | The resulting language conditioned visuomotor policy can follow many free-form human text instructions over a long horizon in a simulated 3D tabletop setting, i.e. “open the door. . . now pick up the block. . . now press the red button” (see video). <br> Combine text conditioned policies with large pretrained neural language models to scale up the number of instructions an agent can follow. |
| Policy adaptation from foundation model feedback | 2023 | CVPR | By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. The policy still fails in most cases given an unseen task or environment. Generalize the learned policy to unseen tasks and environments. | VLM | When deploying the trained policy to a new task or a new environment, let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong,  can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning.|
| CLIPORT : What and Where Pathways for Robotic Manipulation | 2021 | CoRL | End-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. This method is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. ( experiment in the Ravens). CLIPort grounds semantic concepts in precise spatial reasoning, but it is limited to 2D observation and action spaces. | A language-conditioned imitation learning agent that combines the broad semantic understanding (what) of CLIP with the spatial precision (where) of Transporter.| A two-stream architecture with semantic and spatial pathways for vision-based manipulation. Combines the best of both worlds: end-to-end learning for fine-grained manipulation with the multi-goal and multi-task generalization capabilities of vision-language grounding systems. Two-stream architecture for using internet pre-trained vision-language models for conditioning precise manipulation policies with language goals.|
| Perceiver-actor: A multi-task transformer for robotic manipulation | 2022 | CoRL | Exploit the 3D structure of voxel patches for efficient 6-DoF(degrees of freedom) behavior-cloning(BC) with Transformers (analogous to how vision transformers exploit the 2D structure of image patches). Conduct large-scale experiments in the RLBench environment.| A language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. | PERACT encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. PERACT is essentially a classifier trained with supervised learning to detect actions akin to prior work like CLIPort, except our observations and actions are represented with 3D voxels instead of 2D image pixels. Use a Perceiver Transformer to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. |
|Language-driven representation learning for robotics|2023|arxiv|Visual representation learning for robotics.|Multi modal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others,  where Voltron differs, however, is in their novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics.|We then introduce Voltron, a framework for languagedriven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics.|
|CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning| 2022 | arxiv | Novel framework designed to enhance scalability in robot learning using foundation models such as Stable Diffusion. Scaling robot learning, with specific focus on multi-task and multi-scene manipulation in kitchen environments, both in simulation and in the real world. | Visual generative models such as Stable Diffusion pretrained zero-shot visual representation models trained on out-of-domain data. | In the data collection stage, limited in-domain expert demonstration data is collected. In the data augmentation stage, CACTI employs visual generative models such as Stable Diffusion [117] to boost visual diversity by augmenting the data with scene and layout variations. In the visual representation learning stage, CACTI leverages pretrained zero-shot visual representation models trained on out-of-domain data to improve training efficiency. Finally, in the imitation policy training stage, a general multi-task policy is learned using imitation learning on the augmented dataset with compressed visual representations as input. |
| MUTEX: Learning Unified Policies from Multimodal Task Specifications | 2023 | CoRL | Policy learning from multimodal task specifications.| Transformer-based model  can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them.| Trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure.|

