# Awesome-Survey-on-Foundation-Models-for-RL-Robotics

## Paper Taxonomy
| Index | Paper | Year | Conference/Journal | Application | Type of foundation model | The role that VLM plays in robotics/RL |
| --- | --- | --- | --- | --- | --- | --- |
|1| RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback | 2024 | arxiv | classic control, rigid and articulated object manipulation | VLMs, leveraging vision language foundation models (VLMs) that are trained on diverse, general text and image corpora (e.g., GPT-4V (OpenAI, 2023), Gemini (Team et al., 2023)). | Query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. |
|2| Foundation Reinforcement Learning: Towards Embodied Generalist Agents With Foundation Prior Assistance | 2023 | arxiv | Foundation Actor-Critic on robotics manipulation tasks | Policy Foundation Prior：Follow the work UniPi, which infers actions based on a language conditioned video prediction model and a universal inverse dynamics model. For the video prediction model, use VLM （Seer, Seer: Language instructed video prediction with latent diffusion models）predicts a video conditioned on one image and a language instruction with latent diffusion models, pre-trained on Something Something V2 (Goyal et al., 2017) and Bridge Data (Ebert et al., 2021) datasets. <br> Value Foundation Prior: utilize VIP (Vip: Towards universal visual reward and representation via value-implicit pre-training) , which trains a universal value function via pre-training on internet-scale robotics datasets. | Build prior knowledge for Policy and Value of RL. |
|3| Language Conditioned Imitation Learning over Unstructured Data | 2020 | arxiv |Human language conditioned robotic manipulation, a single agent must execute a series of visual manipulation tasks in a row, each expressed in free-form natural language, e.g. “open the door all the way to the right...now grab the block...now push the red button...now open the drawer”. <br> Agents in this scenario are expected to be able to perform any combination of subtasks in any order. Incorporating free-form natural language conditioning into imitation learning, learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network.|Language conditioned visuomotor policy. Text conditioned policies with large pretrained neural language models. | The resulting language conditioned visuomotor policy can follow many free-form human text instructions over a long horizon in a simulated 3D tabletop setting, i.e. “open the door. . . now pick up the block. . . now press the red button” (see video). <br> Combine text conditioned policies with large pretrained neural language models to scale up the number of instructions an agent can follow. |
|4| Policy adaptation from foundation model feedback | 2023 | CVPR | By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. The policy still fails in most cases given an unseen task or environment. Generalize the learned policy to unseen tasks and environments. | VLM | When deploying the trained policy to a new task or a new environment, let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong,  can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning.|
|5| CLIPORT : What and Where Pathways for Robotic Manipulation | 2021 | CoRL | End-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. This method is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. ( experiment in the Ravens). CLIPort grounds semantic concepts in precise spatial reasoning, but it is limited to 2D observation and action spaces. | A language-conditioned imitation learning agent that combines the broad semantic understanding (what) of CLIP with the spatial precision (where) of Transporter.| A two-stream architecture with semantic and spatial pathways for vision-based manipulation. Combines the best of both worlds: end-to-end learning for fine-grained manipulation with the multi-goal and multi-task generalization capabilities of vision-language grounding systems. Two-stream architecture for using internet pre-trained vision-language models for conditioning precise manipulation policies with language goals.|
|6| Perceiver-actor: A multi-task transformer for robotic manipulation | 2022 | CoRL | Exploit the 3D structure of voxel patches for efficient 6-DoF(degrees of freedom) behavior-cloning(BC) with Transformers (analogous to how vision transformers exploit the 2D structure of image patches). Conduct large-scale experiments in the RLBench environment.| A language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. | PERACT encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. PERACT is essentially a classifier trained with supervised learning to detect actions akin to prior work like CLIPort, except our observations and actions are represented with 3D voxels instead of 2D image pixels. Use a Perceiver Transformer to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. |
|7|Language-driven representation learning for robotics|2023|arxiv|Visual representation learning for robotics.|Multi modal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others,  where Voltron differs, however, is in their novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics.|We then introduce Voltron, a framework for languagedriven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics.|
|8|CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning| 2022 | arxiv | Novel framework designed to enhance scalability in robot learning using foundation models such as Stable Diffusion. Scaling robot learning, with specific focus on multi-task and multi-scene manipulation in kitchen environments, both in simulation and in the real world. | Visual generative models such as Stable Diffusion pretrained zero-shot visual representation models trained on out-of-domain data. | In the data collection stage, limited in-domain expert demonstration data is collected. In the data augmentation stage, CACTI employs visual generative models such as Stable Diffusion [117] to boost visual diversity by augmenting the data with scene and layout variations. In the visual representation learning stage, CACTI leverages pretrained zero-shot visual representation models trained on out-of-domain data to improve training efficiency. Finally, in the imitation policy training stage, a general multi-task policy is learned using imitation learning on the augmented dataset with compressed visual representations as input. |
|9| MUTEX: Learning Unified Policies from Multimodal Task Specifications | 2023 | CoRL | Policy learning from multimodal task specifications.| Transformer-based model  can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them.| Trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure.|
|10| Human-Timescale Adaptation in an Open-Ended Task Space | 2023 | ICML | Fast and flexible adaptation. Training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. This work considers navigation, coordination, and division of labor tasks.|Transformers as an architectural choice.| Use Transformers as an architectural choice to scale incontext fast adaptation via model-based RL. An agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. |
|11|R3M: A Universal Visual Representation for Robot Manipulation|2022|CoRL|Visual Representation Learning, Robotic Manipulation. Study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks.| Visual representations.| Visual representation for robot manipulation. Provides pretrained visual representation for robot manipulation using diverse human video datasets such as Ego4D and can be used as a frozen perception module for policy learning in robot manipulation tasks.|
|12|Towards universal visual reward and representation via value-implicit pre-training|2023|ICLR|A key unsolved problem to pre-training for robotic control is the challenge of reward specification. Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks.|Visual representation.| Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. Casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self supervised goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos.|
|13|LIV: Language-Image Representations and Rewards for Robotic Control| 2023|ICML|Vision-language representation and reward learning from action-free videos with text annotations for Robotic Control. Use LIV to pre-train the first control-centric vision-language representation from large human video datasets such as EpicKitchen.|Vision-language representation. Builds on our prior work Value Implicit Pre-Training (VIP),  a selfsupervised approach for learning visual goal-conditioned value functions and representations from videos, generalizing it to learning multi-modal, vision-language values and representations from language-aligned videos, as described above.|Given only a language or image goal, the pre-trained LIV model can assign dense rewards to each frame in videos of unseen robots or humans attempting that task in unseen environments. Further, when some target domain-specific data is available, the same objective can be used to fine-tune and improve LIV and even other pre-trained representations for robotic control and reward specification in that domain.|
|14|Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation|2021|CoRL|Natural Language, Offline RL, Visuomotor Manipulation. Learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. Grounding language(provides a convenient and flexible alternative for task specification) in the robot’s observation space.|Combine language-conditioned reward with visual model predictive control. Pretrained language models.|Learns a language-conditioned reward from offline data and uses it during model predictive control to complete language specified tasks.|
|15| Do As I Can, Not As I Say: Grounding Language in Robotic Affordances | 2022 | CoRL | Large language models can encode a wealth of semantic knowledge about the world, but a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. | LLMs | Provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. <be> They show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. <br> They use the language model to provide task-grounding (Say), enabling the determination of useful sub-goals based on high-level instructions, and a learned affordance function to achieve world-grounding (Can), enabling the identification of feasible actions to execute the plan.|
|16| Inner Monologue: Embodied Reasoning through Planning with Language Models | 2022 | arxiv | Studies the role of grounded environment feedback provided to the LLM, thus closing the loop with the environment. The feedback is used for robot planning with large language models by leveraging a collection of perception models (e.g., scene descriptors and success detectors) in tandem with pretrained language-conditioned robot skills. | LLMs applied to domains beyond natural language processing (such as planning and interaction for robots). | By leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. |
|17|Text2Motion: From Natural Language Instructions to Feasible Plans| 2023|Autonomous Robots|Language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Long-horizon planning, Robot manipulation, Large language models.|LLM Language-based planning.|A language-based planning framework that interfaces an LLM with a library of learned skills and a geometric feasibility planner [8] to solve complex sequential manipulation tasks.|
|18|Voxposer: Composable 3d value maps for robotic manipulation with language models|2023|CoRL|Synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. Manipulation, Large Language Models, Model-based Planning| VLMs | First observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. <br> More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. <br> Given the RGB-D observation of the environment and language instruction, VoxPoser utilizes large language models to generate code, which interacts with vision-language models to extract a sequence of 3D affordance maps and constraint maps. These maps are composed together to create 3D value maps. The value maps are then utilized as objective functions to guide motion planners to synthesize trajectories for everyday manipulation tasks without requiring any prior training or instruction.|
|19|Zero-shot reward specification via grounded natural language|2022|ICML|Learning text-conditioned policies. Robot manipulation tasks. | Large-scale visuolanguage models like CLIP. | Use developments in building large-scale visuo-lingual models like CLIP to devise a framework that generates the task reward signal just from goal text description and raw pixel observations which is then used to learn the task policy.|
|20|Grounding Language with Visual Affordances over Unstructured Data|2023|ICRA|Considers robot manipulation task.|Hierarchical language-conditioned agent that integrates the task-agnostic control of HULC (state-of-the-art language-conditioned imitation learning agent that learns 7DoF goal-reaching policies end-to-end) [10] with the object-centric semantic understanding of VAPO (extracts a self-supervised visual affordance model of unstructured data and not only accelerates learning, but was also shown to boost generalization of downstream control policies).| A self-supervised visuo-lingual affordance model is used to learn general-purposed language conditioned robot skills from unstructured offline data in the real world. <br> Given visual observations and language instructions as input, the affordance model outputs a pixel-wise heat map that represents affordance regions and the corresponding depth map.|
|21|Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception|2023|arxiv|The paper focuses on enhancing robot perception in human-robot interaction scenarios by using vision-language models (VLMs) to integrate inertial measurement unit (IMU) data with visual inputs. Robot perception requires accurate scene understanding, which is challenging when relying on a single modality like vision. The aim is to align the embedding spaces of different modalities (vision and IMU data) to improve scene understanding and reasoning without extensive retraining.|The approach involves extending frozen, pre-trained vision-language models to incorporate IMU data. This is achieved through a combination of supervised and contrastive training, aligning the embedding spaces of different modalities to the vision embedding space. This method leverages the abstract skills learned by large language models (LLMs) during pre-training and applies them to multiple modalities.|The paper addresses the challenge of robot perception by integrating inertial measurement unit (IMU) data with visual inputs using vision-language models (VLMs). By aligning the embedding spaces of different modalities through supervised and contrastive training, the VLM enhances scene understanding and reasoning without retraining. This approach improves robot perception, making VLMs more versatile and effective in various robotic and reinforcement learning tasks.|
|22|Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation|2022|CVPR|The research focuses on Vision-and-Language Navigation (VLN) within the context of both discrete and continuous environments. The primary application is to develop agents that can navigate real-world-like continuous spaces based on human instructions.  The main problem addressed is the domain gap between discrete and continuous VLN environments. Training agents in discrete spaces (which rely on pre-defined connectivity graphs) is significantly easier than in continuous spaces (which are more realistic but lack such graphs). This gap hinders the transferability and generalization of VLN agents trained in one environment to another.| The research utilizes a candidate waypoints predictor trained with refined connectivity graphs from Matterport3D to Habitat-Matterport3D environments. This predictor generates a set of accessible waypoints during navigation in continuous spaces. The models include: Cross-Modal Matching Agent (CMA) VLN\BERT. The training approach employs a simple imitation learning objective to enhance agent performance.|This research addresses the domain gap between discrete and continuous Vision-and-Language Navigation (VLN) environments by using a candidate waypoints predictor trained on refined connectivity graphs. The predictor generates navigable waypoints in continuous environments, enabling agents to perform high-level actions and navigate effectively based on visual observations. This approach enhances the transferability and generalization of VLN agents, allowing them to operate more robustly in realistic, continuous spaces, thereby improving their applicability in real-world robotics and reinforcement learning scenarios.|
|23|NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation|2023|arxiv|The application addressed in this paper is Vision-and-Language Navigation (VLN), which involves enabling AI agents to navigate unseen environments by following linguistic instructions. The primary aim is to improve generalization in navigation tasks, ensuring that agents can effectively operate in diverse and previously unencountered environments, ranging from out-of-distribution scenes to real-world applications without relying on maps, odometers, or depth inputs.|The foundation model used in this paper is a video-based large Vision-Language Model (VLM) called NaVid. This model leverages pre-trained vision encoders and large language models (LLMs) to interpret visual data from a monocular RGB camera and human-issued instructions. NaVid integrates these inputs to make real-time navigation decisions, bypassing the need for traditional navigation aids like maps and odometer data.|In this work, the VLM plays a critical role in enhancing the generalization and adaptability of robotic navigation systems. By encoding historical observations and spatio-temporal contexts from video data, NaVid provides a robust framework for action planning and instruction following. The VLM allows the robot to understand and execute navigation tasks based on visual observations and natural language instructions, addressing the challenges of odometer noise and domain discrepancies, thus facilitating smoother Sim2Real transitions and improving overall navigation performance in real-world settings.|
|24|Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs|2023|CoRL|The paper introduces the Open-Vocabulary 3D Scene Graph (OVSG), a framework aimed at enhancing robotic systems' ability to ground various entities, such as objects, agents, and regions, in real-world scenarios using free-form text queries. The key application is to improve context-aware localization, enabling robots to understand and execute complex commands like "pick up a cup on the kitchen table" or "navigate to a sofa where someone is sitting". This addresses the challenge of grounding semantic entities in real-world navigation and manipulation tasks, thereby enhancing the practical functionality of robots in everyday environments.|The foundation model used in this paper combines open-vocabulary detection with 3D scene graphs. By leveraging the strengths of 3D scene graphs, which encode spatial and relational context among objects, and incorporating an open-vocabulary approach, OVSG can handle a broader range of queries, including those involving unseen semantic categories and relationships. This hybrid approach enhances the system's ability to accurately ground entities in varied and dynamic real-world settings.|In this work, Vision-Language Models (VLMs) play a critical role in bridging the gap between natural language queries and robotic perception. By integrating VLMs with 3D scene graphs, the system can interpret complex, context-aware commands and translate them into actionable tasks for robots. This integration allows robots to utilize semantic and spatial information more effectively, enabling precise localization and interaction with objects based on detailed human instructions. Consequently, VLMs significantly enhance the robots' capability to understand and respond to open-vocabulary queries in reinforcement learning and real-world applications.|
|25|Interactive Language: Talking to Robots in Real Time|2022|arxiv|The primary application of this paper is to develop a framework for real-time, natural language-instructable robots capable of executing a diverse array of tasks in the real world. These tasks range from basic commands to complex, long-horizon manipulation goals. The framework aims to produce robots that can proficiently understand and act upon natural language instructions, enabling them to perform tasks that require precise and dynamic control in real-time.|The foundation model utilized in this paper is based on behavioral cloning, trained on a vast dataset of language-annotated trajectories. This model leverages imitation learning techniques to interpret and execute natural language commands. The approach integrates components such as event-selectable hindsight relabeling and combines existing methods to create a scalable and efficient recipe for learning a wide range of language-conditionable skills.|In this work, the Vision-Language Model (VLM) plays a crucial role in bridging the gap between natural language instructions and robotic actions. VLMs facilitate the robot's understanding of complex, context-rich commands by mapping language inputs to corresponding visual and motor actions. This enables the robot to perform continuous-control visuo-motor manipulation in real-time, effectively responding to new instructions during ongoing task execution. By incorporating VLMs, the framework demonstrates improved interaction capabilities and task performance, allowing the robot to handle a broader spectrum of language-guided tasks in dynamic environments.|
