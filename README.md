# Awesome-Survey-on-Foundation-Models-for-RL-Robotics
This is the official repository of "Unifying Modern AI with Robotics: Survey on MDPs with Diffusion and Foundation Models".

## Relevant Surveys
| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| [A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093)    | arxiv    | 2024 | [Web](https://github.com/yueen-ma/awesome-vla)|
| [Large Multimodal Agents: A Survey](https://arxiv.org/abs/2402.15116)    | arxiv    | 2024 | [Web](https://github.com/jun0wanan/awesome-large-multimodal-agents)|
| [Agent AI: Surveying the Horizons of Multimodal Interaction](https://arxiv.org/abs/2401.03568)    | arxiv    | 2024 | - |
| [Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](https://arxiv.org/abs/2407.06886)    | arxiv    | 2024 | [Web](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List) |
| [A survey on large language model based autonomous agents](https://arxiv.org/abs/2308.11432)    | Front. Comput. Sci   | 2024 | [Web](https://github.com/Paitesanshi/LLM-Agent-Survey) |
| [A Survey on Robotics with Foundation Models: toward Embodied AI](https://arxiv.org/abs/2402.02385)    | arxiv   | 2024 | - |
| [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)    | arxiv   | 2024 | - |
| [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)    | arxiv    | 2023 | [Web](https://robotics-fm-survey.github.io/) |





## Planning

| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| PSL: [Plan-seq-learn: Language model guided rl for solving long horizon robotics tasks](https://arxiv.org/abs/2405.01534)                                                            | ICLR-LLMAgents         | 2024 | [Code](https://github.com/mihdalal/planseqlearn)|
| TWOSOME: [True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning](https://arxiv.org/abs/2401.14151)                                                        | ICLR                   | 2024 | [Code](https://github.com/WeihaoTan/TWOSOME) |
| POLIFORMER: [PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators](https://arxiv.org/abs/2406.20083)                                              | CoRL                   | 2024 | [Code](https://github.com/allenai/poliformer) |
| LLM-MCTS: [Large Language Models as Commonsense Knowledge for Large-Scale Task Planning](https://arxiv.org/abs/2305.14078)                                                       | NeurIPS                | 2024 | [Code](https://github.com/1989Ryan/llm-mcts) |
| Lang2LTL-2: [Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models](https://semrob.github.io/docs/rss_semrob2024_cr_paper23.pdf)                    | CoRL Workshop          | 2024 | [Code](https://github.com/h2r/Lang2LTL-2), [Data](https://drive.google.com/drive/folders/1gWomkuVqxLU01ftzF34bEacJBeUwBMOf) |
| LGMCTS: [LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement](https://arxiv.org/pdf/2309.15821v2)                              | IROS                   | 2023 |[Code](https://github.com/changhaonan/LGMCTS-D)|
| DoReMi: [DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment](https://arxiv.org/abs/2307.00329)                                 | IROS                   | 2023 |[Web](https://sites.google.com/view/doremi-paper)|
| CoELA: [Building Cooperative Embodied Agents Modularly with Large Language Models](https://arxiv.org/abs/2307.02485)                                                          | ICLR                   | 2023 | [Web](https://vis-www.cs.umass.edu/Co-LLM-Agents)|
| GD: [Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents](https://arxiv.org/abs/2303.00855)                                                              | NeurIPS                | 2023 |[Web](grounded-decoding.github.io)|
| UNREST: [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments](https://arxiv.org/abs/2309.16397)                                                  | CoRL                   | 2023 |[Code](https://github.com/Emiyalzn/CoRL24-UNREST)|
| Text2Motion: [Text2motion: From natural language instructions to feasible plans](https://arxiv.org/abs/2303.12153)                                                     | Autonomous Robots      | 2023 |[Web](https://sites.google.com/stanford.edu/text2motion)|
| GLAM: [Grounding large language models in interactive environments with online reinforcement learning](https://arxiv.org/abs/2302.02662)                                                | ICML                   | 2023 |[Code](https://github.com/flowersteam/Grounding_LLMs_with_online_RL)|
| UniSim: [Learning interactive real-world simulators](https://arxiv.org/abs/2310.06114)                                                          | ICLR                   | 2023 |[Web](https://universal-simulator.github.io/unisim/)|
| Retroformer: [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://arxiv.org/abs/2308.02151)                    | ICLR                   | 2023 |[Code](https://github.com/weirayao/Retroformer)|
| ALFRED: [Egocentric planning for scalable embodied task achievement](https://arxiv.org/abs/2306.01295)                                                         | NeurIPS                | 2023 | -    |
| SayCan: [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691)                                                          | CoRL                   | 2022 | [Web](say-can.github.io)|
| Inner Monologue: [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608)                                              | CoRL                   | 2022 |[Web](https://innermonologue.github.io/)|

## Exploration
| Title                              | Venue                | Date | Source |
|------------------------------------|----------------------|------|------|
| RoboEXP: [RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation](https://arxiv.org/abs/2402.15487)                       | arxiv                | 2024 |[Web](https://jianghanxiao.github.io/roboexp-web/)|
| IGE-LLMs: [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks](https://arxiv.org/abs/2309.16347)                     | ICRA                 | 2024 | -    |
| ELLM: [Guiding pretraining in reinforcement learning with large language models](https://arxiv.org/abs/2302.06692)                         | ICML                 | 2023 |[Code](https://github.com/yuqingd/ellm)|
| Motif: [Motif: Intrinsic motivation from artificial intelligence feedback](https://arxiv.org/abs/2310.00166)                        | arxiv                | 2023 |[Code](https://github.com/facebookresearch/motif)|



## Reasoning
| Title                              | Venue                | Date | Source  |
|------------------------------------|----------------------|------|------|
| HAZARD: [HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments](https://arxiv.org/abs/2401.12975)                        | ICLR                 | 2024 |[Code](https://github.com/UMass-Embodied-AGI/HAZARD)|
| PR2L: [Vision-language models provide promptable representations for reinforcement learning](https://arxiv.org/abs/2402.02651)                           | arxiv                | 2024 |[Web](https://pr2l.github.io/)|
| CAPE: [Cape: Corrective actions from precondition errors using large language models](https://arxiv.org/abs/2211.09935)                          | ICRA                 | 2024 |[Web](https://shreyas-s-raman.github.io/CAPE/)|
| SayCanPay: [SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge](https://arxiv.org/abs/2308.12682)                   | AAAI                 | 2024 |[Web](https://rishihazra.github.io/SayCanPay/)|
| MASE: [Multi-agent strategy explanations for human-robot collaboration](https://arxiv.org/abs/2311.11955)                        | ICRA                 | 2024 | -    |
| FoMo: [FoMo Rewards: Can we cast foundation models as reward functions?](https://arxiv.org/abs/2312.03881)                   | NeurIPS Workshop     | 2023 | -    |
| Language to Rewards: [Language to rewards for robotic skill synthesis](https://arxiv.org/abs/2306.08647)         | CoRL                 | 2023 |[Web](https://language-to-reward.github.io/)|




