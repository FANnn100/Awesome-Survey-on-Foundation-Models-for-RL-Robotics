# Awesome-Survey-on-Foundation-Models-for-RL-Robotics

## Paper Taxonomy
| Paper | Year | Conference/Journal | Application | Type of foundation model | The role that VLM plays in robotics/RL |
| --- | --- | --- | --- | --- | --- |
| RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback | 2024 | arxiv | classic control, rigid and articulated object manipulation | VLMs, leveraging vision language foundation models (VLMs) that are trained on diverse, general text and image corpora (e.g., GPT-4V (OpenAI, 2023), Gemini (Team et al., 2023)). | Query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. |
| Foundation Reinforcement Learning: Towards Embodied Generalist Agents With Foundation Prior Assistance | 2023 | arxiv | Foundation Actor-Critic on robotics manipulation tasks | Policy Foundation Prior：Follow the work UniPi, which infers actions based on a language conditioned video prediction model and a universal inverse dynamics model. For the video prediction model, use VLM （Seer, Seer: Language instructed video prediction with latent diffusion models）predicts a video conditioned on one image and a language instruction with latent diffusion models, pre-trained on Something Something V2 (Goyal et al., 2017) and Bridge Data (Ebert et al., 2021) datasets. <br> Value Foundation Prior: utilize VIP (Vip: Towards universal visual reward and representation via value-implicit pre-training) , which trains a universal value function via pre-training on internet-scale robotics datasets. | Build prior knowledge for Policy and Value of RL. |
| Language Conditioned Imitation Learning over Unstructured Data | 2020 | arxiv |Human language conditioned robotic manipulation, a single agent must execute a series of visual manipulation tasks in a row, each expressed in free-form natural language, e.g. “open the door all the way to the right...now grab the block...now push the red button...now open the drawer”. <br> Agents in this scenario are expected to be able to perform any combination of subtasks in any order. Incorporating free-form natural language conditioning into imitation learning, learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network.|Language conditioned visuomotor policy. Text conditioned policies with large pretrained neural language models. | The resulting language conditioned visuomotor policy can follow many free-form human text instructions over a long horizon in a simulated 3D tabletop setting, i.e. “open the door. . . now pick up the block. . . now press the red button” (see video). <br> Combine text conditioned policies with large pretrained neural language models to scale up the number of instructions an agent can follow. |
| Policy adaptation from foundation model feedback | 2023 | CVPR | By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. The policy still fails in most cases given an unseen task or environment. Generalize the learned policy to unseen tasks and environments. | VLM | When deploying the trained policy to a new task or a new environment, let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong,  can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning.|
