# Awesome-Survey-on-Foundation-Models-for-RL-Robotics
This is the official repository of "Unifying Modern AI with Robotics: Survey on MDPs with Diffusion and Foundation Models".
<img width="1073" alt="image" src="https://github.com/user-attachments/assets/6cda6ff4-c451-4481-a18d-7dc30b81e19a" />

## Relevant Surveys
| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| [A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093)    | arxiv    | 2024 | [Web](https://github.com/yueen-ma/awesome-vla)|
| [Large Multimodal Agents: A Survey](https://arxiv.org/abs/2402.15116)    | arxiv    | 2024 | [Web](https://github.com/jun0wanan/awesome-large-multimodal-agents)|
| [Agent AI: Surveying the Horizons of Multimodal Interaction](https://arxiv.org/abs/2401.03568)    | arxiv    | 2024 | - |
| [Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](https://arxiv.org/abs/2407.06886)    | arxiv    | 2024 | [Web](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List) |
| [A survey on large language model based autonomous agents](https://arxiv.org/abs/2308.11432)    | Front. Comput. Sci   | 2024 | [Web](https://github.com/Paitesanshi/LLM-Agent-Survey) |
| [A Survey on Robotics with Foundation Models: toward Embodied AI](https://arxiv.org/abs/2402.02385)    | arxiv   | 2024 | - |
| [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)    | arxiv   | 2024 | - |
| [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)    | arxiv    | 2023 | [Web](https://robotics-fm-survey.github.io/) |





## Planning

| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| DPPO: [Diffusion Policy Policy Optimization](https://openreview.net/pdf?id=mEpqHvbD2h)                              | ICLR                   | 2025 |[Code](https://diffusion-ppo.github.io/)|
| DART: [A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control](https://openreview.net/pdf?id=XNA3Mnnbvb)                              | ICLR                   | 2025 |[Code](https://zkf1997.github.io/DART/)|
| SafeDiffuser: [SafeDiffuser: Safe Planning with Diffusion Probabilistic Models](https://openreview.net/forum?id=ig2wk7kK9J)                              | ICLR                   | 2025 |[Code](https://safediffuser.github.io/safediffuser/)|
| SMILING: [Diffusing States and Matching Scores: A New Framework for Imitation Learning](https://openreview.net/pdf?id=kWRKNDU6uN)                              | ICLR                   | 2025 |[Code](https://github.com/ziqian2000/SMILING)|
| DV: [What Makes a Good Diffusion Planner for Decision Making?](https://openreview.net/pdf?id=7BQkXXM8Fy)                              | ICLR                   | 2025 |[Code](https://github.com/Josh00-Lu/DiffusionVeteran)|
| Diffusion Planner: [Diffusion-Based Planning for Autonomous Driving with Flexible Guidance](https://openreview.net/pdf?id=wM2sfVgMDH)                              | ICLR                   | 2025 |[Code](https://zhengyinan-air.github.io/Diffusion-Planner/)|
| MMD: [Multi-Robot Motion Planning with Diffusion Models](https://openreview.net/pdf?id=AUCYptvAf3)                              | ICLR                   | 2025 |[Code](https://multi-robot-diffusion.github.io/)|
| Instant Policy: [Instant Policy: In-Context Imitation Learning via Graph Diffusion](https://openreview.net/pdf?id=je3GZissZc)                              | ICLR                   | 2025 |[Code](https://www.robot-learning.uk/instant-policy)|
| Diffusion-ES: [Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following](https://arxiv.org/pdf/2403.10794)                              | CVPR                   | 2024 |[Code](https://github.com/bhyang/diffusion-es)|
| AlignDiff: [AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model](https://arxiv.org/pdf/2310.02054) | ICLR | 2024 | [code](https://aligndiff.github.io/) |
| LDCQ: [Reasoning with Latent Diffusion in Offline Reinforcement Learning](https://openreview.net/pdf?id=tGQirjzddO) | ICLR | 2024 | [code](https://github.com/ldcq/ldcq) |
| LatentDiffuser: [Efficient Planning with Latent Diffusion](https://arxiv.org/pdf/2310.00311) | ICLR | 2024 | - |
| SRPO: [Score Regularized Policy Optimization through Diffusion Behavior](https://arxiv.org/pdf/2310.07297) | ICLR | 2024 | [Code](https://github.com/thu-ml/SRPO) |
| FTB: [Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation](https://openreview.net/pdf?id=EG68RSznLT) | ICLR | 2024 | [Code](https://github.com/Zzl35/flow-to-better) |
| FISOR: [Safe offline reinforcement learning with feasibility-guided diffusion model](https://arxiv.org/pdf/2401.10700)| ICLR | 2024 | [Code](https://zhengyinan-air.github.io/FISOR/) |
| HD: [Simple Hierarchical Planning with Diffusion](https://arxiv.org/pdf/2401.02644) | ICLR | 2024 | [Code](https://github.com/changchencc/Simple-Hierarchical-Planning-with-Diffusion/tree/dmc?tab=readme-ov-file) |
| SuSIE: [Zero-shot robotic manipulation with pretrained image-editing diffusion models](https://arxiv.org/pdf/2310.10639)                                                            | ICLR        | 2024 | [Code](https://rail-berkeley.github.io/susie/)|
| PSL: [Plan-seq-learn: Language model guided rl for solving long horizon robotics tasks](https://arxiv.org/abs/2405.01534)                                                            | ICLR-LLMAgents         | 2024 | [Code](https://github.com/mihdalal/planseqlearn)|
| TWOSOME: [True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning](https://arxiv.org/abs/2401.14151)                                                        | ICLR                   | 2024 | [Code](https://github.com/WeihaoTan/TWOSOME) |
| NoMaD: [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration](https://arxiv.org/pdf/2310.07896) | ICRA | 2024 | [Code](https://general-navigation-models.github.io/) |
| Potential-model-plan: [Potential Based Diffusion Motion Planning](https://arxiv.org/pdf/2407.06169) | ICML | 2024 | [Code](https://energy-based-model.github.io/potential-motion-plan/) |
| RGG: [Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans](https://arxiv.org/pdf/2310.19427) | NIPS | 2024 | [Code](https://github.com/leekwoon/rgg) |
| MADIFF: [MADiff: Offline Multi-agent Learning with Diffusion Models](https://arxiv.org/pdf/2305.17330) | NIPS | 2024 | [Code](https://github.com/zbzhu99/madiff) |
| DiMSam: [DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability](https://arxiv.org/pdf/2306.13196) | IROS | 2024 | [Code](https://sites.google.com/view/dimsam-tamp) |
| POLIFORMER: [PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators](https://arxiv.org/abs/2406.20083)                                              | CoRL                   | 2024 | [Code](https://github.com/allenai/poliformer) |
| LLM-MCTS: [Large Language Models as Commonsense Knowledge for Large-Scale Task Planning](https://arxiv.org/abs/2305.14078)                                                       | NeurIPS                | 2024 | [Code](https://github.com/1989Ryan/llm-mcts) |
| Lang2LTL-2: [Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models](https://semrob.github.io/docs/rss_semrob2024_cr_paper23.pdf)                    | CoRL Workshop          | 2024 | [Code](https://github.com/h2r/Lang2LTL-2), [Data](https://drive.google.com/drive/folders/1gWomkuVqxLU01ftzF34bEacJBeUwBMOf) |
| CADENCE: [Diffusion models for multi-target adversarial tracking](https://arxiv.org/pdf/2307.06244) | MRS | 2023 |[code]() |
| BESO: [Goal-Conditioned Imitation Learning using Score-based Diffusion Policies](https://arxiv.org/pdf/2304.02532) | RSS | 2023 | [Code](https://intuitive-robots.github.io/beso-website/) |
| Diffusion-Policy: [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/pdf/2303.04137) | RSS | 2023 | [Code](https://diffusion-policy.cs.columbia.edu/) |
| SceneDiffuser: [Diffusion-based Generation, Optimization, and Planning in 3D Scenes](https://scenediffuser.github.io/)                              | CVPR                   | 2023 |[Code](https://github.com/scenediffuser/Scene-Diffuser)|
| Diffusion-RL: [Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games](https://arxiv.org/pdf/2403.10794)                              | IROS                   | 2023 |[Code](https://github.com/CORE-Robotics-Lab/Opponent-Modeling)|
| LGMCTS: [LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement](https://arxiv.org/pdf/2309.15821v2)                              | IROS                   | 2023 |[Code](https://github.com/changhaonan/LGMCTS-D)|
| DoReMi: [DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment](https://arxiv.org/abs/2307.00329)                                 | IROS                   | 2023 |[Web](https://sites.google.com/view/doremi-paper)|
| CoELA: [Building Cooperative Embodied Agents Modularly with Large Language Models](https://arxiv.org/abs/2307.02485)                                                          | ICLR                   | 2023 | [Web](https://vis-www.cs.umass.edu/Co-LLM-Agents)|
| MTDIFF: [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/ccda3c632cc8590ee60ca5ba226a4c30-Paper-Conference.pdf) | NeurIPS | 2023 | [Code](https://github.com/tinnerhrhe/MTDiff) |
| GD: [Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents](https://arxiv.org/abs/2303.00855)                                                              | NeurIPS                | 2023 |[Web](grounded-decoding.github.io)|
| GSC: [Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models](https://proceedings.mlr.press/v229/mishra23a/mishra23a.pdf) | CoRL | 2023 | [Code](https://generative-skill-chaining.github.io/) |
| UNREST: [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments](https://arxiv.org/abs/2309.16397)                                                  | CoRL                   | 2023 |[Code](https://github.com/Emiyalzn/CoRL24-UNREST)|
| Text2Motion: [Text2motion: From natural language instructions to feasible plans](https://arxiv.org/abs/2303.12153)                                                     | Autonomous Robots      | 2023 |[Web](https://sites.google.com/stanford.edu/text2motion)|
| TREBI: [Safe Offline Reinforcement Learning with Real-Time Budget Constraints](https://arxiv.org/abs/2306.00603) | ICML | 2023 | [Code](https://github.com/qianlin04/Safe-offline-RL-with-diffusion-model) |
| AdaptDiffuser: [AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners](https://arxiv.org/pdf/2302.01877) | ICML | 2023 | [Code](https://github.com/Liang-ZX/AdaptDiffuser) |
| MetaDiffuser: [MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL](https://proceedings.mlr.press/v202/ni23a/ni23a.pdf) | ICML | 2023 | [Code](https://metadiffuser.github.io/) |
| CEP: [Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning](https://proceedings.mlr.press/v202/lu23d/lu23d.pdf) | ICML | 2023 | [Code](https://github.com/thu-ml/CEP-energy-guided-diffusion) |
| GLAM: [Grounding large language models in interactive environments with online reinforcement learning](https://arxiv.org/abs/2302.02662)                                                | ICML                   | 2023 |[Code](https://github.com/flowersteam/Grounding_LLMs_with_online_RL)|
| Decision-Diffuser: [Is Conditional Generative Modeling all you need for Decision-Making?](https://arxiv.org/pdf/2211.15657)                                                            | ICLR        | 2023 | [Code](https://anuragajay.github.io/decision-diffuser/)|
| UniSim: [Learning interactive real-world simulators](https://arxiv.org/abs/2310.06114)                                                          | ICLR                   | 2023 |[Web](https://universal-simulator.github.io/unisim/)|
| Retroformer: [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://arxiv.org/abs/2308.02151)                    | ICLR                   | 2023 |[Code](https://github.com/weirayao/Retroformer)|
| HDMI: [Hierarchical diffusion for offline decision making](https://proceedings.mlr.press/v202/li23ad/li23ad.pdf)  | ICML | 2023 | - |
| ALFRED: [Egocentric planning for scalable embodied task achievement](https://arxiv.org/abs/2306.01295)                                                         | NeurIPS                | 2023 | -    |
| Diffuser: [Planning with diffusion for flexible behavior synthesis](https://arxiv.org/pdf/2205.09991)                                                            | ICML        | 2022 | [Code](https://diffusion-planning.github.io/)|
| SayCan: [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691)                                                          | CoRL                   | 2022 | [Web](say-can.github.io)|
| Inner Monologue: [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608)                                              | CoRL                   | 2022 |[Web](https://innermonologue.github.io/)|
| LLaRA: [LLaRA: Supercharging Robot Learning Data for Vision-Language Policy](https://arxiv.org/abs/2406.20095) | ICLR | 2025 | [Code](https://github.com/LostXine/LLaRA)|

## Perception
| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| LocoTransformer: [Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers](https://arxiv.org/abs/2107.03996)                 | ICLR        | 2022 | [Code](https://github.com/Mehooz/vision4leg)|
| AVLEN: [Avlen: Audio-visual-language embodied navigation in 3d environments](https://proceedings.neurips.cc/paper_files/paper/2022/hash/28f699175783a2c828ae74d53dd3da20-Abstract-Conference.html) | NeurIPS | 2022 |  - |
| DetCLIP: [Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection](https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ba960559212691be13fa81d9e5e0047-Abstract-Conference.html) | NeurIPS | 2022 | - |
| PointCLIP: [Pointclip: Point cloud understanding by clip](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.html) | CVPR | 2022 | [Code](https://github.com/ZrrSkywalker/PointCLIP) |
| VIP: [Vip: Towards universal visual reward and representation via value-implicit pre-training](https://arxiv.org/abs/2210.00030) | ICLR | 2023 | [Code](https://github.com/facebookresearch/vip) |
| OWL-ViT: [Simple open-vocabulary object detection](https://link.springer.com/chapter/10.1007/978-3-031-20080-9_42) | ECCV | 2022 | [Code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) |
| CoOp: [Learning to prompt for vision-language models](https://link.springer.com/article/10.1007/s11263-022-01653-1) | IJCV | 2022 | [Code](https://github.com/KaiyangZhou/CoOp) |
| OVRL: [Offline visual representation learning for embodied navigation](https://openreview.net/forum?id=Spfbts_vNY) | ICLR workshop | 2023 | - |
| DreamerV3: [Mastering diverse domains through world models](https://arxiv.org/abs/2301.04104) | arxiv | 2023 | - |
| VC-1: [Where are we in the search for an artificial visual cortex for embodied intelligence?](https://proceedings.neurips.cc/paper_files/paper/2023/hash/022ca1bed6b574b962c48a2856eb207b-Abstract-Conference.html) | NeurIPS | 2023 | [Code](https://github.com/facebookresearch/eai-vc)|
| Grounding DINO: [Grounding dino: Marrying dino with grounded pre-training for open-set object detection](https://link.springer.com/chapter/10.1007/978-3-031-72970-6_3) | ECCV | 2024 | [Code](https://github.com/IDEA-Research/GroundingDINO) |
| LLaRP: [Large language models as generalizable policies for embodied tasks](https://openreview.net/forum?id=u6imHU4Ebu) | ICLR | 2023 | [Code](https://github.com/apple/ml-llarp) |
| LIV: [Liv: Language-image representations and rewards for robotic control](https://proceedings.mlr.press/v202/ma23b.html) | ICML | 2023 | [Code](https://github.com/penn-pal-lab/LIV) |
| NaVid: [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/abs/2402.15852) | RSS | 2024 | [Code](https://github.com/jzhzhang/NaVid-VLN-CE) |
| LLM2Automata: [Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception](https://arxiv.org/pdf/2308.05295) | AAMAS | 2024 | - |
| CONPE: [Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad72633e034990a97e878fc2fc100afb-Abstract-Conference.html) | NeurIPS | 2024 | - |
| CLIP-Adapter: [Clip-adapter: Better vision-language models with feature adapters](https://link.springer.com/article/10.1007/s11263-023-01891-x) | IJCV | 2024 | [Code](https://github.com/gaopengcuhk/CLIP-Adapter) | 
| PAC: [Offline actor-critic reinforcement learning scales to large models](https://arxiv.org/abs/2402.05546) | ICML | 2024 | - |
| LLM-POP: [Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks](https://ieeexplore.ieee.org/abstract/document/10610981) | ICRA | 2024 | - |
| ImagineNav: [ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination](https://arxiv.org/abs/2410.09874) | ICLR | 2025 | - |
| LAPA: [Latent action pretraining from videos](https://arxiv.org/abs/2410.11758) | ICLR | 2025 | [Web](https://latentactionpretraining.github.io/) |
| TraceVLA: [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://arxiv.org/pdf/2412.10345) | ICLR | 2025 | - |
| HAMSTER: [HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation](https://arxiv.org/abs/2502.05485v3) | ICLR | 2025 | [Web](https://hamster-robot.github.io/) |
| X-Fi: [X-FI: A Modality-Invariant Foundation Model for Multimodal Human Sensing](https://arxiv.org/abs/2410.10167) | ICLR | 2025 | [Code](https://xyanchen.github.io/X-Fi/) |
| TinyVLA: [TinyVLA: Toward Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation](https://ieeexplore.ieee.org/abstract/document/10900471) | RA-L | 2025 | [Web](https://tiny-vla.github.io/) |

## Exploration
| Title                              | Venue                | Date | Source |
|------------------------------------|----------------------|------|------|
| RoboEXP: [RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation](https://arxiv.org/abs/2402.15487)                       | arxiv                | 2024 |[Web](https://jianghanxiao.github.io/roboexp-web/)|
| IGE-LLMs: [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks](https://arxiv.org/abs/2309.16347)                     | ICRA                 | 2024 | -    |
| ELLM: [Guiding pretraining in reinforcement learning with large language models](https://arxiv.org/abs/2302.06692)                         | ICML                 | 2023 |[Code](https://github.com/yuqingd/ellm)|
| Motif: [Motif: Intrinsic motivation from artificial intelligence feedback](https://arxiv.org/abs/2310.00166)                        | arxiv                | 2023 |[Code](https://github.com/facebookresearch/motif)|
| Diffusion-QL: [Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning](https://arxiv.org/pdf/2208.06193)                        | ICLR                | 2023 |[Code](https://github.com/Zhendong-Wang/Diffusion-Policies-for-Offline-RL)|



## Reasoning
| Title                              | Venue                | Date | Source  |
|------------------------------------|----------------------|------|------|
| HAZARD: [HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments](https://arxiv.org/abs/2401.12975)                        | ICLR                 | 2024 |[Code](https://github.com/UMass-Embodied-AGI/HAZARD)|
| PR2L: [Vision-language models provide promptable representations for reinforcement learning](https://arxiv.org/abs/2402.02651)                           | arxiv                | 2024 |[Web](https://pr2l.github.io/)|
| CAPE: [Cape: Corrective actions from precondition errors using large language models](https://arxiv.org/abs/2211.09935)                          | ICRA                 | 2024 |[Web](https://shreyas-s-raman.github.io/CAPE/)|
| SayCanPay: [SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge](https://arxiv.org/abs/2308.12682)                   | AAAI                 | 2024 |[Web](https://rishihazra.github.io/SayCanPay/)|
| MASE: [Multi-agent strategy explanations for human-robot collaboration](https://arxiv.org/abs/2311.11955)                        | ICRA                 | 2024 | -    |
| FoMo: [FoMo Rewards: Can we cast foundation models as reward functions?](https://arxiv.org/abs/2312.03881)                   | NeurIPS Workshop     | 2023 | -    |
| Language to Rewards: [Language to rewards for robotic skill synthesis](https://arxiv.org/abs/2306.08647)         | CoRL                 | 2023 |[Web](https://language-to-reward.github.io/)|

## Others
| Title                              | Venue                | Date | Source  |
|------------------------------------|----------------------|------|------|
| MADiTS: [Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching](https://openreview.net/pdf?id=EpnZEzYDUT)                        | ICLR                 | 2025 | [code]() |
| RPA: [Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment](https://ieeexplore.ieee.org/abstract/document/9785410)                        | RAL                 | 2022 | - |
| Language-centric Agents: [Towards a unified agent with foundation models](https://arxiv.org/abs/2307.09668) | ICLR workshop | 2023 | - |
| AdA: [Human-timescale adaptation in an open-ended task space](https://arxiv.org/abs/2301.07608) | ICML | 2023 | - |
| EUREKA: [Eureka: Human-level reward design via coding large language models](https://arxiv.org/abs/2310.12931) | ICLR | 2024 | [Code](https://github.com/eureka-research/Eureka) |
| LESR: [LLM-Empowered State Representation for Reinforcement Learning](https://arxiv.org/abs/2407.13237) | ICML | 2024 | [Code](https://github.com/thu-rllab/LESR) |
| Text2Reward: [Text2Reward: Reward Shaping with Language Models for Reinforcement Learning](https://arxiv.org/abs/2309.11489) |ICLR | 2024 | [Code](https://github.com/xlang-ai/text2reward) |
| Automation method: [Position: Automatic Environment Shaping is the Next Frontier in RL](https://openreview.net/forum?id=dslUyy1rN4) | ICML | 2024 | [Code](https://github.com/auto-env-shaping/EnvCoderBench) |
| ChatAdp: [ChatAdp: ChatGPT-powered Adaptation System for Human-Robot Interaction](https://ieeexplore.ieee.org/abstract/document/10611520) | ICRA | 2024 | - |
| PromptGAT: [Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning](https://ojs.aaai.org/index.php/AAAI/article/view/27758) | AAAI| 2024 | [Code](https://github.com/DaRL-LibSignal/PromptGAT) |
| PPP: [Prompt, plan, perform: Llm-based humanoid control via quantized imitation learning](https://ieeexplore.ieee.org/abstract/document/10610948) | ICRA | 2024 | - |
| BMQ: [Learning agile bipedal motions on a quadrupedal robot](https://ieeexplore.ieee.org/abstract/document/10611442) | ICRA | 2024 | - |
| DSCRL: [Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications](https://ieeexplore.ieee.org/abstract/document/10610942) | ICRA | 2024 | - |
