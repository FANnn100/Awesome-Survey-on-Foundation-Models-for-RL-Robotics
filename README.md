# Awesome-Survey-on-Foundation-Models-for-RL-Robotics

## Paper Taxonomy
| Index | Paper | Year | Conference/Journal | Application | Type of foundation model | The role that VLM plays in robotics/RL |
| --- | --- | --- | --- | --- | --- | --- |
|1| RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback | 2024 | arxiv | classic control, rigid and articulated object manipulation | VLMs, leveraging vision language foundation models (VLMs) that are trained on diverse, general text and image corpora (e.g., GPT-4V (OpenAI, 2023), Gemini (Team et al., 2023)). | Query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. |
|2| Foundation Reinforcement Learning: Towards Embodied Generalist Agents With Foundation Prior Assistance | 2023 | arxiv | Foundation Actor-Critic on robotics manipulation tasks | Policy Foundation Prior：Follow the work UniPi, which infers actions based on a language conditioned video prediction model and a universal inverse dynamics model. For the video prediction model, use VLM （Seer, Seer: Language instructed video prediction with latent diffusion models）predicts a video conditioned on one image and a language instruction with latent diffusion models, pre-trained on Something Something V2 (Goyal et al., 2017) and Bridge Data (Ebert et al., 2021) datasets. <br> Value Foundation Prior: utilize VIP (Vip: Towards universal visual reward and representation via value-implicit pre-training) , which trains a universal value function via pre-training on internet-scale robotics datasets. | Build prior knowledge for Policy and Value of RL. |
|3| Language Conditioned Imitation Learning over Unstructured Data | 2020 | arxiv |Human language conditioned robotic manipulation, a single agent must execute a series of visual manipulation tasks in a row, each expressed in free-form natural language, e.g. “open the door all the way to the right...now grab the block...now push the red button...now open the drawer”. <br> Agents in this scenario are expected to be able to perform any combination of subtasks in any order. Incorporating free-form natural language conditioning into imitation learning, learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network.|Language conditioned visuomotor policy. Text conditioned policies with large pretrained neural language models. | The resulting language conditioned visuomotor policy can follow many free-form human text instructions over a long horizon in a simulated 3D tabletop setting, i.e. “open the door. . . now pick up the block. . . now press the red button” (see video). <br> Combine text conditioned policies with large pretrained neural language models to scale up the number of instructions an agent can follow. |
|4| Policy adaptation from foundation model feedback | 2023 | CVPR | By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. The policy still fails in most cases given an unseen task or environment. Generalize the learned policy to unseen tasks and environments. | VLM | When deploying the trained policy to a new task or a new environment, let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong,  can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning.|
|5| CLIPORT : What and Where Pathways for Robotic Manipulation | 2021 | CoRL | End-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. This method is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. ( experiment in the Ravens). CLIPort grounds semantic concepts in precise spatial reasoning, but it is limited to 2D observation and action spaces. | A language-conditioned imitation learning agent that combines the broad semantic understanding (what) of CLIP with the spatial precision (where) of Transporter.| A two-stream architecture with semantic and spatial pathways for vision-based manipulation. Combines the best of both worlds: end-to-end learning for fine-grained manipulation with the multi-goal and multi-task generalization capabilities of vision-language grounding systems. Two-stream architecture for using internet pre-trained vision-language models for conditioning precise manipulation policies with language goals.|
|6| Perceiver-actor: A multi-task transformer for robotic manipulation | 2022 | CoRL | Exploit the 3D structure of voxel patches for efficient 6-DoF(degrees of freedom) behavior-cloning(BC) with Transformers (analogous to how vision transformers exploit the 2D structure of image patches). Conduct large-scale experiments in the RLBench environment.| A language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. | PERACT encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. PERACT is essentially a classifier trained with supervised learning to detect actions akin to prior work like CLIPort, except our observations and actions are represented with 3D voxels instead of 2D image pixels. Use a Perceiver Transformer to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. |
|7|Language-driven representation learning for robotics|2023|arxiv|Visual representation learning for robotics.|Multi modal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others,  where Voltron differs, however, is in their novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics.|We then introduce Voltron, a framework for languagedriven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics.|
|8|CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning| 2022 | arxiv | Novel framework designed to enhance scalability in robot learning using foundation models such as Stable Diffusion. Scaling robot learning, with specific focus on multi-task and multi-scene manipulation in kitchen environments, both in simulation and in the real world. | Visual generative models such as Stable Diffusion pretrained zero-shot visual representation models trained on out-of-domain data. | In the data collection stage, limited in-domain expert demonstration data is collected. In the data augmentation stage, CACTI employs visual generative models such as Stable Diffusion [117] to boost visual diversity by augmenting the data with scene and layout variations. In the visual representation learning stage, CACTI leverages pretrained zero-shot visual representation models trained on out-of-domain data to improve training efficiency. Finally, in the imitation policy training stage, a general multi-task policy is learned using imitation learning on the augmented dataset with compressed visual representations as input. |
|9| MUTEX: Learning Unified Policies from Multimodal Task Specifications | 2023 | CoRL | Policy learning from multimodal task specifications.| Transformer-based model  can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them.| Trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure.|
|10| Human-Timescale Adaptation in an Open-Ended Task Space | 2023 | ICML | Fast and flexible adaptation. Training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. This work considers navigation, coordination, and division of labor tasks.|Transformers as an architectural choice.| Use Transformers as an architectural choice to scale incontext fast adaptation via model-based RL. An agent that has been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad range of downstream tasks. |
|11|R3M: A Universal Visual Representation for Robot Manipulation|2022|CoRL|Visual Representation Learning, Robotic Manipulation. Study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks.| Visual representations.| Visual representation for robot manipulation. Provides pretrained visual representation for robot manipulation using diverse human video datasets such as Ego4D and can be used as a frozen perception module for policy learning in robot manipulation tasks.|
|12|Towards universal visual reward and representation via value-implicit pre-training|2023|ICLR|A key unsolved problem to pre-training for robotic control is the challenge of reward specification. Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks.|Visual representation.| Self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. Casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self supervised goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos.|
|13|LIV: Language-Image Representations and Rewards for Robotic Control| 2023|ICML|Vision-language representation and reward learning from action-free videos with text annotations for Robotic Control. Use LIV to pre-train the first control-centric vision-language representation from large human video datasets such as EpicKitchen.|Vision-language representation. Builds on our prior work Value Implicit Pre-Training (VIP),  a selfsupervised approach for learning visual goal-conditioned value functions and representations from videos, generalizing it to learning multi-modal, vision-language values and representations from language-aligned videos, as described above.|Given only a language or image goal, the pre-trained LIV model can assign dense rewards to each frame in videos of unseen robots or humans attempting that task in unseen environments. Further, when some target domain-specific data is available, the same objective can be used to fine-tune and improve LIV and even other pre-trained representations for robotic control and reward specification in that domain.|
|14|Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation|2021|CoRL|Natural Language, Offline RL, Visuomotor Manipulation. Learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. Grounding language(provides a convenient and flexible alternative for task specification) in the robot’s observation space.|Combine language-conditioned reward with visual model predictive control. Pretrained language models.|Learns a language-conditioned reward from offline data and uses it during model predictive control to complete language specified tasks.|
|15| Do As I Can, Not As I Say: Grounding Language in Robotic Affordances | 2022 | CoRL | Large language models can encode a wealth of semantic knowledge about the world, but a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. | LLMs | Provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. <be> They show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. <br> They use the language model to provide task-grounding (Say), enabling the determination of useful sub-goals based on high-level instructions, and a learned affordance function to achieve world-grounding (Can), enabling the identification of feasible actions to execute the plan.|
|16| Inner Monologue: Embodied Reasoning through Planning with Language Models | 2022 | arxiv | Studies the role of grounded environment feedback provided to the LLM, thus closing the loop with the environment. The feedback is used for robot planning with large language models by leveraging a collection of perception models (e.g., scene descriptors and success detectors) in tandem with pretrained language-conditioned robot skills. | LLMs applied to domains beyond natural language processing (such as planning and interaction for robots). | By leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. |
|17|Text2Motion: From Natural Language Instructions to Feasible Plans| 2023|Autonomous Robots|Language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. Long-horizon planning, Robot manipulation, Large language models.|LLM Language-based planning.|A language-based planning framework that interfaces an LLM with a library of learned skills and a geometric feasibility planner [8] to solve complex sequential manipulation tasks.|
|18|Voxposer: Composable 3d value maps for robotic manipulation with language models|2023|CoRL|Synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. Manipulation, Large Language Models, Model-based Planning| VLMs | First observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. <br> More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. <br> Given the RGB-D observation of the environment and language instruction, VoxPoser utilizes large language models to generate code, which interacts with vision-language models to extract a sequence of 3D affordance maps and constraint maps. These maps are composed together to create 3D value maps. The value maps are then utilized as objective functions to guide motion planners to synthesize trajectories for everyday manipulation tasks without requiring any prior training or instruction.|
|19|Zero-shot reward specification via grounded natural language|2022|ICML|Learning text-conditioned policies. Robot manipulation tasks. | Large-scale visuolanguage models like CLIP. | Use developments in building large-scale visuo-lingual models like CLIP to devise a framework that generates the task reward signal just from goal text description and raw pixel observations which is then used to learn the task policy.|
|20|Grounding Language with Visual Affordances over Unstructured Data|2023|ICRA|Considers robot manipulation task.|Hierarchical language-conditioned agent that integrates the task-agnostic control of HULC (state-of-the-art language-conditioned imitation learning agent that learns 7DoF goal-reaching policies end-to-end) [10] with the object-centric semantic understanding of VAPO (extracts a self-supervised visual affordance model of unstructured data and not only accelerates learning, but was also shown to boost generalization of downstream control policies).| A self-supervised visuo-lingual affordance model is used to learn general-purposed language conditioned robot skills from unstructured offline data in the real world. <br> Given visual observations and language instructions as input, the affordance model outputs a pixel-wise heat map that represents affordance regions and the corresponding depth map.|
|21|Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception|2023|arxiv|The paper focuses on enhancing robot perception in human-robot interaction scenarios by using vision-language models (VLMs) to integrate inertial measurement unit (IMU) data with visual inputs. Robot perception requires accurate scene understanding, which is challenging when relying on a single modality like vision. The aim is to align the embedding spaces of different modalities (vision and IMU data) to improve scene understanding and reasoning without extensive retraining.|The approach involves extending frozen, pre-trained vision-language models to incorporate IMU data. This is achieved through a combination of supervised and contrastive training, aligning the embedding spaces of different modalities to the vision embedding space. This method leverages the abstract skills learned by large language models (LLMs) during pre-training and applies them to multiple modalities.|The paper addresses the challenge of robot perception by integrating inertial measurement unit (IMU) data with visual inputs using vision-language models (VLMs). By aligning the embedding spaces of different modalities through supervised and contrastive training, the VLM enhances scene understanding and reasoning without retraining. This approach improves robot perception, making VLMs more versatile and effective in various robotic and reinforcement learning tasks.|
|22|Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation|2022|CVPR|The research focuses on Vision-and-Language Navigation (VLN) within the context of both discrete and continuous environments. The primary application is to develop agents that can navigate real-world-like continuous spaces based on human instructions.  The main problem addressed is the domain gap between discrete and continuous VLN environments. Training agents in discrete spaces (which rely on pre-defined connectivity graphs) is significantly easier than in continuous spaces (which are more realistic but lack such graphs). This gap hinders the transferability and generalization of VLN agents trained in one environment to another.| The research utilizes a candidate waypoints predictor trained with refined connectivity graphs from Matterport3D to Habitat-Matterport3D environments. This predictor generates a set of accessible waypoints during navigation in continuous spaces. The models include: Cross-Modal Matching Agent (CMA) VLN\BERT. The training approach employs a simple imitation learning objective to enhance agent performance.|This research addresses the domain gap between discrete and continuous Vision-and-Language Navigation (VLN) environments by using a candidate waypoints predictor trained on refined connectivity graphs. The predictor generates navigable waypoints in continuous environments, enabling agents to perform high-level actions and navigate effectively based on visual observations. This approach enhances the transferability and generalization of VLN agents, allowing them to operate more robustly in realistic, continuous spaces, thereby improving their applicability in real-world robotics and reinforcement learning scenarios.|
|23|NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation|2023|arxiv|The application addressed in this paper is Vision-and-Language Navigation (VLN), which involves enabling AI agents to navigate unseen environments by following linguistic instructions. The primary aim is to improve generalization in navigation tasks, ensuring that agents can effectively operate in diverse and previously unencountered environments, ranging from out-of-distribution scenes to real-world applications without relying on maps, odometers, or depth inputs.|The foundation model used in this paper is a video-based large Vision-Language Model (VLM) called NaVid. This model leverages pre-trained vision encoders and large language models (LLMs) to interpret visual data from a monocular RGB camera and human-issued instructions. NaVid integrates these inputs to make real-time navigation decisions, bypassing the need for traditional navigation aids like maps and odometer data.|In this work, the VLM plays a critical role in enhancing the generalization and adaptability of robotic navigation systems. By encoding historical observations and spatio-temporal contexts from video data, NaVid provides a robust framework for action planning and instruction following. The VLM allows the robot to understand and execute navigation tasks based on visual observations and natural language instructions, addressing the challenges of odometer noise and domain discrepancies, thus facilitating smoother Sim2Real transitions and improving overall navigation performance in real-world settings.|
|24|Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs|2023|CoRL|The paper introduces the Open-Vocabulary 3D Scene Graph (OVSG), a framework aimed at enhancing robotic systems' ability to ground various entities, such as objects, agents, and regions, in real-world scenarios using free-form text queries. The key application is to improve context-aware localization, enabling robots to understand and execute complex commands like "pick up a cup on the kitchen table" or "navigate to a sofa where someone is sitting". This addresses the challenge of grounding semantic entities in real-world navigation and manipulation tasks, thereby enhancing the practical functionality of robots in everyday environments.|The foundation model used in this paper combines open-vocabulary detection with 3D scene graphs. By leveraging the strengths of 3D scene graphs, which encode spatial and relational context among objects, and incorporating an open-vocabulary approach, OVSG can handle a broader range of queries, including those involving unseen semantic categories and relationships. This hybrid approach enhances the system's ability to accurately ground entities in varied and dynamic real-world settings.|In this work, Vision-Language Models (VLMs) play a critical role in bridging the gap between natural language queries and robotic perception. By integrating VLMs with 3D scene graphs, the system can interpret complex, context-aware commands and translate them into actionable tasks for robots. This integration allows robots to utilize semantic and spatial information more effectively, enabling precise localization and interaction with objects based on detailed human instructions. Consequently, VLMs significantly enhance the robots' capability to understand and respond to open-vocabulary queries in reinforcement learning and real-world applications.|
|25|Interactive Language: Talking to Robots in Real Time|2022|arxiv|The primary application of this paper is to develop a framework for real-time, natural language-instructable robots capable of executing a diverse array of tasks in the real world. These tasks range from basic commands to complex, long-horizon manipulation goals. The framework aims to produce robots that can proficiently understand and act upon natural language instructions, enabling them to perform tasks that require precise and dynamic control in real-time.|The foundation model utilized in this paper is based on behavioral cloning, trained on a vast dataset of language-annotated trajectories. This model leverages imitation learning techniques to interpret and execute natural language commands. The approach integrates components such as event-selectable hindsight relabeling and combines existing methods to create a scalable and efficient recipe for learning a wide range of language-conditionable skills.|In this work, the Vision-Language Model (VLM) plays a crucial role in bridging the gap between natural language instructions and robotic actions. VLMs facilitate the robot's understanding of complex, context-rich commands by mapping language inputs to corresponding visual and motor actions. This enables the robot to perform continuous-control visuo-motor manipulation in real-time, effectively responding to new instructions during ongoing task execution. By incorporating VLMs, the framework demonstrates improved interaction capabilities and task performance, allowing the robot to handle a broader spectrum of language-guided tasks in dynamic environments.|
|26|ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts|2022|CVPR|This paper addresses the challenge of Vision-Language Navigation (VLN), which involves guiding an embodied agent to navigate complex visual environments based on language instructions. The application problem tackled is enabling the agent to perform action-level modality alignment, ensuring that it can accurately interpret and execute instructions within the given visual context to achieve successful navigation.|The paper utilizes the Contrastive Language-Image Pretraining (CLIP) model as the foundation model. CLIP is known for its powerful cross-modality alignment capabilities, effectively bridging the gap between visual and textual information. It helps in collecting high-quality action prompts that are crucial for training the VLN agent to understand and act upon complex instructions.|In this work, foundation models like CLIP play a pivotal role in enhancing the VLN agent's ability to align actions with visual and language inputs. By providing modality-aligned action prompts, the model explicitly learns the alignment between different modalities, which improves the agent's decision-making during navigation. This approach leverages the strengths of CLIP in cross-modal alignment to address the inherent challenges in robotics and reinforcement learning, leading to more robust and interpretable navigation strategies.|
|27|The Unsurprising Effectiveness of Pre-Trained Vision Models for Control|2022|ICML|The application addressed in this paper is the use of pre-trained visual representations for control tasks in robotics and reinforcement learning (RL). The study focuses on whether these pre-trained visual models, which are typically used for computer vision tasks, can be effectively applied to diverse control environments, such as habitat navigation, dexterous manipulation, and robotic kitchen tasks, without being trained on data from these specific environments.|The paper utilizes pre-trained visual representation (PVR) models that have been trained on large-scale computer vision datasets like ImageNet. These models are used as frozen perception modules for various control tasks. The study investigates different PVR models, including those trained with supervised learning and self-supervised learning (SSL), and examines their effectiveness in control domains.|In this work, the pre-trained visual models (akin to visual language models, VLMs) serve as foundational perception components that can generalize across different control tasks. The paper demonstrates that these models, despite being trained on out-of-domain datasets, can outperform or match the performance of ground-truth state features in training control policies. This finding suggests that pre-trained visual models can significantly reduce the need for task-specific data collection and training in robotics and RL, thus enhancing the efficiency and scalability of deploying control policies in diverse environments.|
|28|CoWs on PASTURE: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation|2022|arxiv|The application addressed by this paper is language-driven zero-shot object navigation (L-ZSON). This involves enabling robots to find arbitrary objects based on natural language descriptions without needing extensive navigation training on domain-specific data. The research aims to enhance robots' practical utility by enabling them to locate objects described in human language, addressing a more challenging but highly applicable version of object navigation that includes finding uncommon, spatially described, and hidden objects.|The foundation model used in this paper is CLIP (Contrastive Language-Image Pretraining), adapted in a framework referred to as CLIP on Wheels (CoW). This model leverages the success of open-vocabulary models for image classification, object detection, and semantic segmentation to perform zero-shot object navigation without additional fine-tuning. The CoW framework enables robots to interpret natural language descriptions and localize objects accordingly.|In this work, the foundation models like CLIP play a crucial role in enabling robots to understand and act upon natural language instructions. CLIP's ability to match images with textual descriptions allows the CoW framework to perform object localization and navigation without requiring extensive pre-training on specific environments or objects. This approach demonstrates the potential of leveraging large language models and vision-language models to bridge the gap between human language and robotic perception, thereby improving the robots' efficiency in zero-shot tasks and expanding their applicability in real-world scenarios.|
|29|A Recurrent Vision-and-Language BERT for Navigation|2021|CVPR|The application problem addressed in this paper is vision-and-language navigation (VLN), where an agent must navigate through an environment following natural language instructions. The main challenge in VLN lies in adapting the BERT architecture to the partially observable Markov decision process inherent in VLN tasks. This requires the model to handle history-dependent attention and decision-making to effectively interpret and act on the instructions.|The foundation model used in this paper is a recurrent vision-and-language BERT (V&L BERT) model, which is enhanced to be time-aware. The authors propose a recurrent function within the BERT architecture to maintain cross-modal state information, enabling the model to manage historical context and make informed navigation decisions. This approach leverages pre-trained V&L BERT models and fine-tunes them for the VLN task.|In this work, foundation models like V&L BERT play a critical role by providing a robust framework for integrating visual and language information, essential for navigating based on instructions. The recurrent V&L BERT model addresses the VLN challenges by utilizing self-attention mechanisms to process partial observations and maintain relevant sub-instruction localization. This allows the agent to perform navigation tasks more effectively, demonstrating the potential of foundation models to enhance multi-modal decision-making processes in robotics and reinforcement learning.|
|30|ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes|2023|ICCV|The paper addresses the challenge of understanding and manipulating continuous object states in robotics tasks, which is crucial for effective task learning and planning in the real world. Existing benchmarks often assume discrete object states, limiting the ability of robots to follow human instructions based on detailed and nuanced object states. This work introduces the ARNOLD benchmark, which focuses on language-grounded task learning with continuous object states in realistic 3D scenes, aiming to bridge the gap between simulated environments and real-world applications.|The foundation models used in this paper are state-of-the-art language-conditioned policy learning models. These models are designed to process and interpret human language instructions, enabling robots to understand and execute tasks in a continuous state space. The paper utilizes these models to assess task performance and generalization abilities across various scenes and object states.|In this work, foundation models like language-conditioned policy learning models play a crucial role in grounding language instructions to precise robot actions and object states. These models enable robots to interpret and act upon detailed language descriptions of tasks, promoting the learning of precise manipulations in continuous state environments. The models are evaluated on their ability to generalize learned skills to unseen scenarios, highlighting both the strengths and the areas needing improvement for effective language-grounded task learning in robotics and reinforcement learning (RL).|
|31|BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs|2024|arxiv|This paper addresses the challenge of generating effective behavior trees (BTs) for robots, particularly focusing on creating adaptable, flexible, and efficient task planning systems. The study aims to demonstrate that compact large language models (LLMs) with up to 7 billion parameters can be fine-tuned to produce satisfactory results in generating behavior trees, thus facilitating improved robot behaviors in dynamic environments. The practical applicability of this approach is validated through static syntactical analysis, a validation system, a simulated environment, and real robot deployments.|The research utilizes lightweight large language models (LLMs) with a maximum of 7 billion parameters. Specifically, it employs models such as GPT-3.5, llama2, llama-chat, and code-llama. These models are fine-tuned on a specially created dataset based on existing behavior trees to generate the desired robot behaviors. The study involves a comprehensive comparison of these LLMs across nine distinct tasks to evaluate their performance and effectiveness in generating behavior trees.|In this work, the foundation models, such as LLMs, play a crucial role in addressing the complexity of robot task planning. By leveraging the vast knowledge encoded within these models, the research demonstrates that LLMs can generate effective and efficient behavior trees, even with a limited number of parameters. The LLMs are fine-tuned to understand and generate complex task plans, allowing robots to perform high-level decision-making and adapt to dynamic environments. This approach reduces the resource intensity typically required for larger models while maintaining comparable effectiveness, thereby enhancing the practical deployment of such models in robotics and reinforcement learning applications.|
|32|To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions|2024|arxiv|The application of this paper is to enable robots to provide unobtrusive physical support within a group of humans. The concept, termed Attentive Support, focuses on ensuring that robots can assist in social and group interactions without disturbing or distracting the human participants. The goal is to facilitate seamless human-robot interactions in multiparty settings by interpreting human needs and constraints, offering help only when necessary or explicitly requested.|The paper employs a framework based on Large Language Models (LLMs) to implement the concept of Attentive Support. The adaptable LLM-based framework is used as a modular set of capabilities, which includes components like a robot-character to interpret and respond to human interactions effectively. This approach leverages the advanced natural language understanding and reasoning capabilities of LLMs to achieve its objectives.|In this work, the foundation models, such as LLMs, play a crucial role in interpreting natural language requests, understanding the current physical and social context, and making informed decisions about when and how to provide support. These models enable the robot to ground its actions in the physical and social states of the group members, ensuring that the robot’s behavior is neither obtrusive nor distracting. By leveraging the capabilities of LLMs, the robot can maintain awareness of human goals, constraints, and situational factors, thus enhancing its ability to support humans in various scenarios where physical assistance is required.|
|33|Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues|2024|arxiv|The primary application of this paper is in enhancing human-robot interaction (HRI) by improving the robot's ability to interpret verbal instructions, particularly in scenarios requiring social navigation. The paper addresses the shortcomings of Large Language Models (LLMs) in handling the nuances of verbal instructions, such as tone and affect, which are critical for effective human-robot communication. By integrating audio transcription with affective vocal features, the proposed approach, "Beyond Text," aims to make LLMs more reliable and effective in interpreting and responding to human instructions, thus facilitating more natural and trustworthy robot behavior in social contexts.|This paper utilizes advanced Large Language Models (LLMs) like GPT-4 and the Gemini series as the foundation models. The key innovation is the integration of audio transcription and analysis of affective vocal features—such as pitch, loudness, and duration—alongside textual input. This approach allows the LLMs to interpret the nuances of human speech more accurately, leveraging the combined strengths of text and audio inputs to enhance decision-making processes in robots.|In this work, foundation models like LLMs play a crucial role in advancing the capabilities of robots in understanding and responding to human verbal instructions. By extending the traditional text-based input to include audio cues, these models can capture and interpret the subtleties of human speech, such as emotional tone and emphasis. This multi-modal approach enables robots to better understand human intentions and uncertainties, leading to more accurate and contextually appropriate responses. As a result, the integration of LLMs with audio analysis significantly enhances the robots' performance in social navigation tasks, making them more robust against adversarial attacks and improving their overall reliability and trustworthiness in human-robot interactions.|
|34|SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge|2024|AAAI|The primary application addressed by this paper is the enhancement of planning capabilities using Large Language Models (LLMs) combined with heuristic planning methods. Traditional planning methods, which rely on domain-specific knowledge such as Planning Domain Definition Language (PDDL), face challenges in generating feasible and optimal plans in complex environments. This paper proposes a novel approach called SayCanPay that leverages the extensive world knowledge embedded in LLMs to generate candidate actions, assesses their feasibility, and evaluates their cost-effectiveness, thereby improving planning efficiency and effectiveness in various scenarios.|The foundation model utilized in this paper is a Large Language Model (LLM). The approach, SayCanPay, employs the LLM to generate potential candidate actions based on a given goal and initial observation. These actions are then scored for feasibility using a domain-specific affordance model, and for cost-effectiveness using a heuristic estimator. By integrating these evaluations, SayCanPay aims to identify the most feasible and cost-effective sequence of actions for planning tasks.|In this work, the foundation model, specifically the LLM, plays a crucial role in enhancing the planning process within the context of robotics and reinforcement learning (RL). The LLM is utilized to generate a broad set of potential actions based on its extensive training on world knowledge. This generation capability allows the model to propose actions that might not be explicitly defined in traditional planning frameworks. The generated actions are then evaluated for feasibility and payoff, integrating heuristic search techniques to identify optimal plans. This combined approach leverages the generative power and world knowledge of LLMs to address planning challenges in dynamic and complex environments, ultimately leading to more effective and adaptable robotic and RL systems.|
|35|Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning|2023|arxiv|The application of this paper is focused on improving robotic task planning by integrating physically-grounded perception with language understanding. It aims to enhance robots' capability to perform complex, long-horizon tasks in dynamic and varied environments by grounding their planning process in visual and spatial information. This integration allows robots to effectively interpret and execute high-level language instructions in real-world scenarios.|The paper utilizes Vision-Language Models (VLMs) as the foundation model. These models combine visual perception with language processing, allowing the system to generate a sequence of actionable steps based on visual observations and language instructions. The specific approach introduced is called Robotic Vision-Language Planning (ViLA), which directly incorporates perceptual data into the reasoning and planning process.|In this work, foundation models like LLMs and VLMs play a crucial role by providing the semantic and perceptual grounding necessary for effective task planning in robotics. By leveraging VLMs, the system can understand and integrate common-sense knowledge, spatial layouts, and object attributes, enabling robots to perform tasks more accurately and safely. This multimodal approach addresses the limitations of traditional LLM-based planners by ensuring that robots can reason about and adapt to their physical environment in real-time, leading to more robust and flexible robotic planning and execution.|
|36|Corrective Planning of Robot Actions with Large Language Models|2023|ICRA|The primary application addressed in this paper is the development of fully autonomous robotic systems capable of performing tasks traditionally handled by humans in open-world environments. The complexity of these environments necessitates advanced task and motion planning capabilities. This study specifically aims to enhance robot manipulation tasks by integrating reasoning, planning, and motion generation using a hierarchical architecture that includes feedback loops to handle errors in generated plans.|The foundation model employed in this study is Large Language Models (LLMs). These models are utilized to leverage their rich commonsense knowledge and implicit reasoning capabilities to generate potential action plans. The proposed architecture, CoPAL (closed-loop task planning mechanism with a multi-level feedback loop), relies on LLMs to create and refine task plans, incorporating feedback to improve plan execution and handling unexpected environmental changes.|In this work, Large Language Models (LLMs) play a crucial role in bridging the gap between high-level reasoning and low-level motion planning in robotics. LLMs are used to generate initial action plans and are subsequently involved in a feedback loop where their outputs are continuously checked and refined based on execution success or failure. This approach allows for dynamic adjustment of plans in real-time, enhancing the robot's ability to adapt to unforeseen changes and errors, thus improving the overall robustness and efficiency of task execution in complex, real-world scenarios.|
|37|LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement|2023|arxiv|The application of this paper focuses on the task of semantic object rearrangement in robotics, where robots are required to organize objects according to linguistic descriptions. This involves interpreting natural language instructions to generate actionable plans for rearranging objects in complex geometric patterns. The goal is to enhance the robots' capability to perform everyday tasks, such as setting up a kitchen, by accurately following verbal instructions.|This paper utilizes a framework called Language-Guided Monte Carlo Tree Search (LGMCTS), which integrates large language models (LLMs) with Monte Carlo Tree Search (MCTS) for task and motion planning (TAMP). The LLMs are used to generate intermediate representations from language inputs, which are then used by MCTS to formulate feasible action plans that achieve the specified geometric rearrangements.|In this work, LLMs play a crucial role in bridging the gap between natural language descriptions and executable robotic actions. The LLMs generate intermediate representations that capture the semantic and geometric constraints of the task, which are then used by MCTS to plan the sequence of actions required to achieve the desired object rearrangement. This approach leverages the strengths of LLMs in understanding and interpreting complex linguistic inputs and combines them with the robustness of traditional TAMP algorithms to create feasible and executable plans for robotic manipulation tasks.|
|38|Prompt a Robot to Walk with Large Language Models|2023|arxiv|The application of this paper is focused on using large language models (LLMs) to control robot walking in dynamic environments. The authors aim to leverage the capabilities of LLMs to generate low-level control commands for robots, enabling them to perform dynamic tasks such as walking. This approach addresses the challenge of grounding LLMs in the physical world to create dynamic robot motions, without the need for task-specific fine-tuning of the models.|The foundation model used in this paper is GPT-4, a large language model pre-trained on vast internet-scale text data. The paper introduces a novel paradigm where few-shot prompts collected from the physical environment are used to interact with the LLM, enabling it to generate appropriate control commands for the robot. This method bypasses the need for extensive task-specific fine-tuning by utilizing the LLM's ability to generalize from the few-shot prompts provided.|In this work, the foundation model like GPT-4 plays a crucial role in generating control commands for robot motion. By using few-shot prompts derived from physical environment interactions, the LLM can produce dynamic and contextually appropriate actions for the robot. This framework positions the LLM as a feedback policy generator that outputs target joint positions for the robot, thereby enabling it to interact with and adapt to its physical surroundings effectively. The approach demonstrates the potential of LLMs to function as low-level feedback controllers for dynamic motion control in robotics, even in complex and high-dimensional environments.|
|39|Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment|2023|arxiv|The application of this paper focuses on improving the execution of tasks in robotic systems by addressing the misalignments between high-level plans generated by large language models (LLMs) and the actual low-level execution in the physical world. The proposed DoReMi framework aims to enable robots to detect and recover from these plan-execution misalignments in real-time, ensuring more reliable and efficient task completion.|This paper leverages large language models (LLMs) and vision language models (VLMs) as the foundation models. The LLMs are utilized for high-level planning and generating constraints for low-level execution, while VLMs are employed to continuously monitor and detect any constraint violations during the execution of low-level skills.|In this work, the foundation models like LLMs and VLMs play a crucial role in enhancing the reliability and efficiency of robotic task execution. LLMs aid in high-level planning and generating actionable constraints for low-level skills, facilitating rapid detection and recovery from misalignments. VLMs, on the other hand, act as constraint detectors, monitoring the execution process and providing precise feedback to ensure alignment between the plan and execution. This collaborative approach allows for immediate re-planning and correction, improving the overall success rate and reducing task completion time.|
|40|Building Cooperative Embodied Agents Modularly with Large Language Models|2024|ICLR|This paper addresses the challenge of multi-agent cooperation in environments requiring decentralized control, raw sensory observations, costly communication, and multi-objective tasks. The focus is on creating embodied agents that can plan, communicate, and collaborate to accomplish long-horizon tasks efficiently. The application is particularly relevant in scenarios like household tasks, where agents need to interact dynamically and make decisions based on partial observations and effective communication to achieve common goals.|The foundation model used in this paper is a Large Language Model (LLM), specifically leveraging the GPT-4 model. This model is integrated into a novel modular framework named Cooperative Embodied Language Agent (CoELA), which utilizes its rich world knowledge, reasoning abilities, and natural language understanding for effective multi-agent cooperation. Additionally, the paper explores fine-tuning techniques using LoRA (Low-Rank Adaptation) for optimizing the performance of the CoLLAMA model, derived from the LLAMA-2 foundation model, in collaborative tasks.|In this work, foundation models like LLMs play a crucial role in enhancing the cognitive capabilities of embodied agents. LLMs contribute by providing advanced language comprehension, reasoning, and dialogue generation, which are essential for planning and communication in multi-agent systems. The cognitive-inspired modular framework incorporates LLMs to perceive complex observations, maintain long-term memory, generate high-level plans, and execute actions efficiently. By harnessing these capabilities, the agents can perform better in cooperative tasks, achieve higher efficiency, and build trust in human-agent interactions, thus addressing key challenges in robotics and reinforcement learning.|
|41|LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action|2023|CoRL|The application focuses on robotic navigation using large pre-trained models of language, vision, and action. The system, LM-Nav, aims to enable mobile robots to follow high-level natural language instructions to navigate complex outdoor environments. The study demonstrates the ability of LM-Nav to execute long-horizon navigation tasks based on free-form textual instructions without requiring any fine-tuning or language-annotated navigational data.|GPT-3, a large language model (LLM) for parsing textual instructions into landmarks. CLIP, a vision-language model (VLM) for grounding textual landmarks in visual observations. ViNG, a visual navigation model (VNM) for constructing a topological map and executing navigation plans.|In this context, Vision-Language Models (VLMs) play a critical role in grounding textual instructions within the robot's visual observations. Specifically, CLIP is used to associate images with text, computing the likelihood that a given image corresponds to a textual landmark. This capability allows the robot to interpret and follow natural language instructions by matching described landmarks to its visual surroundings. The VLM facilitates the translation of language-based navigation commands into actionable visual targets, enabling the robot to navigate effectively in real-world environments based on high-level human instructions. The integration of VLMs with the LLM and VNM allows the LM-Nav system to create a comprehensive navigation plan that adheres to user instructions while optimizing the traversal path.|
|42|Open-vocabulary Queryable Scene Representations for Real World Planning|2023|ICRA|The application focuses on enabling robots to perform varied, real-world tasks by understanding and acting on diverse human commands within the context of their environment. The framework, NLMap, integrates natural language and visual-language models to create open-vocabulary, queryable scene representations that allow robots to understand and plan tasks based on the current state of their environment. This system aims to improve task planning and execution by enabling robots to query available objects and their locations in the scene before generating a context-conditioned plan.|Vision-Language Models (VLMs), specifically ViLD and CLIP, for generating and querying scene representations. Large Language Models (LLMs) for object proposal and planning.|In this context, Vision-Language Models (VLMs) play a crucial role in generating a queryable scene representation. VLMs like CLIP and ViLD are used to create embeddings for objects detected in the environment, allowing the scene representation to be queried with natural language. The VLMs provide open-vocabulary understanding and object detection capabilities, enabling the robot to identify and locate objects based on human commands. This information is then used by an LLM planner to generate and execute plans that are grounded in the actual context of the environment, significantly enhancing the robot's ability to perform long-horizon planning and real-world tasks.|
|43|CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory|2023|ICRA|The application focuses on building a queryable 3D scene representation for robotic tasks such as segmentation, instance identification, semantic search over space, and view localization. The proposed CLIP-Fields framework allows robots to create and use a spatial-semantic memory for various complex tasks in human environments. The system is designed to provide robots with a detailed understanding of their surroundings without relying on explicit human supervision.|CLIP (Contrastive Language-Image Pretraining) for capturing semantic information from images. Detic, an open-label object detection model for detecting objects and generating CLIP embeddings. Sentence-BERT for improving language query performance.|In this context, Vision-Language Models (VLMs) play a central role in creating a semantic understanding of the robot's environment. CLIP-Fields uses CLIP to generate embeddings that associate visual and language information, creating a semantic map that can be queried using natural language. The VLMs enable the robot to understand and navigate its environment based on semantic information derived from images, facilitating tasks such as semantic navigation, object search, and instance identification. The integration of VLMs allows the system to function with weak supervision from web-trained models, thus minimizing the need for extensive human annotations.|
|44|Visual Language Maps for Robot Navigation|2023|ICRA|The application focuses on improving robotic navigation by fusing pretrained visual-language model features with a 3D reconstruction of the physical world to create Visual Language Maps (VLMaps). These maps enable natural language indexing of the environment and enhance the robot's ability to follow complex language instructions for spatial navigation tasks.|Pretrained visual-language models such as LSeg, which is based on CLIP for generating semantic segmentation.Large Language Models (LLMs) for interpreting natural language commands and generating navigation goals.|In this context, Vision-Language Models (VLMs) are crucial for grounding language commands in visual observations and generating spatially precise navigation goals. The VLMs help create a semantic map that can be queried using natural language, allowing robots to navigate based on instructions like "move between the sofa and the TV" or "go to the right of the chair." These maps enable robots to understand and execute complex, zero-shot spatial navigation tasks without additional data collection or model fine-tuning. The integration of VLMs with LLMs further enhances the robot's ability to parse and act upon detailed language instructions, improving the efficiency and effectiveness of navigation in diverse environments.|
|45|ConceptFusion: Open-set Multimodal 3D Mapping|2023|ICRA|The application focuses on creating open-set multimodal 3D maps from RGB images, depth estimates, and features from foundation models like CLIP, DINO, and AudioCLIP. These maps enable robots to query their environment using diverse modalities such as text, images, audio samples, and clicks on the 3D map. The aim is to enhance robotic navigation, planning, and interaction with objects in a scene by integrating semantic concepts with 3D spatial data in a zero-shot manner.|CLIP for aligning images and text. DINO for visual features. AudioCLIP for audio features. Additional segmentation models like SAM and Mask2Former for instance segmentation.|In this context, Vision-Language Models (VLMs) play a crucial role in providing semantic understanding and enabling multimodal queries within the 3D maps. The VLMs help map visual and linguistic information onto 3D spaces, allowing robots to understand and interact with their environment through natural language, images, and sounds. This facilitates complex spatial reasoning tasks and enables robots to perform tasks based on high-level semantic queries, enhancing their ability to navigate and manipulate objects in real-world environments without the need for additional training or fine-tuning.|
|46|Fast Traversability Estimation for Wild Visual Navigation|2023|ICRA|The application focuses on real-time traversability estimation for robotic navigation in challenging natural environments such as forests and grasslands. The proposed system, Wild Visual Navigation (WVN), aims to enable robots to learn and adapt to traversable terrains quickly through online self-supervised learning based on visual inputs, allowing autonomous navigation in complex outdoor terrains.|Self-supervised visual transformer models, specifically DINO-ViT (Vision Transformer trained using the DINO method), for extracting high-dimensional visual features.|In this context, Vision-Language Models (VLMs) are not directly used. Instead, self-supervised visual transformer models are utilized to extract meaningful semantic and instance features from visual inputs without requiring manual labels. These features are processed to estimate the traversability of terrains based on proprioception and control performance. The system leverages high-dimensional features to generate supervision signals in real-time, enabling the robot to distinguish between traversable and untraversable areas after a short human demonstration. This allows the robot to adapt quickly to new environments and perform tasks such as negotiating obstacles in high grass and following footpaths autonomously.|
|47|HomeRobot: Open-Vocabulary Mobile Manipulation|2023|CoRL|The application focuses on enabling robots to perform open-vocabulary mobile manipulation (OVMM) tasks in household environments. The framework, HomeRobot, aims to benchmark and improve the performance of mobile manipulators in real-world settings, where the robot needs to navigate, perceive, and manipulate a wide variety of objects based on natural language instructions.|DETIC (DEtection Transformed by Image-level Supervision) for open-vocabulary object detection and segmentation.|In this context, Vision-Language Models (VLMs) like DETIC are crucial for enabling the robot to understand and interact with its environment based on natural language instructions. DETIC provides the robot with the ability to detect and segment a wide variety of objects, including those not seen during training, by leveraging large-scale image-level supervision. This capability allows the robot to follow instructions such as "move the toy animal from the chair to the table" by identifying and locating the specified objects and receptacles. The integration of VLMs enhances the robot's perception, enabling it to perform complex manipulation tasks in dynamic and unstructured environments. The HomeRobot framework demonstrates the effectiveness of combining VLMs with traditional robotic control and navigation systems to achieve robust and scalable mobile manipulation capabilities.|
|48|Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation|2023|CoRL|The application focuses on enhancing robotic manipulation tasks by leveraging 3D perceptual representations. The Act3D framework aims to improve the spatial precision of end-effector pose prediction in multi-task robotic manipulation tasks. The system is designed to perform complex manipulation tasks by representing the robot’s workspace using a 3D feature field with adaptive resolutions.|CLIP for pre-training 2D image features, which are then lifted into 3D using sensed depth information.|In this context, Vision-Language Models (VLMs) like CLIP are used to extract and featurize 2D image views, which are then integrated into a 3D feature field using sensed depth data. This integration allows the robot to construct a detailed 3D representation of its workspace, enabling high spatial precision in predicting 3D end-effector poses. The model leverages relative-position attention mechanisms to process these 3D features efficiently, facilitating fine-grained manipulation tasks and generalization across different environments and camera viewpoints. The use of VLMs like CLIP enhances the robot's ability to interpret visual data and perform complex tasks with high accuracy, significantly improving performance on benchmarks like RLBench.|
|49|Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation|2023|CoRL|The application focuses on enhancing robotic manipulation tasks using a few-shot learning approach. The goal is to enable robots to pick up novel objects based on a few demonstrations or natural language descriptions, leveraging distilled feature fields that integrate 3D geometry with rich semantic information from 2D vision-language models.|CLIP for extracting semantic features from images. DINO Vision Transformer (ViT) for providing dense visual descriptors.|In this context, Vision-Language Models (VLMs) like CLIP play a crucial role in providing the semantic understanding necessary for language-guided manipulation. The system uses CLIP features to interpret natural language commands and associate them with visual features in the robot's environment. By distilling these 2D features into a 3D feature field, the robot can leverage both the spatial and semantic information to perform manipulation tasks such as grasping and placing objects. This approach enables the robot to generalize to new objects and tasks that were not explicitly seen during training, demonstrating robust open-ended generalization capabilities.|
|50|GNFactor Multi-Task Real Robot Learning with Generalizable Neural Feature Fields|2023|CoRL|The application focuses on enabling robots to execute diverse manipulation tasks from visual observations in unstructured real-world environments. The system, GNFactor, aims to improve the generalization of robotic manipulation by using Generalizable Neural Feature Fields (GNF) and behavior cloning. GNFactor is evaluated on multiple real robot tasks and RLBench tasks to demonstrate its effectiveness and generalization capabilities.|Vision-language foundation models, specifically Stable Diffusion, for distilling rich semantic information into 3D voxel representations.|In this context, Vision-Language Models (VLMs) are used to provide semantic understanding and enhance the 3D volumetric representations. The VLMs, such as Stable Diffusion, are utilized to extract 2D features that are then distilled into a 3D voxel space, creating a rich and informative representation of the scene. This enables the robot to understand and manipulate objects based on both their 3D structure and semantic information. The GNFactor framework leverages these enhanced representations for policy learning, improving the robot's ability to perform multi-task manipulation in diverse and complex environments. The integration of VLMs allows the robot to interpret language instructions and execute tasks with greater accuracy and flexibility.|
|51|SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities|2024|CVPR|The application focuses on enhancing the spatial reasoning capabilities of Vision-Language Models (VLMs) to improve their performance in tasks requiring understanding of 3D spatial relationships. The proposed system, SpatialVLM, aims to generate and train on a large-scale spatial Visual Question Answering (VQA) dataset to improve the VLM's ability to perform both qualitative and quantitative spatial reasoning. This enhanced capability is particularly useful for applications in robotics and other fields requiring precise spatial understanding.|Vision-Language Models (VLMs) such as CLIP for filtering and generating semantic annotations from 2D images. Pre-trained expert models for object detection, depth estimation, semantic segmentation, and object-centric captioning.|In this context, Vision-Language Models (VLMs) are enhanced to perform direct spatial reasoning tasks by integrating 3D spatial information into their training. The VLMs are trained on a synthetic dataset generated using internet-scale real-world images, which includes spatial reasoning questions and answers. This allows the VLMs to answer complex spatial questions, make quantitative estimations, and perform spatial reasoning tasks that are critical for robotics applications. The enhanced VLMs can be used to generate rewards, perform success detection, and plan tasks in robotics by providing precise spatial information and reasoning capabilities. The integration of these spatially-aware VLMs with large language models (LLMs) further allows for complex chain-of-thought reasoning, enabling robots to understand and execute tasks with high spatial accuracy.|
|52|OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics|2024|ICRA|The application focuses on integrating various open-knowledge models for robotics to perform pick-and-drop tasks in real-world home environments. The OK-Robot framework combines vision-language models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, aiming to achieve high success rates in open-vocabulary mobile manipulation (OVMM).|CLIP for vision-language representation and object detection. OWL-ViT for open-vocabulary object detection. Segment Anything Model (SAM) for segmentation. AnyGrasp for generating grasp poses.|In this context, Vision-Language Models (VLMs) play a crucial role in providing semantic understanding and enabling the robot to detect and locate objects based on natural language queries. CLIP and OWL-ViT are used to identify objects in the environment and generate semantic embeddings stored in a VoxelMap, which serves as the robot's memory module. The VLMs enable zero-shot navigation to objects and support the robot in performing manipulation tasks by identifying and segmenting objects accurately. The integration of VLMs with navigation and grasping primitives allows OK-Robot to achieve a 58.5% success rate in cluttered environments and 82.4% in cleaner settings, demonstrating the effectiveness of combining open-knowledge models for complex robotic tasks.|
|53|MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting|2024|ICRA|The application focuses on enabling robots to perform open-vocabulary manipulation tasks specified by free-form language descriptions. The approach, Marking Open-vocabulary Keypoint Affordances (MOKA), employs vision-language models (VLMs) to solve manipulation tasks involving complex and diverse environments and task goals. The tasks include tool use, deformable body manipulation, and object rearrangement in real-world settings.|Vision-Language Models (VLMs) such as GPT-4V for affordance prediction and motion generation based on RGB images. GroundingDINO and SAM (Segment Anything Model) for object segmentation.|In this context, Vision-Language Models (VLMs) play a central role in bridging visual observations with robot motions. MOKA leverages VLMs to predict affordances and generate corresponding motions through a compact point-based representation. The VLMs are prompted to solve visual question-answering (VQA) problems, effectively converting affordance reasoning into a series of visual prompts. This enables the VLMs to generate motions by reasoning about keypoints and waypoints on 2D images, which are then translated into executable trajectories for the robot. The use of VLMs allows MOKA to perform diverse and complex manipulation tasks specified by natural language instructions, demonstrating robust performance in zero-shot and in-context learning scenarios.|
|54|DNAct: Diffusion Guided Multi-Task 3D Policy Learning|2024|arxiv|The application focuses on multi-task object manipulation in complex environments. The DNAct framework integrates neural rendering pre-training and diffusion training to create a unified, semantic-aware, and multi-modal 3D representation. This representation enables robots to perform various manipulation tasks with enhanced generalization and robustness.|Stable Diffusion for extracting 2D semantic features. Neural Radiance Fields (NeRF) for constructing a 3D scene representation.|In this context, Vision-Language Models (VLMs) like Stable Diffusion are used to extract rich semantic features from 2D images, which are then distilled into a 3D space using NeRF. This process creates a comprehensive 3D semantic representation that is used to guide the robot's manipulation tasks. The VLMs provide semantic understanding and context, enabling the robot to interpret complex scenes and perform multi-task manipulation with improved accuracy and efficiency. The diffusion training further refines these representations by learning multi-modal features from the action sequences, enhancing the robustness and generalizability of the learned policies.|
|55|Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?|2020|L4DC|The application focuses on the task specification for robot manipulation. Specifically, it aims to provide a low-effort and intuitive modality for task specification to engage non-expert end-users, enabling personalized robot agents in home environments. The study explores using various forms of goal specification, such as images from the internet, hand sketches, or simple language descriptions, to facilitate the task specification process.|ImageNet-supervised ResNet50 ImageNet-trained MoCo CLIP (a multi-modal embedding network that handles both visual and language goals)|The role of Visual Language Models (VLMs) in this context is to enable zero-shot task specification and goal-conditioning in robot manipulation tasks. The proposed Zero Shot Task-specification (ZeST) framework utilizes foundation models to embed both the observation and goal specifications, compute features of these embeddings, and measure the similarity between them. High similarity indicates that the goal specification has been achieved. This approach aims to overcome the limitations of traditional goal-conditioned methods, which require compact state space goals or goal images from the same scene. By leveraging off-domain images and language instructions, ZeST aims to reduce the task specification burden and facilitate multi-task learning and policy execution in reinforcement learning settings.|
|56|AUTORT: EMBODIED FOUNDATION MODELS FOR LARGE SCALE ORCHESTRATION OF ROBOTIC AGENTS|2024|arxiv|The application focuses on orchestrating robotic agents for large-scale, autonomous data collection in diverse and unstructured real-world environments. The system, named AutoRT, aims to deploy operational robots in new scenarios with minimal human supervision, enabling them to perform various tasks based on high-level objectives and environmental affordances. The ultimate goal is to gather diverse robotic experience to improve robot learning and autonomy.|Vision-Language Models (VLMs) for scene understanding and grounding. Large Language Models (LLMs) for generating instructions and guiding data collection.|In the context of AutoRT, Vision-Language Models (VLMs) play a crucial role in scene understanding and grounding. They describe the environment by identifying objects and their locations, which allows the system to generate appropriate manipulation tasks. The VLMs enable the robots to comprehend their surroundings and provide the necessary context for the LLMs to propose tasks. The LLMs, guided by the VLMs' outputs, generate diverse and novel instructions for the robots to execute. This collaborative use of VLMs and LLMs allows AutoRT to perform tasks autonomously or under minimal supervision, ensuring the scalability and diversity of the collected data while adhering to safety and operational constraints.|
|57|ViNT: A Foundation Model for Visual Navigation|2023|CoRL|The application focuses on visual navigation for mobile robotics. The goal is to develop a general-purpose pre-trained model, called Visual Navigation Transformer (ViNT), that can be adapted to various navigation tasks in different environments using visual inputs. ViNT aims to enable efficient navigation by leveraging a Transformer-based architecture to learn navigational affordances from diverse datasets.|Visual Navigation Transformer (ViNT), which is based on a Transformer architecture. ViNT is trained on a large and diverse set of robotic navigation datasets, enabling it to learn general navigational capabilities and adapt to new tasks and environments.|In this context, Vision-Language Models (VLMs) are utilized to enhance the navigation capabilities of mobile robots by providing a flexible and generalizable model that can handle various goal specifications. ViNT uses VLMs to interpret visual inputs and generate navigation plans based on those inputs. The model can adapt to different task specifications by fine-tuning or prompt-tuning, allowing it to be used in a wide range of navigation applications, from indoor mapping to kilometer-scale outdoor navigation. The VLM's role is to generalize across different environments and robot embodiments, enabling zero-shot deployment and efficient adaptation to new tasks with minimal additional data.|
|58|Composing Pre-Trained Object-Centric Representations for Robotics From "What" and "Where" Foundation Models|2024|ICRA|The application focuses on robotic manipulation in multi-object scenes. The paper introduces POCR (Pre-trained Object-Centric Representations) to enhance robot learning by leveraging pre-trained visual representations and object segmentation models. The goal is to improve the robot's ability to understand and manipulate objects in diverse environments by combining "what" (identity and properties of objects) and "where" (location and spatial relationships) information.|Segment Anything Model (SAM) for object segmentation ("where" information)   Pre-trained visual representations such as LIV (Language-Image Representations), R3M, and ImageNet models for encoding object properties ("what" information)|In this context, Vision-Language Models (VLMs) enhance robotic manipulation by providing a comprehensive understanding of objects and their spatial relationships within a scene. SAM generates segmentation masks to identify and locate objects, while pre-trained visual encoders describe object properties. This combination allows the POCR framework to create detailed and accurate representations of the environment, which are used for policy learning in robotic manipulation tasks. The VLMs enable robots to generalize across different tasks and environments, improving their ability to learn and perform complex manipulation tasks with minimal task-specific training data.|
|59|Gesture-Informed Robot Assistance via Foundation Models|2023|CoRL|The application focuses on human-robot interaction, specifically using gestures to inform robot assistance in various tasks. The proposed system, GIRAF, aims to improve the interpretation of deictic gestures (e.g., pointing) in table-top manipulation tasks, enhancing the robot's ability to understand and respond to human instructions effectively. The system is evaluated on its ability to reason about diverse gestures and perform tasks based on human-robot collaboration.|Large Language Models (LLMs), such as GPT-3.5, for reasoning and task planning. Vision-Language Models (VLMs) for scene description and grounding. Expert models like MediaPipe for gesture detection and classification.|In this context, Vision-Language Models (VLMs) are used to describe the scene and detect objects, providing essential information for grounding gestures. The VLMs identify and label objects in the environment, while the LLMs reason about the gestures and generate robot policies based on multimodal instructions (language and gestures). VLMs help in grounding the context of the task, enabling the robot to interpret the meaning of gestures and perform the corresponding actions accurately. The GIRAF framework integrates these models to enable more natural and efficient human-robot interaction by leveraging gesture and language inputs.|
|60|Towards A Unified Agent with Foundation Models|2023|ICLR|The application focuses on enhancing reinforcement learning (RL) agents using foundation models to tackle tasks in sparse-reward environments. The goal is to improve exploration efficiency, skill reuse, task scheduling, and learning from observations in robotic manipulation tasks. The framework is tested in a simulated robotic manipulation environment where the robot is tasked with stacking objects.|Large Language Models (LLMs), specifically FLAN-T5, for reasoning and generating sub-goals. Vision-Language Models (VLMs), specifically CLIP, for scene understanding and providing rewards based on visual observations.|In this context, Vision-Language Models (VLMs) play a pivotal role in describing the environment and providing intrinsic rewards for sub-goal completion. The VLMs map visual inputs to text descriptions, enabling the RL agent to understand and interact with its environment through language. This allows the agent to efficiently explore sparse-reward environments by breaking down tasks into sub-goals and using the VLM to verify if sub-goals are achieved, thus providing additional rewards. The VLMs also enable the agent to reuse data from past experiences and transfer learned skills to new tasks by identifying and relabeling successful trajectories from an offline buffer, significantly improving learning efficiency and scalability.|
|61|History Compression via Language Models in Reinforcement Learning|2022|ICML|The application focuses on reinforcement learning (RL) in partially observable environments. The paper introduces HELM (History comprEssion via Language Models), which leverages pre-trained language models for history compression to improve sample efficiency in RL. HELM is evaluated in environments like Minigrid and Procgen, where maintaining a memory of past observations is crucial for optimal decision-making.|Pretrained Language Transformer (PLT) specifically TransformerXL for history representation and compression.|In this context, Vision-Language Models (VLMs) are not directly used; instead, the focus is on using pre-trained language transformers (like TransformerXL) to compress history in RL tasks. The pre-trained language model captures and compresses past observations, creating a compact representation that improves the RL agent's ability to make decisions based on past experiences. The proposed HELM framework integrates this compressed history into actor-critic network architectures, enhancing sample efficiency and performance in partially observable environments. The TransformerXL helps transform partially observable Markov decision processes (POMDPs) into Markov decision processes (MDPs) by providing a comprehensive history representation, facilitating better learning and decision-making for RL agents.|
|62|Semantic HELM: A Human-Readable Memory for Reinforcement Learning|2023|NeurIPS|The application focuses on reinforcement learning (RL) in partially observable environments. The paper introduces Semantic HELM (SHELM), a novel memory mechanism that represents past events in human-readable language to enhance interpretability and performance. SHELM is evaluated in various environments, including MiniGrid, MiniWorld, Avalon, and Psychlab, where maintaining a memory of past observations is crucial for optimal decision-making.|CLIP for associating visual inputs with language tokens. Pre-trained language models, such as TransformerXL, for memory representation and compression.|In this context, Vision-Language Models (VLMs) play a crucial role in mapping visual observations to language tokens. CLIP is used to embed visual inputs and retrieve semantically related language tokens, which are then passed to a pre-trained language model serving as the memory module. This process allows the RL agent to maintain a coherent and human-readable representation of past events, enhancing the agent's ability to perform tasks that require memory. The VLMs enable the agent to understand and interpret visual inputs in a way that is comprehensible to humans, facilitating better troubleshooting and refinement of the memory mechanism. The human-readable memory component significantly improves the interpretability and performance of RL agents in partially observable environments.|
|63|Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning|2024|ICLR|The application focuses on using pre-trained Vision-Language Models (VLMs) as zero-shot reward models for reinforcement learning (RL) tasks. The study explores how VLMs can specify tasks via natural language descriptions and provide reward signals without the need for manually engineered reward functions. This approach is validated in environments like MuJoCo, where a humanoid robot is trained to perform complex tasks using natural language prompts.|CLIP (Contrastive Language-Image Pretraining), a vision-language model used to compute rewards based on the cosine similarity between the current environment state and a natural language task description.|In this context, Vision-Language Models (VLMs) are used as reward models to guide the RL agent's behavior. The VLMs, specifically CLIP, provide reward signals based on the similarity between a task description and the current state of the environment. This method allows the RL agent to learn complex tasks without manually specified reward functions. The VLMs are crucial in translating natural language prompts into meaningful rewards, facilitating zero-shot task specification and enabling the agent to learn tasks such as kneeling, sitting in a lotus position, doing the splits, and more. The study demonstrates the effectiveness of VLM-RMs in various RL benchmarks and highlights the scalability and robustness of VLMs in providing reward signals for RL agents.|
|64|VISION-LANGUAGE FOUNDATION MODELS AS EFFECTIVE ROBOT IMITATORS|2024|ICLR|The application focuses on utilizing vision-language foundation models (VLMs) for robotic manipulation tasks. The proposed framework, RoboFlamingo, aims to leverage existing VLMs, specifically OpenFlamingo, to perform language-conditioned manipulation tasks through imitation learning. The study evaluates RoboFlamingo's performance on tasks that require understanding and executing sequential instructions for manipulating objects.|OpenFlamingo, a vision-language model that integrates vision and language understanding. The framework is built on pre-trained VLMs, utilizing models like MPT (MosaicML Pretrained Transformers) and LLaMA (Large Language Model) for different configurations.|In this context, Vision-Language Models (VLMs) play a critical role in understanding and executing language-conditioned manipulation tasks. RoboFlamingo uses pre-trained VLMs to comprehend visual observations and language instructions at each decision step. The model decouples vision-language understanding from decision-making by adding a policy head that processes the fused representations from VLMs to predict robot actions. This approach allows the model to handle open-loop control and deploy on low-performance platforms, making it a cost-effective and flexible solution for robotic manipulation. The VLMs provide the capability to generalize across various tasks and environments, achieving state-of-the-art performance in benchmarks like CALVIN for long-horizon language-conditioned tasks.|
|65|Learning Generalizable Feature Fields for Mobile Manipulation|2024|arxiv|The paper addresses the challenge of open-vocabulary navigation and mobile manipulation tasks for robots. Specifically, it focuses on enabling robots to navigate and manipulate objects in diverse and dynamic real-world environments using language instructions.|Generalizable Neural Radiance Fields (Gen-NeRF) combined with CLIP feature distillation. The model, named GeFF (Generalizable Feature Fields), is designed to provide a unified scene representation for both navigation and manipulation by encoding geometric and semantic information from an RGB-D stream.|The VLM in this work is used to align the rich scene priors obtained from the neural feature fields with natural language. This is achieved through feature distillation from the pre-trained CLIP model, allowing the robot to understand and act upon language instructions. The VLM enables the robot to perform real-time, language-conditioned navigation and manipulation tasks by decoding the scene representation into actionable information for the robot.|


















