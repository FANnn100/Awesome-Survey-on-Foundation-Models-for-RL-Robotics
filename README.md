# Awesome-Survey-on-Foundation-Models-for-RL-Robotics
This is the official repository of "Unifying Modern AI with Robotics: Survey on MDPs with Diffusion and Foundation Models".
<img width="1073" alt="image" src="https://github.com/user-attachments/assets/6cda6ff4-c451-4481-a18d-7dc30b81e19a" />

## Relevant Surveys
| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| [A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093)    | arxiv    | 2024 | [Web](https://github.com/yueen-ma/awesome-vla)|
| [Large Multimodal Agents: A Survey](https://arxiv.org/abs/2402.15116)    | arxiv    | 2024 | [Web](https://github.com/jun0wanan/awesome-large-multimodal-agents)|
| [Agent AI: Surveying the Horizons of Multimodal Interaction](https://arxiv.org/abs/2401.03568)    | arxiv    | 2024 | - |
| [Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](https://arxiv.org/abs/2407.06886)    | arxiv    | 2024 | [Web](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List) |
| [A survey on large language model based autonomous agents](https://arxiv.org/abs/2308.11432)    | Front. Comput. Sci   | 2024 | [Web](https://github.com/Paitesanshi/LLM-Agent-Survey) |
| [A Survey on Robotics with Foundation Models: toward Embodied AI](https://arxiv.org/abs/2402.02385)    | arxiv   | 2024 | - |
| [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)    | arxiv   | 2024 | - |
| [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)    | arxiv    | 2023 | [Web](https://robotics-fm-survey.github.io/) |





## Planning

| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| SuSIE: [Zero-shot robotic manipulation with pretrained image-editing diffusion models](https://arxiv.org/pdf/2310.10639)                                                            | ICLR        | 2024 | [Code](https://rail-berkeley.github.io/susie/)|
| PSL: [Plan-seq-learn: Language model guided rl for solving long horizon robotics tasks](https://arxiv.org/abs/2405.01534)                                                            | ICLR-LLMAgents         | 2024 | [Code](https://github.com/mihdalal/planseqlearn)|
| TWOSOME: [True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning](https://arxiv.org/abs/2401.14151)                                                        | ICLR                   | 2024 | [Code](https://github.com/WeihaoTan/TWOSOME) |
| POLIFORMER: [PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators](https://arxiv.org/abs/2406.20083)                                              | CoRL                   | 2024 | [Code](https://github.com/allenai/poliformer) |
| LLM-MCTS: [Large Language Models as Commonsense Knowledge for Large-Scale Task Planning](https://arxiv.org/abs/2305.14078)                                                       | NeurIPS                | 2024 | [Code](https://github.com/1989Ryan/llm-mcts) |
| Lang2LTL-2: [Lang2LTL-2: Grounding Spatiotemporal Navigation Commands Using Large Language and Vision-Language Models](https://semrob.github.io/docs/rss_semrob2024_cr_paper23.pdf)                    | CoRL Workshop          | 2024 | [Code](https://github.com/h2r/Lang2LTL-2), [Data](https://drive.google.com/drive/folders/1gWomkuVqxLU01ftzF34bEacJBeUwBMOf) |
| SceneDiffuser : [Diffusion-based Generation, Optimization, and Planning in 3D Scenes](https://scenediffuser.github.io/)                              | CVPR                   | 2023 |[Code](https://github.com/scenediffuser/Scene-Diffuser)|
| LGMCTS: [LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement](https://arxiv.org/pdf/2309.15821v2)                              | IROS                   | 2023 |[Code](https://github.com/changhaonan/LGMCTS-D)|
| DoReMi: [DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment](https://arxiv.org/abs/2307.00329)                                 | IROS                   | 2023 |[Web](https://sites.google.com/view/doremi-paper)|
| CoELA: [Building Cooperative Embodied Agents Modularly with Large Language Models](https://arxiv.org/abs/2307.02485)                                                          | ICLR                   | 2023 | [Web](https://vis-www.cs.umass.edu/Co-LLM-Agents)|
| GD: [Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents](https://arxiv.org/abs/2303.00855)                                                              | NeurIPS                | 2023 |[Web](grounded-decoding.github.io)|
| UNREST: [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments](https://arxiv.org/abs/2309.16397)                                                  | CoRL                   | 2023 |[Code](https://github.com/Emiyalzn/CoRL24-UNREST)|
| Text2Motion: [Text2motion: From natural language instructions to feasible plans](https://arxiv.org/abs/2303.12153)                                                     | Autonomous Robots      | 2023 |[Web](https://sites.google.com/stanford.edu/text2motion)|
| GLAM: [Grounding large language models in interactive environments with online reinforcement learning](https://arxiv.org/abs/2302.02662)                                                | ICML                   | 2023 |[Code](https://github.com/flowersteam/Grounding_LLMs_with_online_RL)|
| Decision-Diffuser: [Is Conditional Generative Modeling all you need for Decision-Making?](https://arxiv.org/pdf/2211.15657)                                                            | ICLR        | 2023 | [Code](https://anuragajay.github.io/decision-diffuser/)|
| UniSim: [Learning interactive real-world simulators](https://arxiv.org/abs/2310.06114)                                                          | ICLR                   | 2023 |[Web](https://universal-simulator.github.io/unisim/)|
| Retroformer: [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://arxiv.org/abs/2308.02151)                    | ICLR                   | 2023 |[Code](https://github.com/weirayao/Retroformer)|
| ALFRED: [Egocentric planning for scalable embodied task achievement](https://arxiv.org/abs/2306.01295)                                                         | NeurIPS                | 2023 | -    |
| Diffuser: [Planning with diffusion for flexible behavior synthesis](https://arxiv.org/pdf/2205.09991)                                                            | ICML        | 2022 | [Code](https://diffusion-planning.github.io/)|
| SayCan: [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691)                                                          | CoRL                   | 2022 | [Web](say-can.github.io)|
| Inner Monologue: [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608)                                              | CoRL                   | 2022 |[Web](https://innermonologue.github.io/)|

## Perception
| Title                                                                 | Venue                  | Date | Source |
|------------------------------------------------------------------------|------------------------|------|------|
| LocoTransformer: [Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers](https://arxiv.org/abs/2107.03996)                 | ICLR        | 2022 | [Code](https://github.com/Mehooz/vision4leg)|
| AVLEN: [Avlen: Audio-visual-language embodied navigation in 3d environments](https://proceedings.neurips.cc/paper_files/paper/2022/hash/28f699175783a2c828ae74d53dd3da20-Abstract-Conference.html) | NeurIPS | 2022 |  - |
| DetCLIP: [Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection](https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ba960559212691be13fa81d9e5e0047-Abstract-Conference.html) | NeurIPS | 2022 | - |
| PointCLIP: [Pointclip: Point cloud understanding by clip](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.html) | CVPR | 2022 | [Code](https://github.com/ZrrSkywalker/PointCLIP) |
| VIP: [Vip: Towards universal visual reward and representation via value-implicit pre-training](https://arxiv.org/abs/2210.00030) | ICLR | 2023 | [Code](https://github.com/facebookresearch/vip) |
| OWL-ViT: [Simple open-vocabulary object detection](https://link.springer.com/chapter/10.1007/978-3-031-20080-9_42) | ECCV | 2022 | [Code](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) |
| CoOp: [Learning to prompt for vision-language models](https://link.springer.com/article/10.1007/s11263-022-01653-1) | IJCV | 2022 | [Code](https://github.com/KaiyangZhou/CoOp) |
| OVRL: [Offline visual representation learning for embodied navigation](https://openreview.net/forum?id=Spfbts_vNY) | ICLR workshop | 2023 | - |
| DreamerV3: [Mastering diverse domains through world models](https://arxiv.org/abs/2301.04104) | arxiv | 2023 | - |
| VC-1: [Where are we in the search for an artificial visual cortex for embodied intelligence?](https://proceedings.neurips.cc/paper_files/paper/2023/hash/022ca1bed6b574b962c48a2856eb207b-Abstract-Conference.html) | NeurIPS | 2023 | [Code](https://github.com/facebookresearch/eai-vc)|
| Grounding DINO: [Grounding dino: Marrying dino with grounded pre-training for open-set object detection](https://link.springer.com/chapter/10.1007/978-3-031-72970-6_3) | ECCV | 2024 | [Code](https://github.com/IDEA-Research/GroundingDINO) |
| LLaRP: [Large language models as generalizable policies for embodied tasks](https://openreview.net/forum?id=u6imHU4Ebu) | ICLR | 2023 | [Code](https://github.com/apple/ml-llarp) |
| LIV: [Liv: Language-image representations and rewards for robotic control](https://proceedings.mlr.press/v202/ma23b.html) | ICML | 2023 | [Code](https://github.com/penn-pal-lab/LIV) |
| NaVid: [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/abs/2402.15852) | RSS | 2024 | [Code](https://github.com/jzhzhang/NaVid-VLN-CE) |
| LLM2Automata: [Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception](https://arxiv.org/pdf/2308.05295) | AAMAS | 2024 | - |
| CONPE: [Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ad72633e034990a97e878fc2fc100afb-Abstract-Conference.html) | NeurIPS | 2024 | - |
| CLIP-Adapter: [Clip-adapter: Better vision-language models with feature adapters](https://link.springer.com/article/10.1007/s11263-023-01891-x) | IJCV | 2024 | [Code](https://github.com/gaopengcuhk/CLIP-Adapter) | 
| PAC: [Offline actor-critic reinforcement learning scales to large models](https://arxiv.org/abs/2402.05546) | ICML | 2024 | - |
| LLM-POP: [Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks](https://ieeexplore.ieee.org/abstract/document/10610981) | 

## Exploration
| Title                              | Venue                | Date | Source |
|------------------------------------|----------------------|------|------|
| RoboEXP: [RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation](https://arxiv.org/abs/2402.15487)                       | arxiv                | 2024 |[Web](https://jianghanxiao.github.io/roboexp-web/)|
| IGE-LLMs: [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks](https://arxiv.org/abs/2309.16347)                     | ICRA                 | 2024 | -    |
| ELLM: [Guiding pretraining in reinforcement learning with large language models](https://arxiv.org/abs/2302.06692)                         | ICML                 | 2023 |[Code](https://github.com/yuqingd/ellm)|
| Motif: [Motif: Intrinsic motivation from artificial intelligence feedback](https://arxiv.org/abs/2310.00166)                        | arxiv                | 2023 |[Code](https://github.com/facebookresearch/motif)|



## Reasoning
| Title                              | Venue                | Date | Source  |
|------------------------------------|----------------------|------|------|
| HAZARD: [HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments](https://arxiv.org/abs/2401.12975)                        | ICLR                 | 2024 |[Code](https://github.com/UMass-Embodied-AGI/HAZARD)|
| PR2L: [Vision-language models provide promptable representations for reinforcement learning](https://arxiv.org/abs/2402.02651)                           | arxiv                | 2024 |[Web](https://pr2l.github.io/)|
| CAPE: [Cape: Corrective actions from precondition errors using large language models](https://arxiv.org/abs/2211.09935)                          | ICRA                 | 2024 |[Web](https://shreyas-s-raman.github.io/CAPE/)|
| SayCanPay: [SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge](https://arxiv.org/abs/2308.12682)                   | AAAI                 | 2024 |[Web](https://rishihazra.github.io/SayCanPay/)|
| MASE: [Multi-agent strategy explanations for human-robot collaboration](https://arxiv.org/abs/2311.11955)                        | ICRA                 | 2024 | -    |
| FoMo: [FoMo Rewards: Can we cast foundation models as reward functions?](https://arxiv.org/abs/2312.03881)                   | NeurIPS Workshop     | 2023 | -    |
| Language to Rewards: [Language to rewards for robotic skill synthesis](https://arxiv.org/abs/2306.08647)         | CoRL                 | 2023 |[Web](https://language-to-reward.github.io/)|

## Others
| Title                              | Venue                | Date | Source  |
|------------------------------------|----------------------|------|------|
| RPA: [Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment](https://ieeexplore.ieee.org/abstract/document/9785410)                        | RAL                 | 2022 | - |
| Language-centric Agents: [Towards a unified agent with foundation models](https://arxiv.org/abs/2307.09668) | ICLR workshop | 2023 | - |
| AdA: [Human-timescale adaptation in an open-ended task space](https://arxiv.org/abs/2301.07608) | ICML | 2023 | - |
| EUREKA: [Eureka: Human-level reward design via coding large language models](https://arxiv.org/abs/2310.12931) | ICLR | 2024 | [Code](https://github.com/eureka-research/Eureka) |
| LESR: [LLM-Empowered State Representation for Reinforcement Learning](https://arxiv.org/abs/2407.13237) | ICML | 2024 | [Code](https://github.com/thu-rllab/LESR) |
| Text2Reward: [Text2Reward: Reward Shaping with Language Models for Reinforcement Learning](https://arxiv.org/abs/2309.11489) |ICLR | 2024 | [Code](https://github.com/xlang-ai/text2reward) |
| Automation method: [Position: Automatic Environment Shaping is the Next Frontier in RL](https://openreview.net/forum?id=dslUyy1rN4) | ICML | 2024 | [Code](https://github.com/auto-env-shaping/EnvCoderBench) |
| ChatAdp: [ChatAdp: ChatGPT-powered Adaptation System for Human-Robot Interaction](https://ieeexplore.ieee.org/abstract/document/10611520) | ICRA | 2024 | - |
| PromptGAT: [Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning](https://ojs.aaai.org/index.php/AAAI/article/view/27758) | AAAI| 2024 | [Code](https://github.com/DaRL-LibSignal/PromptGAT) |
| PPP: [Prompt, plan, perform: Llm-based humanoid control via quantized imitation learning](https://ieeexplore.ieee.org/abstract/document/10610948) | ICRA | 2024 | - |
| BMQ: [Learning agile bipedal motions on a quadrupedal robot](https://ieeexplore.ieee.org/abstract/document/10611442) | ICRA | 2024 | - |
| DSCRL: [Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications](https://ieeexplore.ieee.org/abstract/document/10610942) | ICRA | 2024 | - |
